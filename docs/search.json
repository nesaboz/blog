[
  {
    "objectID": "projects/solar/solar.html",
    "href": "projects/solar/solar.html",
    "title": "Nenad Bozinovic",
    "section": "",
    "text": "Solar\n\nDetecting solar panels from satellite images (github)\n\nGoal of this project is to deliver segmentation masks of solar panel farms using satellite images. Due to large varieties of the images, I decided to use deep segmentation/PyTorch.\n\n\nData\nData preparation/cleaning was intensive. Most labels were provided thought quality was not ideal. Nevertheless, I am grateful for what seems to have been a painful labeling job. I also generated some labels using LabelMe. I first split several large (~1GB) satellite images into smaller ones (256x256 size). There were 449 labels of 3 classes: racks, common panels, and dense panels:\n  \n\n\nAugmentation\nIt was insightful to plot mean and standard deviation of labeled and unlabeled images to identified range for ColorJitter as augmentation: \nLabeled and unlabeled data have decent overlap, with non-covered datapoints typically being clouds (high mean, low stdev) or with partial out-of-bounds, i.e. black, zones (low mean). See main_experimentation.py for examples.\n\n\nTraining\nI used Adam optimizer and weighed CrossEntropyLoss. Segmentation was based on UNet-like arhitecture and this paper (see models.py):\n\n\n\nlosses.png\n\n\nTraining for 120 epoch took about 6 minutes (RTX5000). Achieved 71% Jaccard index (aka IoU) on validation dataset.\n\n\nInference\nFinally, I evaluated and stitched together predicted images back into large satellite images (size ~1Gb, showing here only smaller section):\n\n\n\nimg.png\n\n\n(Note: some missing patches in the image were used for training).\nShadows and clouds are probably the biggest obstacle for precise counting. While augmentations can help to some extent, ideally multiple images of the same solar farm should be obtained and combined for thorough coverage.\nThe following are some notable examples.\nHandling multiple classes:\n   \nHandling different backgrounds:\n   \nAnd here are the common failures, mostly shades and clouds:\n      \n\n\nConclusion\nObject segmentation has shown to perform well for solar panel detection. Augmentation using brightness and contrast improved detection significantly.\nNumber of solar panels is calculated knowing pixel resolution is 50cm/pixel and panels width of about 100 cm (i.e. 2 pixels).\nSmall edge improvements can be done by allowing for some overlap when cropping large images and combining masks subsequently.\nReferences: Boiler-plate code based on a book “PyTorch: Step by Step” by Daniel Voigt Godoy."
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media",
    "section": "",
    "text": "Implemented in all BLI instruments the following patent enabled COVID-19 antibody discovery:\n\nD. Thaker, M. Fowler, S. Nedungadi, D. V. Banda, B. Bruhn, N. Bozinovic, K. Mobilia, “Systems and methods for optimizing an instrument system workflow”, Patent, No. WO-2020243015A1 (2019) (link/download)."
  },
  {
    "objectID": "media.html#press",
    "href": "media.html#press",
    "title": "Media",
    "section": "Press",
    "text": "Press\nTogether with Prof. Ramachandran at Boston University I invented fiber-optic system based on orbital-angular-momentum multiplexing, following are few of the media releases:"
  },
  {
    "objectID": "media.html#publications",
    "href": "media.html#publications",
    "title": "Media",
    "section": "Publications",
    "text": "Publications\nMy publications span work from bio-microscopy to fiber-optics and optical networking. I designed complex prototypes as well as built robust systems that went to the bottom of the ocean.\nFor citations see here.\n\nN. Bozinovic, Y. Yue, Y. Ren, M. Tur, P. Kristensen, H. Huang, A. E. Willner, S. Ramachandran, “Terabit-Scale Orbital Angular Momentum Mode Division Multiplexing in Fibers”, Science, 28 June 2013: 340 (6140), 1545-1548. (link / download)\nN. Bozinovic, “Orbital Angular Momentum in Fibers”, PhD thesis, 2013. (download)\nJ. Wang, M.J. Padgett, S. Ramachandran, N. Bozinovic, S. Golowich, M.P.J. Lavery, H. Huang, Y. Yue, A.E. Willner “Multimode communications using OAM,” in Optical Fiber Telecommunications VI-B, I. Kaminow, T. Li, A.E. Willner, Ed., Academic Press, 2013. (link)\nY. Yue, N. Bozinovic, Y. Ren, H. Huang, M. Tur, P. Kristensen, S. Ramachandran, and A. E. Willner, “1.6-Tbit/s Muxing, Transmission and Demuxing through 1.1-km of Vortex Fiber Carrying 2 OAM Beams Each with 10 Wavelength Channels,” in Optical Fiber Communication Conference/National Fiber Optic Engineers Conference 2013, OSA Technical Digest, paper OTh4G.2. (link)\nN. Bozinovic, Y. Yue, Y. Ren, M. Tur, P. Kristensen, A. Willner, and S. Ramachandran, “Orbital Angular Momentum (OAM) Based Mode Division Multiplexing (MDM) over a Km-length Fiber,” in European Conference on Optical Communication, OSA Technical Digest, 2012, post-deadline paper Th.3.C.6. (link / download)\nN. Bozinovic, S. Golowich, P. Kristensen, and S. Ramachandran, “Control of orbital angular momentum of light with optical fibers,” Optics Letters 37, 2451-2453 (2012). (link / download)\nS. Ramachandran, N. Bozinovic, P. Gregg, S. Golowich, and P. Kristensen, “Optical vortices in fibres: A new degree of freedom for mode multiplexing,” in European Conference on Optical Communication, OSA Technical Digest, 2012, invited paper Tu.3.F.3. (link)\nS. Golowich, P. Kristensen, N. Bozinovic, P. Gregg, and S. Ramachandran, “Fibers Supporting Orbital Angular Momentum States for Information Capacity Scaling,” in Frontiers in Optics Conference, OSA Technical Digest, 2012, invited paper FW2D.2. (link)\nS. Golowich, N. Bozinovic, P. Kristensen, and S. Ramachandran, “Vortex Fiber Mode Amplitude Estimation,” in CLEO: Applications and Technology, OSA Technical Digest, 2012, paper JTu2K.2. (link)\nN. Bozinovic, S. Ramachandran, M. Brodsky, and P. Kristensen, “Record-length transmission of entangled photons with orbital angular momentum (vortices),” in Frontiers in Optics, OSA Technical Digest, 2011, post-deadline paper PDPB1. (link) (download)\nN. Bozinovic, P. Kristensen, and S. Ramachandran, “Long-range fiber-transmission of photons with orbital angular momentum,” in CLEO:2011 - Laser Applications to Photonic Applications, OSA Technical Digest , 2011, paper CTuB1. (link / download)\nN. Bozinovic, P. Kristensen, and S. Ramachandran, “Are Orbital Angular Momentum (OAM/Vortex) States of Light Long-Lived in Fibers?,” in Frontiers in Optics/Laser Science, OSA Technical Digest, 2011, paper LWL3. (link)\nS. Santos, K. Chu, D. Lim, N. Bozinovic, T. Ford, C. Hourtoule, A. C. Bartoo, S. K. Singh, J. Mertz, Optically sectioned fluorescence endomicroscopy with hybrid-illumination imaging through a flexible fiber bundle”, J. Biomed. Opt. 14, 030502, (2009) (link)\nS. Santos, K. Chu, D. Lim, N. Bozinovic, T. Ford, C. Hourtoule, A. Bartoo, S. Singh, and J. Mertz, “Optically Sectioned Fluorescence Endomicroscopy with Hybrid-Illumination Imaging through a Flexible Fiber Bundle,” in Novel Techniques in Microscopy, OSA Technical Digest, 2009, paper NWC3. (link)\nN. Bozinovic, C. Ventalon, T. Ford, and J. Mertz, “Fluorescence endomicroscopy with structured illumination,” Opt. Express 16, 8016-8025 (2008). (link)\nN. Bozinovic, C. Ventalon, T. Ford, and J. Mertz, “Fluorescence Endomicroscopy with Out-of-Focus Background Rejection,” in Biomedical Optics, OSA Technical Digest, 2008, paper BTuF57. (link / download)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Solar\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m a software engineer with PhD in Electrical Engineering. I’ve published Science Journal paper with 2500+ citations, patented instrument software that enabled COVID-19 antibody discovery, and built optical instruments that went to the bottom of the ocean. I’m passionate about machine learning, innovation, and great teams.\nWhen not messing with code, I arm-wrestle with my 3-year old twins, do yoga, mountain bike, ski, and hone my darts’ skills.\nemail me"
  },
  {
    "objectID": "tips/010_Python/datetime.html",
    "href": "tips/010_Python/datetime.html",
    "title": "Datetime",
    "section": "",
    "text": "import datetime\n\n\ndatetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n\n'20230119_112149'\n\n\n\ndatetime.datetime.now().strftime('%Y_%m_%d')\n\n'2023_01_19'\n\n\n\ndef get_elapsed_days(b):\n    a = datetime.datetime.now()\n    return (a-b).days\n\nb = datetime.datetime(2018, 1, 10, 14, 37)\nprint(get_elapsed_days(b))\n\n1845"
  },
  {
    "objectID": "tips/010_Python/pydata.html",
    "href": "tips/010_Python/pydata.html",
    "title": "PyData",
    "section": "",
    "text": "import numpy as np\nnp.random.seed(42)\na = np.random.randn(1,10)\nprint(a)\nprint(a.max())\nprint(a.dtype)\na = a.astype(np.uint8)\nprint(a)\nprint(a.max())\nprint(a.dtype)\nprint(a.shape)\na = a.squeeze()\nprint(a.shape)\nTo scale tensor from 0, 255, convert to uint8\nb = (np.array(mask_tensor) / np.array(mask_tensor).max() * 255).astype(np.uint8).squeeze()\nb.max()\nb = (np.array(mask_tensor) / np.array(mask_tensor).max()).astype(np.float32).squeeze()\nb.max()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial import polynomial as P\n\nM = 100;\nN = 4;\nx = np.linspace(-20, 20, M)\nX = np.fliplr(np.vander(x , N + 1))\n# for i in range(N+1):\n#     X[:,i] = x**i\nnoise = 20 * np.random.randn(M)\nbeta = [18, -12, 2, 0.1, 0.1];\ny = np.dot(X, beta) + noise\n\nbeta_r = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\ndef plot_something(p):\n    fig, ax = plt.subplots()\n    ax.plot(p, 'o')\n    ax.set_title('Random')\n    plt.show()\n\n\nbeta_r, stats = P.polyfit(x, y, 4, full=True)\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\nhelp(P.polyfit)\n\n\n\n\n\n\n\nHelp on function polyfit in module numpy.polynomial.polynomial:\n\npolyfit(x, y, deg, rcond=None, full=False, w=None)\n    Least-squares fit of a polynomial to data.\n    \n    Return the coefficients of a polynomial of degree `deg` that is the\n    least squares fit to the data values `y` given at points `x`. If `y` is\n    1-D the returned coefficients will also be 1-D. If `y` is 2-D multiple\n    fits are done, one for each column of `y`, and the resulting\n    coefficients are stored in the corresponding columns of a 2-D return.\n    The fitted polynomial(s) are in the form\n    \n    .. math::  p(x) = c_0 + c_1 * x + ... + c_n * x^n,\n    \n    where `n` is `deg`.\n    \n    Parameters\n    ----------\n    x : array_like, shape (`M`,)\n        x-coordinates of the `M` sample (data) points ``(x[i], y[i])``.\n    y : array_like, shape (`M`,) or (`M`, `K`)\n        y-coordinates of the sample points.  Several sets of sample points\n        sharing the same x-coordinates can be (independently) fit with one\n        call to `polyfit` by passing in for `y` a 2-D array that contains\n        one data set per column.\n    deg : int or 1-D array_like\n        Degree(s) of the fitting polynomials. If `deg` is a single integer\n        all terms up to and including the `deg`'th term are included in the\n        fit. For NumPy versions >= 1.11.0 a list of integers specifying the\n        degrees of the terms to include may be used instead.\n    rcond : float, optional\n        Relative condition number of the fit.  Singular values smaller\n        than `rcond`, relative to the largest singular value, will be\n        ignored.  The default value is ``len(x)*eps``, where `eps` is the\n        relative precision of the platform's float type, about 2e-16 in\n        most cases.\n    full : bool, optional\n        Switch determining the nature of the return value.  When ``False``\n        (the default) just the coefficients are returned; when ``True``,\n        diagnostic information from the singular value decomposition (used\n        to solve the fit's matrix equation) is also returned.\n    w : array_like, shape (`M`,), optional\n        Weights. If not None, the weight ``w[i]`` applies to the unsquared\n        residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n        chosen so that the errors of the products ``w[i]*y[i]`` all have the\n        same variance.  When using inverse-variance weighting, use\n        ``w[i] = 1/sigma(y[i])``.  The default value is None.\n    \n        .. versionadded:: 1.5.0\n    \n    Returns\n    -------\n    coef : ndarray, shape (`deg` + 1,) or (`deg` + 1, `K`)\n        Polynomial coefficients ordered from low to high.  If `y` was 2-D,\n        the coefficients in column `k` of `coef` represent the polynomial\n        fit to the data in `y`'s `k`-th column.\n    \n    [residuals, rank, singular_values, rcond] : list\n        These values are only returned if ``full == True``\n    \n        - residuals -- sum of squared residuals of the least squares fit\n        - rank -- the numerical rank of the scaled Vandermonde matrix\n        - singular_values -- singular values of the scaled Vandermonde matrix\n        - rcond -- value of `rcond`.\n    \n        For more details, see `numpy.linalg.lstsq`.\n    \n    Raises\n    ------\n    RankWarning\n        Raised if the matrix in the least-squares fit is rank deficient.\n        The warning is only raised if ``full == False``.  The warnings can\n        be turned off by:\n    \n        >>> import warnings\n        >>> warnings.simplefilter('ignore', np.RankWarning)\n    \n    See Also\n    --------\n    numpy.polynomial.chebyshev.chebfit\n    numpy.polynomial.legendre.legfit\n    numpy.polynomial.laguerre.lagfit\n    numpy.polynomial.hermite.hermfit\n    numpy.polynomial.hermite_e.hermefit\n    polyval : Evaluates a polynomial.\n    polyvander : Vandermonde matrix for powers.\n    numpy.linalg.lstsq : Computes a least-squares fit from the matrix.\n    scipy.interpolate.UnivariateSpline : Computes spline fits.\n    \n    Notes\n    -----\n    The solution is the coefficients of the polynomial `p` that minimizes\n    the sum of the weighted squared errors\n    \n    .. math:: E = \\sum_j w_j^2 * |y_j - p(x_j)|^2,\n    \n    where the :math:`w_j` are the weights. This problem is solved by\n    setting up the (typically) over-determined matrix equation:\n    \n    .. math:: V(x) * c = w * y,\n    \n    where `V` is the weighted pseudo Vandermonde matrix of `x`, `c` are the\n    coefficients to be solved for, `w` are the weights, and `y` are the\n    observed values.  This equation is then solved using the singular value\n    decomposition of `V`.\n    \n    If some of the singular values of `V` are so small that they are\n    neglected (and `full` == ``False``), a `RankWarning` will be raised.\n    This means that the coefficient values may be poorly determined.\n    Fitting to a lower order polynomial will usually get rid of the warning\n    (but may not be what you want, of course; if you have independent\n    reason(s) for choosing the degree which isn't working, you may have to:\n    a) reconsider those reasons, and/or b) reconsider the quality of your\n    data).  The `rcond` parameter can also be set to a value smaller than\n    its default, but the resulting fit may be spurious and have large\n    contributions from roundoff error.\n    \n    Polynomial fits using double precision tend to \"fail\" at about\n    (polynomial) degree 20. Fits using Chebyshev or Legendre series are\n    generally better conditioned, but much can still depend on the\n    distribution of the sample points and the smoothness of the data.  If\n    the quality of the fit is inadequate, splines may be a good\n    alternative.\n    \n    Examples\n    --------\n    >>> np.random.seed(123)\n    >>> from numpy.polynomial import polynomial as P\n    >>> x = np.linspace(-1,1,51) # x \"data\": [-1, -0.96, ..., 0.96, 1]\n    >>> y = x**3 - x + np.random.randn(len(x))  # x^3 - x + Gaussian noise\n    >>> c, stats = P.polyfit(x,y,3,full=True)\n    >>> np.random.seed(123)\n    >>> c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1\n    array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286]) # may vary\n    >>> stats # note the large SSR, explaining the rather poor results\n     [array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316, # may vary\n              0.28853036]), 1.1324274851176597e-014]\n    \n    Same thing without the added noise\n    \n    >>> y = x**3 - x\n    >>> c, stats = P.polyfit(x,y,3,full=True)\n    >>> c # c[0], c[2] should be \"very close to 0\", c[1] ~= -1, c[3] ~= 1\n    array([-6.36925336e-18, -1.00000000e+00, -4.08053781e-16,  1.00000000e+00])\n    >>> stats # note the minuscule SSR\n    [array([  7.46346754e-31]), 4, array([ 1.38446749,  1.32119158, # may vary\n               0.50443316,  0.28853036]), 1.1324274851176597e-014]\n\n\n\n\n\n\nSome random image manipulations:\n\nimport cv2\n\nvideo = cv2.VideoCapture(0)\n\nfirst_frame = None\n\nwhile True:\n\n    check, frame = video.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    if first_frame is None:\n        first_frame = gray\n        continue\n\n\n    delta_frame = cv2.absdiff(first_frame, gray)\n    thresh_frame = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n    thresh_frame = cv2.dilate(thresh_frame, None, iterations=2)\n\n    ctns, _ = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n\n    for contour in ctns:\n        if cv2.contourArea(contour) < 1000:\n            continue\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n\n    cv2.imshow(\"Gray frame\", gray)\n    cv2.imshow(\"Delta frame\", delta_frame)\n    cv2.imshow(\"Threshold frame\", thresh_frame)\n    cv2.imshow(\"Color frame\", frame)\n\n    key = cv2.waitKey(10)\n\n    if key == ord('q'):\n        break\n\nvideo.release()\n\n\n\n\nTo read the csv file:\n\ndf = pd.read_csv(file_str, header=1, names=['time', 'a', 'b', 'c', 'd', 'e'])"
  },
  {
    "objectID": "tips/010_Python/pil.html",
    "href": "tips/010_Python/pil.html",
    "title": "PIL",
    "section": "",
    "text": "from PIL import Image\nfrom PIL import ImageDraw, ImageStat\nfrom pathlib import Path\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\nim = Image.open(\"assets/example.png\")\nim\n\n\n\n\nThis will show image in a separate window, title doesn’t work, see here.\n\nim.show('pera')\n\nThis can show image in Jupyter with title but not true colors:\n\nplt.imshow(im)\nplt.title('pera')\nplt.show()\n\n\n\n\nIn jupyter one can use only:\n\nim  # display(im)\n\n\n\n\nTo create new image:\n\nwidth, height = 256, 256\nmask = Image.new('L', (width, height), 0)\n\nExample to draw a polygon:\n\npolygon = [(4, 1), (1, 54), (1, 222), (13, 1)]\nImageDraw.Draw(mask).polygon(polygon, outline=255, fill=255)\n\nTo load image from png, show bands (i.e. channels):\n\nim = Image.open('assets/example.png')\nprint(im.getbands())\n\n('R', 'G', 'B')\n\n\n\nim.getchannel('R')\n\n\n\n\nTo convert to RGB (from RGBA for example):\n\nim.convert('RGB')\n\n\n\n\n\nim.convert(\"RGBA\")\n\n\n\n\n\nim.convert('L')\n\n\n\n\nTo convert to tensor:\n\ntensorizer = ToTensor()\nim_tensor = tensorizer(im)\nprint(im_tensor.shape)\nim_tensor[:, :3, :3]\n\ntorch.Size([3, 256, 256])\n\n\ntensor([[[0.3020, 0.2902, 0.2706],\n         [0.2941, 0.2824, 0.2667],\n         [0.2824, 0.2706, 0.2627]],\n\n        [[0.3333, 0.3216, 0.3059],\n         [0.3294, 0.3176, 0.3059],\n         [0.3137, 0.3059, 0.3020]],\n\n        [[0.3059, 0.2980, 0.2863],\n         [0.2941, 0.2902, 0.2863],\n         [0.2745, 0.2745, 0.2784]]])\n\n\n\ndef save_image(im: Image, filepath: Path, overwrite: bool = False):\n    if filepath.exists() and not overwrite:\n        return\n    im.save(filepath, \"PNG\")\n\n\nStat\n\nstat = ImageStat.Stat(im)\nprint(\"\"\"\n* Min/max values for each band in the image:\n    {.extrema}\n\n* Total number of pixels for each band in the image:\n    {.count}\n\n* Sum of all pixels for each band in the image:\n    {.sum}\n\n* Squared sum of all pixels for each band in the image:\n    {.sum2}\n\n* Average (arithmetic mean) pixel level for each band in the image:\n    {.mean}\n\n* Median pixel level for each band in the image:\n    {.median}\n\n* RMS (root-mean-square) for each band in the image:\n    {.rms}\n\n* Variance for each band in the image:\n    {.var}\n\n* Standard deviation for each band in the image:\n    {.stddev}\n\"\"\".format(*((stat, ) * 9)))\n\n\n* Min/max values for each band in the image:\n    [(1, 142), (1, 137), (1, 128)]\n\n* Total number of pixels for each band in the image:\n    [65536, 65536, 65536]\n\n* Sum of all pixels for each band in the image:\n    [3093106.0, 3904015.0, 2879192.0]\n\n* Squared sum of all pixels for each band in the image:\n    [225487516.0, 325785395.0, 221232102.0]\n\n* Average (arithmetic mean) pixel level for each band in the image:\n    [47.197052001953125, 59.57054138183594, 43.9329833984375]\n\n* Median pixel level for each band in the image:\n    [53, 72, 54]\n\n* RMS (root-mean-square) for each band in the image:\n    [58.657194297640025, 70.50596160572695, 58.10106692885669]\n\n* Variance for each band in the image:\n    [1213.1047251960263, 1422.4412214232143, 1445.6269479840994]\n\n* Standard deviation for each band in the image:\n    [34.82965295830589, 37.71526509814314, 38.0214011838609]\n\n\n\n\n\nMerge two images using Image.paste\n\nim1 = im.crop((left, top, right, bottom))\nim1 = im1.resize( (300, 300))\nfrontImage = frontImage.convert(\"RGBA\")\nbackground.paste(frontImage, (width, height), frontImage)"
  },
  {
    "objectID": "tips/010_Python/pytest.html",
    "href": "tips/010_Python/pytest.html",
    "title": "PyTest",
    "section": "",
    "text": "!pip install pytest\n\nYou will need pytest.ini in the root:\n[pytest]\npythonpath = ''\npythonpath adds a path to sys.path relative to the root so the imports in test files work.\nTo discover and run all tests run in terminal:\npytest\nTo run only specific tests:\npytest filename.py\ndocs"
  },
  {
    "objectID": "tips/010_Python/regex.html",
    "href": "tips/010_Python/regex.html",
    "title": "Regex",
    "section": "",
    "text": "Suppose we need to extract row and col from a string ‘R1C2_row62_col24.png’:\n\nimport re\nsome_string = 'R1C2_row62_col24.png'\nm = re.match(r\".*row(\\d+)_col(\\d+).*.png\", some_string)\nrow = int(m.group(1))\ncol = int(m.group(2))\nassert row == 62\nassert col == 24\n\nor suppose we want to extract a string from a name ‘some_class_name_123.jpg’:\n\nimport re\nsome_string = 'some_class_name_123.jpg'\nm = re.match(r\"(.+)_\\d+.jpg\", some_string)\nassert m.group(1) == 'some_class_name'\n\nTest here https://regex101.com/\n\n\n\nCharacter\nAction\n\n\n\n\n.\nany character\n\n\n\\d\nonly numbers (if we need decimal point user a backslash (.)\n\n\n?\nZero or one character\n\n\n+\nOne or many\n\n\n*\nAny character count\n\n\n\nGood tutorial."
  },
  {
    "objectID": "tips/010_Python/fileio.html",
    "href": "tips/010_Python/fileio.html",
    "title": "FileIO",
    "section": "",
    "text": "To dump data to json file:\n\noutput_path = <some_dir> / \"some_file.json\"\nparams = {'a': 1, 'b': 2}\nwith output_path.open('w') as f:\n    json.dump(params, f, indent=4)  # `indent` writes each new input on a new line\n\nto load from json file:\n\nwith input_path.open() as f:\n    result = json.load(f)"
  },
  {
    "objectID": "tips/010_Python/fileio.html#shutil",
    "href": "tips/010_Python/fileio.html#shutil",
    "title": "FileIO",
    "section": "shutil",
    "text": "shutil\n\nshutil.copy(src_path, dst_path)\nfrom shutil import copyfile, rmtree, move"
  },
  {
    "objectID": "tips/010_Python/fileio.html#csv",
    "href": "tips/010_Python/fileio.html#csv",
    "title": "FileIO",
    "section": "CSV",
    "text": "CSV\n\nimport csv\n\n\ndef read_csv_file_into_dict(filename):\n    with open(filename, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=',')\n        x = {row[0]: row[1] for row in reader}\n    return x"
  },
  {
    "objectID": "tips/010_Python/fileio.html#readline-and-readlines",
    "href": "tips/010_Python/fileio.html#readline-and-readlines",
    "title": "FileIO",
    "section": "Readline and Readlines",
    "text": "Readline and Readlines\nwith open(f, \"r\") as f:\n    a = f.readlines()"
  },
  {
    "objectID": "tips/010_Python/fileio.html#find-and-delete-filesfolders",
    "href": "tips/010_Python/fileio.html#find-and-delete-filesfolders",
    "title": "FileIO",
    "section": "Find and delete files/folders",
    "text": "Find and delete files/folders\nfrom pathlib import Path\na = list(Path('path_to_the_folder').glob('**/some_file_name'))\nfor x in a:\n    x.unlink()\nTo delete a folder use shutil.rmtree:\nimport shutil\nshutil.rmtree(Path(\"a_directory\"))"
  },
  {
    "objectID": "tips/010_Python/numpy.html",
    "href": "tips/010_Python/numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "Excellent summary."
  },
  {
    "objectID": "tips/010_Python/numpy.html#broadcasting",
    "href": "tips/010_Python/numpy.html#broadcasting",
    "title": "Numpy",
    "section": "Broadcasting",
    "text": "Broadcasting\n\na = np.zeros([3,4])\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\nrows = np.array([0,2])\nvec = np.array([1,2,3]).reshape(1,-1).T\n\n\na[:,rows] += vec\na\n\narray([[1., 0., 1., 0.],\n       [2., 0., 2., 0.],\n       [3., 0., 3., 0.]])"
  },
  {
    "objectID": "tips/010_Python/setoperations.html",
    "href": "tips/010_Python/setoperations.html",
    "title": "Set operations",
    "section": "",
    "text": "a = set([1,2,3])\nb = set([1,2,4])\n\n\na | b  # union\n\n{1, 2, 3, 4}\n\n\n\na & b  # intersections\n\n{1, 2}\n\n\n\na ^ b  # symmetric intersection\n\n{3, 4}\n\n\n\na - b\n\n{3}\n\n\n\nb - a\n\n{4}"
  },
  {
    "objectID": "tips/010_Python/functools.html",
    "href": "tips/010_Python/functools.html",
    "title": "Functools",
    "section": "",
    "text": "from functools import partial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef general_quadratic_function(x: np.array, params: tuple):\n    a,b,c = params\n    return a*(x**2) + (b*x) + c\n\n\nour_quadratic_function = partial(general_quadratic_function, params=(1,2,3))\n\n\nour_quadratic_function(np.arange(10))\n\narray([  3,   6,  11,  18,  27,  38,  51,  66,  83, 102])"
  },
  {
    "objectID": "tips/010_Python/functools.html#plot-functions",
    "href": "tips/010_Python/functools.html#plot-functions",
    "title": "Functools",
    "section": "Plot functions",
    "text": "Plot functions\n\nnp.arange(0,10,0.1)\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n       1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n       5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4,\n       6.5, 6.6, 6.7, 6.8, 6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7,\n       7.8, 7.9, 8. , 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9. ,\n       9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9])\n\n\n\ndef plot_function(func, x=np.arange(0,10,0.1), ax=None, **kwargs):\n    if 'figsize' not in kwargs:\n        kwargs['figsize'] = (3,3)\n    if ax is None:\n        ax = plt.subplots(**kwargs)[1]\n\n    y = func(x)\n    ax.plot(x, y)\n\n\nplot_function(our_quadratic_function)"
  },
  {
    "objectID": "tips/010_Python/functools.html#typing",
    "href": "tips/010_Python/functools.html#typing",
    "title": "Functools",
    "section": "Typing",
    "text": "Typing\n\nfrom typing import List, TypeVar\nMyType = TypeVar('MyType', str, int )\n\ndef foo(a: List[int]):\n    print(a)"
  },
  {
    "objectID": "tips/010_Python/pathlib.html",
    "href": "tips/010_Python/pathlib.html",
    "title": "Pathlib",
    "section": "",
    "text": "from pathlib import Path\nassert Path(<some_file>).is_file()\nTo get the Path of the current file, or a parent:"
  },
  {
    "objectID": "tips/010_Python/pathlib.html#platform",
    "href": "tips/010_Python/pathlib.html#platform",
    "title": "Pathlib",
    "section": "Platform",
    "text": "Platform\nimport platform\nif 'macOS' in platform.platform():\n    print('This is Mac')\n\n<some_path>.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "tips/010_Python/visualizations.html",
    "href": "tips/010_Python/visualizations.html",
    "title": "Visualizations",
    "section": "",
    "text": "from pathlib import PosixPath\nimport ipyplot\nimport cv2\nfrom PIL import Image\nis_jupyter = get_ipython().__class__.__name__ == 'ZMQInteractiveShell'"
  },
  {
    "objectID": "tips/010_Python/visualizations.html#matplotlib.pyplot.axis.plot",
    "href": "tips/010_Python/visualizations.html#matplotlib.pyplot.axis.plot",
    "title": "Visualizations",
    "section": "Matplotlib.pyplot.axis.plot",
    "text": "Matplotlib.pyplot.axis.plot\nSee here."
  },
  {
    "objectID": "tips/010_Python/visualizations.html#plot-multiple-images",
    "href": "tips/010_Python/visualizations.html#plot-multiple-images",
    "title": "Visualizations",
    "section": "Plot multiple images",
    "text": "Plot multiple images\nWe can use ipyplot (pip install ipyplot), but it has annoying issue. Instead I made a custom one:\n\nfrom utils.plot import plot_pil_images\n\n\nplot_pil_images([im_pil, im_pil, im_pil.transpose(Image.ROTATE_90)], \n                ['orig', 'no change', 'transpose 90'])"
  },
  {
    "objectID": "tips/010_Python/general.html",
    "href": "tips/010_Python/general.html",
    "title": "Python General",
    "section": "",
    "text": "To see what modules have been imported use:\n\nimport sys\nmodulenames = set(sys.modules) & set(globals())\nallmodules = [sys.modules[name] for name in modulenames]\nallmodules\n\nTo see if a particular module is imported:\n\n[i for i, x in enumerate(allmodules) if \"'fastai'\" in str(x)] != []"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html",
    "href": "tips/030_Terminal/zsh.html",
    "title": "zsh",
    "section": "",
    "text": "There are many shells one can use, Mac default is zsh (located at /bin/zsh). I set up PyCharm to use it as well in the Preferences\\Tools\\Terminal\\Shell Path\nBash/zsh comment is the same as python: #\nTo open terminal from folder I added a shortcut: Control + Alt + Shift + T\nTo see hidden files on Mac in finder: Command + Shift + ."
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#ls-command",
    "href": "tips/030_Terminal/zsh.html#ls-command",
    "title": "zsh",
    "section": "ls command",
    "text": "ls command\nTo count files in a folder from terminal:\nls /etc | wc -l\nrecursively:\nfind <directory> -type f | wc -l\nfor example:\nls '/Users/nenad.bozinovic/work/Frame/elasticity_logs' | head -4\nto see file sizes:\nls -l\nls -l --block-size=M\nto see hiden files:\nls -a\nls -la\nTo create/delete directory:\nmkdir\nrm -r  # delete non empty directory\nrmdir # to delete empty directory\nrm -d  # to delete empty directory\n\nmv source dst # to move and/or rename directory"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#tree-command",
    "href": "tips/030_Terminal/zsh.html#tree-command",
    "title": "zsh",
    "section": "tree command",
    "text": "tree command\nTo see tree of directories use tree command:\ntree --help\nTo install on Linux:\n!sudo apt install tree\nto install on Mac:\nbrew install tree"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#zipunzip-gz-files",
    "href": "tips/030_Terminal/zsh.html#zipunzip-gz-files",
    "title": "zsh",
    "section": "Zip/Unzip gz files",
    "text": "Zip/Unzip gz files\nTo zip/unzip gz:\nInstall gnu-tar if seeing warning with the tar, then use gtar:\nbrew install gnu-tar\nTo zip:\ngtar -zcvf myfolder.tar.gz myfolder\ngzip filename  # zip it back\ngzip -k filename  # to zip it and keep original\nto see the content of a zipped file:\ngtar -tf myfolder.tar.gz\nto unzip:\ntar -xf labeled_data.tar.gz\ntar -xf labeled_data.tar.gz -C /home/user/destination"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#zipunzip-zip-files",
    "href": "tips/030_Terminal/zsh.html#zipunzip-zip-files",
    "title": "zsh",
    "section": "Zip/Unzip zip files",
    "text": "Zip/Unzip zip files\nzip filename.zip file1.txt file2.txt file3.txt\nunzip -vl titanic.zip  # to see content without unzipping\nunzip filename.zip\nunzip filename.zip -d /home/user/destination"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#download",
    "href": "tips/030_Terminal/zsh.html#download",
    "title": "zsh",
    "section": "Download",
    "text": "Download\nTo download a file from URL:\n!curl -OL <URL>"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#paths",
    "href": "tips/030_Terminal/zsh.html#paths",
    "title": "zsh",
    "section": "Paths",
    "text": "Paths\nPrint path:\necho \"${PATH//:/$'\\n'}\"\nTo add folder to PATH"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#homebrew",
    "href": "tips/030_Terminal/zsh.html#homebrew",
    "title": "zsh",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew is a package manager for macOS, it might have some unique packages that pip doesn’t have, to install wget for example:\nbrew install wget\nwget https://your.link.png"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#pip",
    "href": "tips/030_Terminal/zsh.html#pip",
    "title": "zsh",
    "section": "pip",
    "text": "pip\nTo install package:\npip install torch torchvision tensorboard\nWhen using [] with pip it is important to use \"\" to avoid shell parsing for example:\npip install \"mpl_interactions[jupyter]\"\nTo see version of the package installed:\npip show torch\nTo see all packages installed:\npip list"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#vim",
    "href": "tips/030_Terminal/zsh.html#vim",
    "title": "zsh",
    "section": "vim",
    "text": "vim\n\n\n\n\n\n\n\nCommand\nExplanation\n\n\n\n\ni\ninsert mode\n\n\nEsc\nexit insert mode\n\n\n:w\nSaves the file you are working on\n\n\n:w [filename]\nAllows you to save your file with the name you’ve defined\n\n\n:wq\nSave your file and close Vim\n\n\n:q!\nQuit without first saving the file you were working on"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#conda",
    "href": "tips/030_Terminal/zsh.html#conda",
    "title": "zsh",
    "section": "Conda",
    "text": "Conda\nyes | conda create -n $repo_name python=$python_ver  # notice there is no keyword env\nconda env list\nconda env remove -n ENV_NAME\nconda env export > env.yml\nconda env create -n ENVNAME --file ENV.yml\nconda list torch"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#if-statement-in-zsh",
    "href": "tips/030_Terminal/zsh.html#if-statement-in-zsh",
    "title": "zsh",
    "section": "if statement in zsh",
    "text": "if statement in zsh\nif ! [[ -n $repo_name ]] || ! [[ -n $python_ver ]]; then  \n  echo \"you didn't enter repo name and/or python version\"  \n  exit 0  \nfi"
  },
  {
    "objectID": "tips/030_Terminal/setupenv.html",
    "href": "tips/030_Terminal/setupenv.html",
    "title": "Set up env",
    "section": "",
    "text": "Update conda:\nconda update -n base -c conda-forge conda\nInit conda:\nconda init zsh  # Must restart terminal if running this one.\nCreate environment:\nyes | conda create -n repo_name python=python_ver\nconda activate repo_name\nInstall packages:\npip install -U numpy pandas matplotlib torchviz scikit-learn tensorboard torchvision torch tqdm torch-lr-finder ipyplot ipywidgets opencv-python\nyes | conda install -c conda-forge jupyter_contrib_nbextensions graphviz python-graphviz\nAdd a kernel:\nipython kernel install --name $repo_name --user\n\nUsing bash script\n\nIf making shell file make sure it has correct line endings for the system (for example in PyCharm go to File -> File Properties -> Line Separators)\n\nMake sure the file is executable:\nchmod +x utils/setup_new_project.sh check with ls -l\n./utils/setup_new_project.sh <repo_name> <python_version>\n# for example ./setup_new_project.sh test_repo 3.10\nTo stop execution after a failure use || exit, for example if the folder doesn’t exist following command will stop the script:\ncd $repo_name || exit"
  },
  {
    "objectID": "tips/050_FastAI/nbdev.html",
    "href": "tips/050_FastAI/nbdev.html",
    "title": "nbdev",
    "section": "",
    "text": "See main page here\nnbdev_help  # this list all commands \nTutorial says we might need to also run the following after export once: pip install -e . (-e installs a project in editable mode (i.e. setuptools “develop mode”) from a local project path (.))\n\n\n\n\n\n\n\nCommand\nAction\n\n\n\n\nnbdev_prepare\nuse it before commit and push, it will run all the code though\n\n\nnbdev_update\npropagates changes from python to jupyter\n\n\nnbdev_preview\nto see pages, and this one is live\n\n\nnbdev_export\nwill export all cells with #export flag into a filename defined in the top using #| default_exp filename\n\n\n\nOne can also export from jupyter notebook:\nfrom nbdev import nbdev_export\nnbdev_export()\nList of directives"
  },
  {
    "objectID": "tips/050_FastAI/huggingface.html",
    "href": "tips/050_FastAI/huggingface.html",
    "title": "HuggingFace",
    "section": "",
    "text": "Couldn’t upload large files without git-lfs (lfs stands for large file storage)\nbrew install git-lfs\ngit lfs install\nHugging space doesn’t allow binary files in repo, so the file extensions needs to be added to .gitattributes.\nWe must have requirements.txt file in the hugging space repo:\n\"If you need other Python packages to run your app, add them to a requirements.txt file at the root of the repository. The Spaces runtime engine will create a custom environment on-the-fly.\""
  },
  {
    "objectID": "tips/050_FastAI/azure.html",
    "href": "tips/050_FastAI/azure.html",
    "title": "Azure",
    "section": "",
    "text": "To see keys that one can use to run Bing API go here"
  },
  {
    "objectID": "tips/050_FastAI/fastai.html",
    "href": "tips/050_FastAI/fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "To install fastai project on any linux machine (more here, also see docs):\nsudo apt update && sudo apt -y install git\ngit clone https://github.com/fastai/fastsetup.git\ncd fastsetup\nsudo ./ubuntu-initial.sh\nwait a couple of minutes for reboot, then ssh back in:\nmamba install -c fastchan fastai\nconda install -c fastchan nbdev\nconda install -c anaconda pillow\nconda install jupyter\npip install gradio"
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html",
    "href": "tips/040_PyTorch/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "Jaccard Index (aka Intersection over Union, aka IoU)"
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html#binary",
    "href": "tips/040_PyTorch/metrics.html#binary",
    "title": "Metrics",
    "section": "Binary",
    "text": "Binary\n\ntarget = torch.tensor([[1, 1], [1, 0]])\npreds = torch.tensor([[1, 1], [0, 0]])\nmetric = BinaryJaccardIndex()\nmetric(preds, target)\n\ntensor(0.6667)"
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html#multiclass",
    "href": "tips/040_PyTorch/metrics.html#multiclass",
    "title": "Metrics",
    "section": "Multiclass",
    "text": "Multiclass\n\ntarget = torch.randint(0, 2, (10, 25, 25))\npred = target.clone()\npred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]\njaccard = JaccardIndex(task=\"multiclass\", num_classes=2)\njaccard(pred, target)\n\ntensor(0.9660)"
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html",
    "href": "tips/040_PyTorch/augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "Augmentations improve generalization of the model by using specified transformations during training. They do not increase the number of samples in the dataset, instead, they transform the samples during training, so with each epoch training sees augmented image. The rate of augmentation is controled by torchvision.transforms.RandomApply.\nAugmentations can be found in torchvision.transforms module, or in albumentations which claims to be fast."
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#brightness",
    "href": "tips/040_PyTorch/augmentations.html#brightness",
    "title": "Augmentations",
    "section": "Brightness",
    "text": "Brightness\n\ndef print_stats(im, aug_imgs):\n    \"\"\"\n    Print \n    \"\"\"\n    mean_orig, stdev_orig = Stat(im).mean, Stat(im).stddev\n    stats_mean = torch.zeros(len(aug_imgs), 3)\n    stats_stdev = torch.zeros(len(aug_imgs), 3)\n    for i, img in enumerate(aug_imgs):\n        stats_mean[i,:] = torch.tensor(Stat(img).mean) / torch.tensor(mean_orig)\n        stats_stdev[i,:] = torch.tensor(Stat(img).stddev) / torch.tensor(stdev_orig)\n    print(f'Brightness min/max: {stats_mean.min():.02f} / {stats_mean.max():.02f}')\n    print(f'Contrast min/max: {stats_stdev.min():.02f} / {stats_stdev.max():.02f}')\n\n\naug_imgs = [ColorJitter(brightness=(0.5, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.58 / 0.96\nContrast min/max: 0.58 / 0.97"
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#contrast",
    "href": "tips/040_PyTorch/augmentations.html#contrast",
    "title": "Augmentations",
    "section": "Contrast",
    "text": "Contrast\n\naug_imgs = [ColorJitter(contrast=(0.25, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.96 / 1.16\nContrast min/max: 0.30 / 0.99"
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#brightness-contrast",
    "href": "tips/040_PyTorch/augmentations.html#brightness-contrast",
    "title": "Augmentations",
    "section": "Brightness + contrast",
    "text": "Brightness + contrast\nTogether the have cummuliteve effect:\n\naug_imgs = [ColorJitter(brightness=(0.5, 1), contrast=(0.25, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.53 / 1.01\nContrast min/max: 0.18 / 0.86"
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html",
    "href": "tips/040_PyTorch/convolutions.html",
    "title": "Convolutions",
    "section": "",
    "text": "Code\nfrom torch.nn.functional import conv2d\nimport torch\nimport numpy as np\nNote: torch.nn.Conv2d is a Module that initializes all parameters i.e. kernel will be learned. Only functional form can take kernel as input."
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#conv2d",
    "href": "tips/040_PyTorch/convolutions.html#conv2d",
    "title": "Convolutions",
    "section": "Conv2d",
    "text": "Conv2d\nWe explore functional form of conv2d. Let’s make some inputs and kernels:\n\ntorch.manual_seed(14)\nx = torch.randn(16, 3, 16, 16)\nkernel = torch.randn(10, 3, 3, 3)\n\nbasic Conv2 does what we expect:\n\ny = conv2d(x, kernel)\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 14, 14])\n\n\nFormula for output size for Convolutions is: \\([(W−K+2P)/S]+1)\\), where [] is the np.floor:\n\ndef get_output_size(w,k,p,s):\n    return int(((w-k+2*p)/s)+1)\n\nIn above example:\n\nget_output_size(16, 3, 0, 1)\n\n14\n\n\nif we want the same size, we could pass padding='same':\n\ny = conv2d(x, kernel, padding='same')\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 16, 16])\n\n\nThis works ok for stride=1, but for stride >1 ‘same’ doesn’t make sense anymore in PyTorch (raises RuntimeError: padding='same' is not supported for strided convolutions). In TensorFlow however ‘same’ assumes one needs \\(width/stride\\) size, which in our case should be 8, instead we get expected 7:\n\ny = conv2d(x, kernel, stride=2)\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 7, 7])\n\n\nformula checks out:\n\nget_output_size(16, 3, 0, 2)\n\n7\n\n\nSo what’s the padding that we need? We need to do some math:\no = output size w = width p = padding s = stride if we solve the equation for p: o = int((w-k+2p)/s) + 1 o - 1 = int((w-k+2p)/s) and here we have a range: o - 1 = int((w-k+2p_min)/s) o - 1 = int((w-k+2p_max)/s) solving this: (o - 1) * s = min = w-k+2p_min <= w-k+2p <= w-k+2p_max = max < o  s Note that inequality is not symmetric, the top bound is exclusive. Then: p_min = ((o-1)s - w + k) / 2 p_max = (os - w + k) / 2 which can both be decimal. at this point we can just get the ceil of p_min and be happy: \\(p = np.ceil(((o-1)*s - w + k) / 2)\\)\n\ndef get_lowest_padding(w,k,s):\n    o = np.ceil(w/s)\n    p = (np.ceil((k - (w - (o-1)*s)) / 2)).astype(int)\n    return p\n\n\nget_lowest_padding(90,15,10)\n\n3\n\n\nIt is quite annoying that padding depends on the width of the input image (this is not always the case though), here is a re-write of the formula where width is defaulted:\n\ndef get_padding(kernel_size, stride, width=256):\n    \"\"\"\n    Padding that ensures the output_size of ceil(input_size/stride). Assuming square images:\n    \"\"\"\n    output_width = np.ceil(width / stride)\n    padding = int(np.ceil(((output_width-1) * stride - width + kernel_size) / 2))\n    return padding"
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#padding-visualizations",
    "href": "tips/040_PyTorch/convolutions.html#padding-visualizations",
    "title": "Convolutions",
    "section": "Padding visualizations",
    "text": "Padding visualizations\nLet’s plot padding for many image widths:\n\nimport nbimporter\nfrom Python.functools import partial, plot_function\n\n\npartial_get_padding = partial(get_lowest_padding, k=5, s=2)\nx = np.arange(200, 300, 1)\nplot_function(partial_get_padding, x)\n\n\n\n\n\nnp.unique(partial_get_padding(np.arange(200,300,1)))\n\narray([2])\n\n\n\npartial_get_padding = partial(get_lowest_padding, k=3, s=1)\nx = np.arange(200, 300, 1)\nplot_function(partial_get_padding, x)\n\n\n\n\nSo this is funny, seems to be independent of the width. Let’s look into math again:\no = can be even or odd p = (np.ceil((k - (1 or 2)) / 2)) # generaly speaking p = (np.ceil((k - (1 to s) / 2)) p = (np.ceil((5 - (1 or 2)) / 2)) p = (np.ceil((4 or 3)/ 2)) p = 2\nso that’s the reason.\nWe can also look into limits of p:\no = np.ceil(w/s) p = (np.ceil(((o-1)*s - w + k) / 2)) note that p is maximized when (o-1)*s - w is highest, i.e. w - (o-1)*s is lowest, which is has lowest value of 1 (by definition of o it can’t be 0): np.ceil((k-s)/2) <= p <= np.ceil((k-1)/2)\n\nprint(get_lowest_padding(90,16,10))  # np.ceil((k-s)/2) = (ceil((16-10)/2) = 3\nprint(get_lowest_padding(91,16,10))   # np.ceil((k-1)/2) = ceil(15/2) = 8\n\n3\n8\n\n\nFinally, for a special, but common case, w = s*o:\ngeneral equations: o = np.ceil(w/s) p = (np.ceil(((o-1)*s - w + k) / 2))\nthen: p = (np.ceil((k-s) / 2))\nso padding doesn’t depend on the width in this case. We will stick with this formula since it makes it simple not to worry about the width, so long our images are divisible by s."
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#conv2dtranspose",
    "href": "tips/040_PyTorch/convolutions.html#conv2dtranspose",
    "title": "Convolutions",
    "section": "Conv2dTranspose",
    "text": "Conv2dTranspose\nnn.ConvTranspose2d is used to upsample the input (for example in UNet). These have additional output_padding that fills only one-side (useful for those ‘same’ paddings). It is a non-symmetric padding of the output image that enables us to get an even size image."
  },
  {
    "objectID": "tips/040_PyTorch/models.html",
    "href": "tips/040_PyTorch/models.html",
    "title": "Models",
    "section": "",
    "text": "torch.nn.BatchNorm2d(\n_num_features_,\n_eps=1e-05_,\n_momentum=0.1_,\n_affine=True_,\n_track_running_stats=True_,\n_device=None_,\n_dtype=None_\n)\nOne must define all layers in the __init__ in order to initialize them. Remember that forward method will be called during training so no layers with parameters should be defined there.\n\nSoftmax\nFor multi-classifications, Softmax converts logits to predictions:\n\npredictions = nn.Softmax(dim=1)(logits)"
  },
  {
    "objectID": "tips/040_PyTorch/loss.html",
    "href": "tips/040_PyTorch/loss.html",
    "title": "Loss",
    "section": "",
    "text": "CrossEntropyloss\nIf logit shape is [N, C, d1, d2] (where N is the number of images and C is the number of classes to predict), then target (i.e. label) shape must be [N, d1, d2].\nThen the loss will be calculated as: nn.CrossEntropyLoss(weight)(logit, target)\nweight is a tensor for unbalanced datasets. Must be tensor.float.\nWhen using masking, one should use masked:\n\ndef masked_cross_entropy_loss_fn(y_pred, y_true):\n    out_of_bounds_mask = (y_true == out_of_bounds_value) # find out the out-of-bounds\n    return nn.CrossEntropyLoss(weight=weights)(\n        y_pred.masked_fill(out_of_bounds.unsqueeze(axis=1), 0), \n        y_true.masked_fill(out_of_bounds, 0)\n    )"
  },
  {
    "objectID": "tips/040_PyTorch/tensor.html",
    "href": "tips/040_PyTorch/tensor.html",
    "title": "Tensor",
    "section": "",
    "text": "import torch\nfrom torchvision.transforms import Normalize, ToPILImage, Resize\nfrom tips.plot import plot_pil_images"
  },
  {
    "objectID": "tips/040_PyTorch/tensor.html#cat-vs-stack",
    "href": "tips/040_PyTorch/tensor.html#cat-vs-stack",
    "title": "Tensor",
    "section": "cat vs stack",
    "text": "cat vs stack\n\nassert (torch.Tensor([1,2,3]) == torch.tensor([1,2,3])).all()\n\n\na = torch.Tensor([1])\nb = torch.Tensor([2])\ntorch.cat([a,b], dim=0)\n\ntensor([1., 2.])\n\n\n\ntorch.stack([a,b], dim=0)\n\ntensor([[1.],\n        [2.]])\n\n\n\ntorch.stack([a,b], dim=1)\n\ntensor([[1., 2.]])"
  },
  {
    "objectID": "tips/060_Misc/git.html",
    "href": "tips/060_Misc/git.html",
    "title": "Git",
    "section": "",
    "text": ".gitignore file (no extension) example:\n/data/*\n/.idea/*\n/.pytest_cache/*\n/.ipynb_checkpoints/*\n/pycode/.ipynb_checkpoints/*\nWhen pushing from a remote server, user is asked for username/password. Instead of a password, provide a token, that can be generated like this.\ngit clone\ngit fetch\ngit pull\ngit push\ngit commit\ngit add\ngit remote -v  # shows URL of a remote"
  },
  {
    "objectID": "tips/060_Misc/kaggle.html",
    "href": "tips/060_Misc/kaggle.html",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle -h # to see the help\nList the currently active competitions:\nkaggle competitions list\ndownload files associated with a competition:\nkaggle competitions download -c [COMPETITION]\nmake a competition submission:\nkaggle competitions submit -c [COMPETITION] -f [FILE] -m [MESSAGE]"
  },
  {
    "objectID": "tips/060_Misc/latex.html",
    "href": "tips/060_Misc/latex.html",
    "title": "Latex",
    "section": "",
    "text": "New line \\\\\nURL link \\href{link}{label}\nTo have hyperlinks available one must have proper library installed (use external TypeSetter like MacTeX). Also have this line in a template file \\usepackage[hidelinks, linkcolor=blue]{hyperref} to hide the default boxes around links in PDFs."
  },
  {
    "objectID": "tips/060_Misc/paperspace.html",
    "href": "tips/060_Misc/paperspace.html",
    "title": "Paperspace.com",
    "section": "",
    "text": "Paperspace offers very affordable remote machines with GPU, and allows for 150Gb permanent storage (that is mounted as /notebook folder). I find it easier to set up then any other platform. It has many pre-installed deployments like PyTorch, FastAI, Stable Diffusion, and more. It does require monthly subscription that is at the time of writting $8 per month.\n\nGradient ML platform\nFor most of my needs I use free machines on their Gradient ML platform:\n\n\n\nimage.png\n\n\nDon’t forget to set auto-shutdown time depending on you need, 6 hours is the max:\n\n\n\nimage.png\n\n\nOnce the machine is running, the VSCode icon allows to get the URL of the kernel:\n \n  Copy URL and on the local machine in VSCode navigate to: “Select Kernel”->“Select another Kernel”->“Existing Jupyter Kernel”->“Enter the URL of the running Jupyter server”->“Python3 (ipykernel)”:\n \nThat’s pretty much it; the kernel is now set in the upper right corner of the VSCode:\n\n\n\nimage.png\n\n\nLink describes this too.\nThis is great for majority of my needs, for longer durations one can always restart the machine.\nNote again that only files in the /notebook folder are persistent.\n\n\nCore machines\nCore machines are virtual machines that one can SSH into. More on setting SSH connection in VSCode can be found here.\n \nOne must install Python as a VSCode extension:\n\n\n\nimage.png\n\n\nFor setting up the environment see here."
  },
  {
    "objectID": "tips/060_Misc/statistics.html",
    "href": "tips/060_Misc/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "import matplotlib.pyplot as plt"
  },
  {
    "objectID": "tips/060_Misc/statistics.html#correlation",
    "href": "tips/060_Misc/statistics.html#correlation",
    "title": "Statistics",
    "section": "Correlation",
    "text": "Correlation\nPaerson coefficient between columns is a way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables: \\(\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{(X,Y)}{\\sigma_X \\sigma_Y}\\)\n\n\n\nimage.png"
  },
  {
    "objectID": "tips/060_Misc/macos.html",
    "href": "tips/060_Misc/macos.html",
    "title": "MacOS",
    "section": "",
    "text": "To see hidden files on Mac in finder: `Command + Shift + .\nHome/End: Fn + Left/Right Arrow Beggining/End of the file: Fn + Cmd + Left/Right"
  },
  {
    "objectID": "tips/060_Misc/customdomain.html",
    "href": "tips/060_Misc/customdomain.html",
    "title": "Custom domain",
    "section": "",
    "text": "To have custom domain to host github pages follow this stackoverflow post carefullly. It is important to have a file CNAME (that is a simple text file that contains a custom domain) in the same folder with _quarto.yml.\nI’m using google hosting services (GSH) for my website www.nenadbozinovic.com. GHS can be accessed here: https://admin.google.com/.\nTo enable comments on a website I’m using gitcus."
  },
  {
    "objectID": "tips/060_Misc/jarvislabsai.html",
    "href": "tips/060_Misc/jarvislabsai.html",
    "title": "Jarvislabs.ai",
    "section": "",
    "text": "At the time of the writing (3/10/2023) there are only servers in India, and my experience was that the response time in US is slow. Beside that immpediement, it’s an ok service. I still very much prefer to use Paperspace."
  },
  {
    "objectID": "tips/060_Misc/jarvislabsai.html#setup",
    "href": "tips/060_Misc/jarvislabsai.html#setup",
    "title": "Jarvislabs.ai",
    "section": "Setup",
    "text": "Setup\nHow to set up an environment link. Make sure to add a prefix otherwise environment will be deleted after pausing.\nconda create --prefix /home/some_env_name python=3.10 ipykernel mamba -y\nconda activate /home/some_env_name\nSet up a jupyter kernel:\npython -m ipykernel install --user --name=some_env_name"
  },
  {
    "objectID": "tips/020_Jupyter/images.html",
    "href": "tips/020_Jupyter/images.html",
    "title": "Images",
    "section": "",
    "text": "To show images in markdown cells:\n![](path to the image)\nfor example:\n![](assets/example.png)\nwill produce:\n\nOne can also use HTML to show images (src) with lot’s of flexibility: - to change the width add width (as string) - for reference add id\n<img width=\"500\" caption=\"Session_rate\" src=\"images/session_rate.jpg\" id=\"figure_id\"/>\nthen refer to the figure as:\n[name to show](#figure_id)\nalso can refer to URL images: (for example in Google Colab we have to replace the reference by it’s github link):\n<img width=\"500\" src=\"https://raw.githubusercontent.com/nesaboz/pytorched/8c41b15364234c31af4f6222b2251954131b4a94/nbs/images/transfer_learning_layer_replacement.jpg\"/>\n\nSee this great markdown guide for more."
  },
  {
    "objectID": "tips/020_Jupyter/matplotlib.html",
    "href": "tips/020_Jupyter/matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n\nnp.random.randn()\n\n-2.5106811179598463\n\n\n\nx = np.random.rand(10,1)\ny = np.random.rand(10,1)\nplt.plot(x,y,'.')\nplt.show()\n\n\n\n\n\ndef plot_means_and_stdevs(x1, y1, x2, y2):\n    fig, axs = plt.subplots(1,3, figsize=(10,3))\n    for i in range(3):\n        axs[i].plot(x1[:,i], y1[:,i], '.')\n        axs[i].plot(x2[:,i], y2[:,i], '.')\n        axs[i].set_xlabel('means')\n        axs[i].set_ylabel('stdev')\n        axs[i].set_title(f'Channel #{i+1}')\n        axs[i].legend(['eval', 'train'])\n    plt.show()\n\n\nx1 = np.random.rand(10,3)\ny1 = np.random.rand(10,3)\nx2 = np.random.rand(10,3)\ny2 = np.random.rand(10,3)\nplot_means_and_stdevs(x1, y1, x2, y2)\n\n\n\n\nA somewhat quicker method without using axes and figsize is to use plt.subplot (note that index is 1-based):\n\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.plot(x1[:,i], y1[:,i], '.')\n    plt.plot(x2[:,i], y2[:,i], '.')\n    plt.xlabel('means')\n    plt.ylabel('stdev')\n    plt.title(f'Channel #{i+1}')\n    plt.legend(['eval', 'train'])\nplt.show()\n\n\n\n\nTo remove axes use:\n\nplt.axis('off')\n\n(0.0, 1.0, 0.0, 1.0)\n\n\n\n\n\nTo save image:\n\nplt.savefig(\"test.png\", bbox_inches='tight')\n\n<Figure size 640x480 with 0 Axes>"
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html",
    "href": "tips/020_Jupyter/kernels.html",
    "title": "Kernels",
    "section": "",
    "text": "To add existing environment as a jupyter kernel:\nList kernels\nRemove kernel\nInstall new kernel"
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#add-default-paths-to-sys.path",
    "href": "tips/020_Jupyter/kernels.html#add-default-paths-to-sys.path",
    "title": "Kernels",
    "section": "Add default path(s) to sys.path",
    "text": "Add default path(s) to sys.path\nTo make sure path is always available in the jupyter kernel do the following (taken from here):\nipython profile create\nipython locate\n/Users/username/.ipython\ncd /Users/username/.ipython\nvi profile_default/ipython_config.py\nadd these lines (use i to enter insert mode and Esc to exit, save and quit as :wq:\nc.InteractiveShellApp.exec_lines = [\n    'import sys; sys.path.append(\"/path/to/your/module\")'\n]\nsys.path can be different in python env and jupyter kernel. The solution that worked is here, in short, one has to compare the sys.executable for both the environment AND jupyter kernel (this one is listed using jupyter kernelspec list and looking into relevant kernel.json file) a must-read that I still need to go over is here here."
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#change-the-default-kernel",
    "href": "tips/020_Jupyter/kernels.html#change-the-default-kernel",
    "title": "Kernels",
    "section": "Change the default kernel",
    "text": "Change the default kernel\njupyter notebook --generate-config\nIn jupyter config file modify and uncomment following line: c.MultiKernelManager.default_kernel_name='newDefault' \\n\""
  },
  {
    "objectID": "tips/020_Jupyter/general.html",
    "href": "tips/020_Jupyter/general.html",
    "title": "Jupyter General",
    "section": "",
    "text": "To see parameters: SHIFT + TAB For help use ? before or after the command. For autocompletion: TAB + TAB Merge cells, select cells then: Shift + M `To see all available functions in numpy that havesinin their name:np.sin?`"
  },
  {
    "objectID": "tips/020_Jupyter/general.html#magic-commands",
    "href": "tips/020_Jupyter/general.html#magic-commands",
    "title": "Jupyter General",
    "section": "Magic commands",
    "text": "Magic commands\nTo autoreload edited python files (doesn’t include class signatures changes, these you have to re-import):\n%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "mini-projects/14_multi_label_classification/multi_label_classification.html",
    "href": "mini-projects/14_multi_label_classification/multi_label_classification.html",
    "title": "Multi-label classification",
    "section": "",
    "text": "Multi-label classification is a classification task where each sample can be assigned to multiple classes. This is different from multi-class classification where each sample can only be assigned to one class. Here I’ll follow FastAI’s book, chapeter 6.\nFirst let’s get the dataset:\n\nfrom fastai.vision.all import *\nfrom tips.plot import plot_pil_images\nfrom nbdev.showdoc import *\n\n\npath = untar_data(URLs.PASCAL_2007)\n\nLet’s take a look at the data, this dataset has a csv file with the info:\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nso we have filename, label (which is in fact multi-label) and is_valid (True if we want it in validation set, probably useful for bencharmarking since this info is not necceseary).\nThe dataset also has segmentation masks (for 422 out of 5011 images). No need for that now but good to know.\n\nDataBlock preparation\nDataBlock is fastai type that is used to create fastais Datasets and DataLoaders (which themselves are wrappers for PyTorch Dataset and DataLoader). Let’s get to know it:\n\ndata_block = DataBlock()\n\nOne can create datasets from pandas dataframe:\n\ndatasets = data_block.datasets(df)\n\n\ntype(datasets)\n\nfastai.data.core.Datasets\n\n\nDatasets have train and valid parameters:\n\ndatasets.train[0]\n\n(fname       009464.jpg\n labels             cat\n is_valid          True\n Name: 4754, dtype: object,\n fname       009464.jpg\n labels             cat\n is_valid          True\n Name: 4754, dtype: object)\n\n\nbut a text format is not useful for training. We can provide a get_x and get_y functions to DataBlock to get the data and labels:\n\ndef get_x(r):\n    return path / 'train' / r['fname']\ndef get_y(r):\n    return r['labels'].split()\ndatablock = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       get_x=get_x, get_y=get_y)\ndatasets = datablock.datasets(df)\n\n\ndatasets.train[0]\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n                      0., 0., 0., 0., 0.]))\n\n\nThis looks better, since now we have image and a MultiCategory label. It is one hot encoded since we can have more then 1. or 0 for that matter. Let’s also use the splitter that will use the is_valid column:\n\ndef splitter(df):\n    train = L(df.index[~df['is_valid']].tolist())\n    valid = L(df.index[df['is_valid']].tolist())\n    return train, valid\n\n\ndef get_x(r):\n    return path / 'train' / r['fname']\ndef get_y(r):\n    return r['labels'].split()\ndata_block = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       splitter=splitter,\n                       get_x=get_x, get_y=get_y)\ndatasets = data_block.datasets(df)\ndatasets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0.]))\n\n\nWe used splitter as a method, previously we used RandomSplitter()and dozen more (see doc below):\n\ndoc(RandomSplitter)\n\n\nRandomSplitter\nRandomSplitter(valid_pct=0.2, seed=None)Create function that splits `items` between train/val with `valid_pct` randomly.\nShow in docs\n\n\nWe can get vocabulary of all classes (there are 20):\n\nprint(datasets.vocab)\nprint(len(datasets.vocab))\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n20\n\n\nFor example, the 0th training sample looks like this:\n\ndatasets.train[0][0]\n\n\n\n\n\nidxs = datasets.train[0][1] == 1\nprint(idxs)\ndatasets.vocab[idxs]\n\nTensorMultiCategory([False, False, False, False, False, False,  True, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False])\n\n\n(#1) ['car']\n\n\nso far so good. We can also use torch.where:\n\nidxs = torch.where(datasets.train[0][1] == 1.0)[0]\nprint(idxs)\ndatasets.vocab[idxs]\n\nTensorMultiCategory([6])\n\n\n(#1) ['car']\n\n\nWe need to resize all the images to the same size, PyTorch can’t store tensors otherwise, the min_scale\n\ndatablock = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       get_x=get_x, get_y=get_y,\n                       splitter=splitter,\n                       item_tfms=RandomResizedCrop(128, min_scale=0.35))\ndatasets = datablock.datasets(df)\n\nfinally, let’s create fastai DataLoaders as well (won’t shuffle just yet):\n\ndls = datablock.dataloaders(df, shuffle=False)\n\n\ndls.show_batch(nrows=1, ncols=3)  # with min_scale = 0.35\n\n\n\n\nmin_scale controls the zoom, too low number (like 0.1) zooms and threatens to crop too much. Let’s stick with 0.35.\n\n\nTraining\nFirst we define the model with dataloaders and model (optimizer and loss will be assigned automatically for now):\n\nlearn = vision_learner(dls, resnet50)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\nThe warnings are becuase vision_learner still uses 0.13 version of torchvision. Not a big deal for now.\n\n!pip show torchvision\n\nName: torchvision\nVersion: 0.13.0+cu116\nSummary: image and video datasets and models for torch deep learning\nHome-page: https://github.com/pytorch/vision\nAuthor: PyTorch Core Team\nAuthor-email: soumith@pytorch.org\nLicense: BSD\nLocation: /usr/local/lib/python3.9/dist-packages\nRequires: numpy, pillow, requests, torch, typing-extensions\nRequired-by: fastai, sentence-transformers\n\n\nI have to send the model to the device not sure why is this not automatic as with dataloaders:\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nlearn.model = learn.model.to(device)\n\nLet’s first see what we get as an output after predicitons:\n\nx,y = dls.train.one_batch()\nactivations = learn.model(x)\nactivations.shape\n\ntorch.Size([64, 20])\n\n\nThis makes sense, each item in a batch will predict a one-hot list of 20 classes. So waht loss type should we use? The answer is BinaryCrossEntropy, and that’s the one fastai assigns automatically:\n\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\nBCEWithLogitsLoss is used since there is last layer in resnet18 is not sigmoid but linear layer with 20 outputs:\n\nlearn.model[-1][-1]\n\nLinear(in_features=512, out_features=20, bias=False)\n\n\nLet’s also add a metric, accuracy_multi is the one we need and we should pass sigmoid=True since we want to convert logits to probabilities and let’s have lower threshold:\n\naccuracy_multi??\n\nSignature: accuracy_multi(inp, targ, thresh=0.5, sigmoid=True)\nSource:   \ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    inp,targ = flatten_check(inp,targ)\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/metrics.py\nType:      function\n\n\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\n\n\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=3)\n\nVSCode doesn’t show the progress bar sometimes unfortunately.\nLet’s get the predictions:\n\npreds, targets = learn.get_preds()\n\n\n\n\n\n\n    \n      \n      0.00% [0/40 00:00<?]\n    \n    \n\n\n\npreds.shape\n\ntorch.Size([2510, 20])\n\n\n\ntargets.shape\n\ntorch.Size([2510, 20])\n\n\nwe can now plot accuracy for differnt thresholds (we use sigmoid=False since we already got probabilities via get_preds):\n\nxs = torch.linspace(0.05, 0.95, 30)\naccs = [accuracy_multi(preds, targets, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\nSo the max accuracy is:\n\nmax_idx, max_val = max(enumerate([float(x) for x in accs]), key=lambda x: x[1])\nxs[max_idx], max_val\n\n(tensor(0.4845), 0.9633266925811768)\n\n\nfor threshold=0.48.\nAnd this is it for multi-label classification."
  },
  {
    "objectID": "mini-projects/2_torchvision_conversions_samplers.html",
    "href": "mini-projects/2_torchvision_conversions_samplers.html",
    "title": "Torchvision, Conversions, Samplers",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing."
  },
  {
    "objectID": "mini-projects/2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "href": "mini-projects/2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on NCHW vs NHWC formats",
    "text": "Note on NCHW vs NHWC formats\nThe format for images is different between libraries: - PyTorch uses NCHW - TensorFlow uses NHWC - PIL/Matplotlib images are HWC.\n\nimage_r  = np.zeros((5, 5), dtype=np.uint8)\nimage_r[:, 0] = 255\nimage_r[:, 1] = 128\n\nimage_g = np.zeros((5, 5), dtype=np.uint8)\nimage_g[:, 1] = 128\nimage_g[:, 2] = 255\nimage_g[:, 3] = 128\n\nimage_b = np.zeros((5, 5), dtype=np.uint8)\nimage_b[:, 3] = 128\nimage_b[:, 4] = 255\n\n\n\nCode\nshow_image(image_r, 'gray')\nzeros = np.zeros((5,5), dtype=np.uint8)\nstacked_red = np.zeros((5,5,3), dtype=np.uint8)  # the format is HxWxX (height X width X channel)\nstacked_red[:,:,0] = image_r  \n# also same thing can be achieved stacked_red = np.stack([image_r, zeros, zeros], axis=2)\nshow_image(stacked_red)\n\n\n\n\nCode\ndef image_channels(red, green, blue, rgb, gray, rows=(0, 1, 2)):\n    fig, axs = plt.subplots(len(rows), 4, figsize=(15, 5.5))\n\n    zeros = np.zeros((5, 5), dtype=np.uint8)\n\n    titles1 = ['Red', 'Green', 'Blue', 'Grayscale Image']\n    titles0 = ['image_r', 'image_g', 'image_b', 'image_gray']\n    titles2 = ['as first channel', 'as second channel', 'as third channel', 'RGB Image']\n\n    idx0 = np.argmax(np.array(rows) == 0)\n    idx1 = np.argmax(np.array(rows) == 1)\n    idx2 = np.argmax(np.array(rows) == 2)\n    \n    for i, m in enumerate([red, green, blue, gray]):\n        if 0 in rows:\n            axs[idx0, i].axis('off')\n            axs[idx0, i].invert_yaxis()\n            if (1 in rows) or (i < 3):\n                axs[idx0, i].text(0.15, 0.25, str(m.astype(np.uint8)), verticalalignment='top')    \n                axs[idx0, i].set_title(titles0[i], fontsize=16)\n\n        if 1 in rows:\n            axs[idx1, i].set_title(titles1[i], fontsize=16)\n            axs[idx1, i].set_xlabel('5x5', fontsize=14)\n            axs[idx1, i].imshow(m, cmap=plt.cm.gray)\n\n        if 2 in rows:\n            axs[idx2, i].set_title(titles2[i], fontsize=16)\n            axs[idx2, i].set_xlabel(f'5x5x3 - {titles1[i][0]} only', fontsize=14)\n            if i < 3:\n                stacked = [zeros] * 3\n                stacked[i] = m\n                axs[idx2, i].imshow(np.stack(stacked, axis=2))\n            else:\n                axs[idx2, i].imshow(rgb)\n\n        for r in [1, 2]:\n            if r in rows:\n                idx = idx1 if r == 1 else idx2\n                axs[idx, i].set_xticks([])\n                axs[idx, i].set_yticks([])\n                for k, v in axs[idx, i].spines.items():\n                    v.set_color('black')\n                    v.set_linewidth(.8)\n\n    if 1 in rows:\n        axs[idx1, 0].set_ylabel('Single\\nChannel\\n(grayscale)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx1, 3].set_xlabel('5x5 = 0.21R + 0.72G + 0.07B')\n    if 2 in rows:\n        axs[idx2, 0].set_ylabel('Three\\nChannels\\n(color)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx2, 3].set_xlabel('5x5x3 = (R, G, B) stacked')\n    fig.tight_layout()\n    return fig\n\n\n\nimage_gray = .2126*image_r + .7152*image_g + .0722*image_b\nimage_rgb = np.stack([image_r, image_g, image_b], axis=2)\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 1))\n\n\n\n\n\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 2))"
  },
  {
    "objectID": "mini-projects/2_torchvision_conversions_samplers.html#note-on-torchvision",
    "href": "mini-projects/2_torchvision_conversions_samplers.html#note-on-torchvision",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on Torchvision",
    "text": "Note on Torchvision\ntorchvision has many existing Datasets, Models, and Transformations.\n\nConversion between ndarray, PIL, and tensor\n\nimages.shape\n\n(300, 1, 5, 5)\n\n\n\nexample_chw = images[3]\nexample_chw\n\narray([[[  0,   0, 255,   0,   0],\n        [  0, 255,   0,   0,   0],\n        [255,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0]]], dtype=uint8)\n\n\n\nexample_chw.shape\n\n(1, 5, 5)\n\n\n\nexample_hwc = np.transpose(example_chw, (1,2,0))\nexample_hwc.shape\n\n(5, 5, 1)\n\n\n\nshow_image(example_hwc, 'gray')\n\n\n\n\nToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\ntorch.as_tensor can work on N-dimensional arrays (ToTensor works only on 2D/3D images), but doesn’t apply scalling and, just fyi, it shares data i.e. doesn’t make a copy.\n\ntensorizer = ToTensor()\nexample_tensor = tensorizer(example_hwc)\nexample_tensor.shape\n\ntorch.Size([1, 5, 5])\n\n\n\nexample_tensor\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\n\n\n\ntype(example_tensor)\n\ntorch.Tensor\n\n\n\nexample_pil = ToPILImage()(example_tensor)  # similar to np.transpose(example_tensor, (1,2,0))\nprint(type(example_pil))\n\n<class 'PIL.Image.Image'>\n\n\n\nshow_image(example_pil, 'gray')\n\n\n\n\nTo convert between Tensor and Numpy:\n\nexample_np = example_tensor.detach().cpu().numpy()\nprint(type(example_np))\n\n<class 'numpy.ndarray'>\n\n\nTo convert between numpy and PIL:\n\nexample_np = np.array(example_pil)\nprint(type(example_np))\n\n<class 'numpy.ndarray'>\n\n\n\nexample_pil_back = Image.fromarray(example_np)\nprint(type(example_pil_back))\n\n<class 'PIL.Image.Image'>\n\n\nmore options for different PIL modes: (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1).\nOne has to be careful: - PIL gray images (L mode) need to be squeezed from 3D to 2D via np.squeeze() - RGB mode images should be floats between [0,1] - L mode (grayscale) images should be np.uint8 from [0,255] - 1 mode (bool) images are True or False.\n\n# PIL_image = Image.fromarray(np.uint8(example_np)).convert('RGB')\n# PIL_image = Image.fromarray(example_np.astype('uint8'), 'RGB')\n\n\nshow_image(example_pil_back, 'gray')\n\n\n\n\n\n\nTransformations\nThere are many transformations that can be done on tensors and/or PIL images:\n\nflipper = RandomHorizontalFlip(p=1.0)\nshow_image(flipper(example_pil), 'gray')\n\n\n\n\n\nprint(example_tensor)\nnormalizer = Normalize(mean=.5, std=.5)\nnormalized_tensor = normalizer(example_tensor)\nprint(normalized_tensor)\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1.,  1., -1., -1., -1.],\n         [ 1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\n\nComposing transforms\n\ncomposer = Compose([RandomHorizontalFlip(p=1.0),\n                    Normalize(mean=(.5,), std=(.5,))])\ncomposer(example_tensor)\n\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1., -1., -1.,  1., -1.],\n         [-1., -1., -1., -1.,  1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\ntorch.as_tensor(example_np/255).float()\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nIf we want to convert the whole numpy array to tensor we can use:\n\nexample_tensor = torch.as_tensor(example_np / 255).float()\nexample_tensor\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nPyTorch default floating point dtype is torch.float32, one can change this if needed via torch.set_default_dtype(torch.float64)."
  },
  {
    "objectID": "mini-projects/2_torchvision_conversions_samplers.html#samplers",
    "href": "mini-projects/2_torchvision_conversions_samplers.html#samplers",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Samplers",
    "text": "Samplers\nOne can also define a Sampler (and it’s subclasses: SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler, and DistributedSampler), use WeightedRandomSampler for example in case data is unbalanced.\n\ncomposer = Compose([RandomHorizontalFlip(p=0.5),\n                    Normalize(mean=.5, std=.5)])\n\ndataset = TransformedTensorDataset(x_tensor, y_tensor, transform=composer)\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=dataset, batch_size=16, sampler=val_sampler)\n\nNote that we need val_sampler just because we are passing the full dataset.\nAlso note that when using Samplers, one can’t use shuffle=True in DataLoaders.\n\nlen(iter(train_loader)), len(iter(val_loader))\n\n(15, 4)"
  },
  {
    "objectID": "mini-projects/2_torchvision_conversions_samplers.html#some-model-methods",
    "href": "mini-projects/2_torchvision_conversions_samplers.html#some-model-methods",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Some Model methods",
    "text": "Some Model methods\nTo see the layers:\n\nsbs_deeper.model\n\nSequential(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear1): Linear(in_features=25, out_features=10, bias=True)\n  (relu): ReLU()\n  (linear2): Linear(in_features=10, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)\n\n\n\nlist(sbs_deeper.model.named_modules())\n\n[('',\n  Sequential(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (linear1): Linear(in_features=25, out_features=10, bias=True)\n    (relu): ReLU()\n    (linear2): Linear(in_features=10, out_features=1, bias=True)\n    (sigmoid): Sigmoid()\n  )),\n ('flatten', Flatten(start_dim=1, end_dim=-1)),\n ('linear1', Linear(in_features=25, out_features=10, bias=True)),\n ('relu', ReLU()),\n ('linear2', Linear(in_features=10, out_features=1, bias=True)),\n ('sigmoid', Sigmoid())]\n\n\nTo see particular weights in a layer:\n\nsbs_deeper.model.linear2.weight.detach()\n\ntensor([[ 0.1024,  0.2038,  0.2085, -0.2298,  0.1973, -0.2550,  0.2233, -0.0626,\n         -0.0030,  0.0018]])\n\n\n\nsbs_deeper.count_parameters()\n\n271\n\n\nThe 271 comes from: (25 * 10 + 10 biases) + (10 * 1 + 1 bias) = 260 + 11 = 271\nto reset parameters I implemented this way but this doesn’t reset all parameters (for example in nn.PReLU) so use caution!\n\nsbs_deeper.reset_parameters()"
  },
  {
    "objectID": "mini-projects/labeling.html",
    "href": "mini-projects/labeling.html",
    "title": "Labeling",
    "section": "",
    "text": "from PIL import Image, ImageDraw\nfrom pathlib import Path\nimport json"
  },
  {
    "objectID": "mini-projects/labeling.html#labelme",
    "href": "mini-projects/labeling.html#labelme",
    "title": "Labeling",
    "section": "LabelMe",
    "text": "LabelMe\nLabelMe is simple-to-use GUI for labeling. It is straightforward to use (pip install labelme, run in terminal as labelme):\n\n\n\nlabelme\n\n\nThe segmentation points will be saved as json files:\n{\n  \"version\": \"5.1.1\",  \n  \"flags\": {},  \n  \"shapes\": [\n    {\n      \"label\": \"1\",\n      \"points\": [\n        [\n          4.029411764705877,\n          1.0882352941176439\n        ],\n        [\n          0.7941176470588189,\n          53.73529411764706\n        ],\n        [\n          0.7941176470588189,\n          221.97058823529412\n        ],\n        [\n          12.852941176470587,\n          1.3823529411764672\n        ]\n      ],\n      \"group_id\": null,\n      \"shape_type\": \"polygon\",\n      \"flags\": {}\n    },\n    ...\nLet’s load some image and generated json file:\n\nim = Image.open(\"images/solar_panels.png\")\nwidth, height = im.size\nprint(width, height)\nim\n\n256 256\n\n\n\n\n\nLet’s convert the json into a mask:\n\nwith open('images/solar_panels.json') as f:\n    data = json.load(f)\n\nThere are:\n\npoints = data['shapes']\nlen(points)\n\n20\n\n\n20 data points, let’s make a mask out of those:\n\nmask = Image.new('L', (width, height), 0)\nfor group in points:\n    label_class = group['label']\n    polygon = group['points']\n    polygon = [(round(x), round(y)) for x,y in polygon]\n    ImageDraw.Draw(mask).polygon(polygon, outline=255, fill=255)\nmask\n\n\n\n\nand save this as a file:\n\nmask.save('images/solar_panels_mask.png')\n\n\nassert Path('images/solar_panels_mask.png').is_file()"
  },
  {
    "objectID": "mini-projects/10_kubernets/kubernets.html",
    "href": "mini-projects/10_kubernets/kubernets.html",
    "title": "Kubernets",
    "section": "",
    "text": "The goal is to host cloud-native application that must:\n\nScale on demand\nSelf-heal\nSupport zero-downtime rolling updates\nRun anywhere (in cloud or non-cloud, like Raspberry PI).\n\nKubernets allows all of this. It is an orchestrator that bring together a set of microservices and organizes them into an application that brings value.\nKubernets emerged in 2014 (from Google) as a way to abstract underlying cloud and server infrastructure. It commoditized infrastructure making and becoming “OS of the cloud”.\n\nA Kubernetes cluster consists of one or more machines that have Kubernetes installed on them.\nMachines in a Kubernetes cluster are referred to as Nodes, but there are Master Nodes (Masters) and Worker Nodes (Nodes).\nWorker nodes run user applications and can either be Linux or Windows Nodes.\nAll Nodes run two main services: kubelet and Container runtime.\nThe kubelet is the main Kubernetes agent. It joins the Node to the cluster and communicates with the control plane, in charge of notifying when tasks are received and reporting on the status of those tasks.\nThe container runtime starts and stops containers.\nHosted Kubernetes is where your cloud provider rents you a Kubernetes cluster. Sometimes, it is called Kubernetes as a Service, where we don’t have to worry about Masters, just Nodes. There are many hosted Kubernets services: AWS: Elastic Kubernetes Service (EKS), Azure: Azure Kubernetes Service (AKS), DO: Digital Ocean Kubernetes Service (DOKS), GCP: Google Kubernetes Engine (GKE), Linode: Linode Kubernetes Engine (LKE). I’ll use LKE here.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nContainerize an App (via Docker)\nFirst we want our App to be containerized. It has a few files:\n\ncd /Users/nenad.bozinovic/Work/blog/nbs/mini-projects/10_kubernets/usercode\n\n/Users/nenad.bozinovic/Work/blog/nbs/mini-projects/10_kubernets/usercode\n\n\n\n!ls App\n\nDockerfile    app.js        bootstrap.css package.json  views\n\n\n\nDockerfile: This file is not part of the application. It contains a list of instructions that Docker executes to create the container image (i.e. containerize the application).\napp.js: This is the main application file. It is a Node.js application.\nbootstrap.css: This is a stylesheet template, which determines how the application’s web page will look.\npackage.json: This lists the application dependencies.\nviews: This is a folder that contains the HTML used to populate the application’s web page.\n\nDocker file contains following:\nFROM node:current-slim\nCOPY . /src\nRUN cd /src; npm install\nEXPOSE 8080\nCMD cd /src && node ./app.js\n\n!docker --version\n\nDocker version 20.10.23, build 7155243\n\n\nWe’ll now build two docker images called qsk-course version 1.0 and 1.1 under my local account (nesaboz):\n\n#!docker image build -t nesaboz/qsk-course:1.0 App/.\n\nModify the App to create a new version and build version 1.1:\n\n#!docker image build -t nesaboz/qsk-course:1.1 .`\n\nWe can see the nesaboz/qsk-course image now in the list of docker images:\n\n!docker image ls\n\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nnesaboz/qsk-course          1.0       8d9065c8c75f   2 hours ago    268MB\nnesaboz/qsk-course          1.1       60932818191e   5 hours ago    268MB\nnesaboz/docker101tutorial   latest    940714c42b63   2 days ago     47MB\nalpine/git                  latest    22d84a66cda4   3 months ago   43.6MB\n\n\nWe can now actually run the application locally by running:\n\n#!docker run -dp 8080 nesaboz/qsk-course:1.0\n\nDocker will assign some port (can be seen in Docker Desktop), and our app will be running locally in a browser:\n\n\n\nimage.png\n\n\nWe can also run version 1.1:\n\n# !docker run -dp 8080 nesaboz/qsk-course:1.1\n\n\n\n\nimage.png\n\n\n\n\nRegister Images (via DockerHub)\nRunning locally is fine but we want to run in a cloud. There are many hosting services for containers, DockerHub is the easiest to use. We can push container to DockerHub by running the following (in Terminal since it requires password):\ndocker login --username nesaboz\ndocker image push nesaboz/qsk-course:1.0\nAnd we can see now that we have two verions on the DockerHub:\n\n\n\nimage-2.png\n\n\n\n\nSet up Hosted Kubernets (via Linode Kubernets Engine aka LKE)\nLKE (https://cloud.linode.com/) is a paid service to host Kubernets by setting a cluster of machines (we’ll use shared, CPU-only VMs). Once we set up a cluster, we can download a config file (aka kubeconfig file) that looks like this:\napiVersion: v1\nkind: Config\npreferences: {}\n\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CR...\n    server: https://8c292154-27de-4e91-a129-fcb2a1614fee.us-west-1.linodelke.net:443\n  name: lke95373\n\nusers:\n- name: lke95373-admin\n  user:\n    as-user-extra: {}\n    token: eyJhbGc...\n\ncontexts:\n- context:\n    cluster: lke95373\n    namespace: default\n    user: lke95373-admin\n  name: lke95373-ctx\n\ncurrent-context: lke95373-ctx\n\n\nConnect to LKE\nWe can now activate nodes:\nexport KUBECONFIG=/usercode/config    <== path might be different, for example KUBECONFIG=config\nkubectl get nodes\nNAME                           STATUS   ROLES    AGE   VERSION\nlke95373-144188-63fe9e5739a9   Ready    <none>   22h   v1.25.4\nlke95373-144188-63fe9e576608   Ready    <none>   22h   v1.25.4\nThe nodes are now running.\n\n\nkubectl\nMain command that executes everything kubernets is kubectl:\nkubectl version -o yaml\nclientVersion:\n  buildDate: \"2022-11-09T13:36:36Z\"\n  compiler: gc\n  gitCommit: 872a965c6c6526caa949f0c6ac028ef7aff3fb78\n  gitTreeState: clean\n  gitVersion: v1.25.4\n  goVersion: go1.19.3\n  major: \"1\"\n  minor: \"25\"\n  platform: darwin/amd64\nkustomizeVersion: v4.5.7\nserverVersion:\n  buildDate: \"2023-01-18T19:15:26Z\"\n  compiler: gc\n  gitCommit: ff2c119726cc1f8926fb0585c74b25921e866a28\n  gitTreeState: clean\n  gitVersion: v1.25.6\n  goVersion: go1.19.5\n  major: \"1\"\n  minor: \"25\"\n  platform: linux/amd64\n\n\nPod object\nPod is a lightweight wrapper for Docker image we already deployed on DockerHub. The config file for Pod is pod.yml:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: first-pod\n  labels:\n    project: qsk-course\nspec:\n  containers:\n    - name: web-ctr\n      image: nesaboz/qsk-course:1.0\n      ports:\n        - containerPort: 8080\nkubectl apply -f pod.yml\nkubectl get pods\nNAME        READY   STATUS    RESTARTS   AGE\nfirst-pod   1/1     Running   0          3m36s\nWe can run similar though more detailed command (doesn’t run here in notebook):\nkubectl describe pod first-pod\nTo delete pods:\nkubectl delete pod first-pod\nkubectl delete --all pods\n\n\nKubernets Service controller\nService controller provides connectivity to the application running in the Pod i.e. provisions an internet-facing load balancer. To do that we use svc-cloud.yml file that contains following:\napiVersion: v1\nkind: Service\nmetadata:\n  name: cloud-lb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n  selector:\n    project: qsk-course    <- this name must match pod.metadata.labels.project above\nNote that spec.selector.project must match the pod.metadata.labels.project\n(We also have an option of running svc-local.yml skipping that for now)\nkubectl apply -f svc-cloud.yml   #  service/cloud-lb created`\nkubectl get svc\nqsk-cloud    LoadBalancer   10.128.141.64   45.79.230.110   8080:32026/TCP   13s\nor for more details run:\nkubectl describe svc cloud-lb\nnow go to the 45.79.230.110:8080 address in your browser:\n\n\n\nimage.png\n\n\nTo delete svc object:\nkubectl delete svc cloud-lb\n\n\nKubernets Deployment controller\nDeployment service provides self-healing, enable scaling, and rolling updates. The file deploy.yml looks like this:\nkind: Deployment                   <<== Type of object being defined\napiVersion: apps/v1                <<== Version of object specification\nmetadata:\n  name: qsk-deploy\nspec:\n  replicas: 5                      <<== How many Pod replicas\n  selector:\n    matchLabels:                   <<== Tells the Deployment controller\n      project: qsk-course            <<== which Pods to manage, must match svc.spec.selector.project above\n  template:\n    metadata:\n      labels:\n        project: qsk-course          <<== Pod label, must match svc.spec.selector.project above\n    spec:\n      containers:\n      - name: qsk-pod\n        imagePullPolicy: Always           <<== Never use local images\n        ports:\n        - containerPort: 8080             <<== Network port\n        image: nesaboz/qsk-course:1.0     <<== Image containing the app\nNote that spec.template.labels.project must match the svc.spec.selector.project\nkubectl apply -f deploy.yml\nkubectl get deployments\nNAME         READY   UP-TO-DATE   AVAILABLE   AGE\nqsk-deploy   3/5     5            3           3s\nkubectl get pods\nNAME                          READY   STATUS    RESTARTS   AGE\nqsk-deploy-767d99b5c7-5v9vs   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          52s\nIf we delete one pod, for example first one qsk-deploy-767d99b5c7-5v9vs:\nkubectl delete pod qsk-deploy-767d99b5c7-5v9vs\nkubectl get pods\nNAME                          READY   STATUS        RESTARTS   AGE\nqsk-deploy-767d99b5c7-4vdjs   1/1     Running       0          13s\nqsk-deploy-767d99b5c7-5v9vs   1/1     Terminating   0          3m19s\nqsk-deploy-767d99b5c7-9h67b   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running       0          3m19s\nwe see that new one gets running immediately, demonstrating self-healing.\nWe can also see that pods are running on both nodes (lke...a9 and lke...08):\n$kubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE     IP         NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-4vdjs   1/1     Running   0          2m50s   10.2.1.7   lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          5m56s   10.2.0.7   lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running   0          5m56s   10.2.1.6   lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running   0          5m56s   10.2.1.4   lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          5m56s   10.2.0.6   lke95373-144188-63fe9e576608   <none>           <none>\n\nIf we delete a node (in a LKE Cloud Console) then we can see that new pods will be created on a available node:\nkubectl get nodes\nNAME                           STATUS   ROLES    AGE   VERSION\nlke95373-144188-63fe9e576608   Ready    <none>   23h   v1.25.4\nkubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE    IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-2qmgn   1/1     Running   0          105s   10.2.0.8    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          13m    10.2.0.7    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-f6k76   1/1     Running   0          104s   10.2.0.10   lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-vsvgv   1/1     Running   0          104s   10.2.0.9    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          13m    10.2.0.6    lke95373-144188-63fe9e576608   <none>           <none>\nWe can see that old pods (qsk-deploy...9h67b and qsk-deploy...wh2rh) are still running, the other 3 are new (created 105 seconds ago). If we had set the Autoscale Pool on LKE, then the node will recover eventually:\n\n\n\nimage.png\n\n\nbut the pods will still run only on one (lke...08) node:\nNAME                          READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-2qmgn   1/1     Running   0          20m   10.2.0.8    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          32m   10.2.0.7    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-f6k76   1/1     Running   0          20m   10.2.0.10   lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-vsvgv   1/1     Running   0          20m   10.2.0.9    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          32m   10.2.0.6    lke95373-144188-63fe9e576608   <none>           <none>\nIf we change a number of replicas in the deployment file, newly pods will intelligently run on a newly recovered node so the distribution is maintained:\nkubectl scale --replicas 10 deployment/qsk-deploy\nkubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE    IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-679dc78b78-bgmfm   1/1     Running   0          14m    10.2.0.3    lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-679dc78b78-ckvqx   1/1     Running   0          14m    10.2.0.5    lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-679dc78b78-cvh5t   1/1     Running   0          2m6s   10.2.1.2    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-679dc78b78-gll5p   1/1     Running   0          2m6s   10.2.1.6    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-679dc78b78-hjsqx   1/1     Running   0          14m    10.2.0.6    lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-679dc78b78-qs4gs   1/1     Running   0          14m    10.2.0.10   lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-679dc78b78-r4kgl   1/1     Running   0          2m6s   10.2.1.5    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-679dc78b78-rqpgh   1/1     Running   0          2m6s   10.2.1.4    lke95373-144188-63fe9e576608   <none>           <none>\nqsk-deploy-679dc78b78-vblz4   1/1     Running   0          14m    10.2.0.4    lke95373-144188-63fe9e5739a9   <none>           <none>\nqsk-deploy-679dc78b78-x6f2g   1/1     Running   0          2m6s   10.2.1.3    lke95373-144188-63fe9e576608   <none>           <none>\nNote: kubectl scale should not be used since there is a discrpancy between deploy.yml and new nubmer of replicas. The common practice is to always edit deploy.yml file and re-apply.\n\n\nRolling update\nWe can apply rolling update if we add following lines to deploy.yml:\n  minReadySeconds: 20        <== to wait for 20 seconds after updating each replica\n  strategy:                  \n    type: RollingUpdate      \n    rollingUpdate:           \n      maxSurge: 1         <== allows Kubernetes to add one extra Pod during an update operation\n      maxUnavailable: 0   <== prevents Kubernetes from reducing the number of Pods during an update\nas well as update the version of the image:\nimage: nesaboz/qsk-course:1.1      <== this version must exists as an container on DockerHub\nso the deploy.yml looks like this:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qsk-deploy\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      project: diy\n  minReadySeconds: 20        \n  strategy:                  \n    type: RollingUpdate      \n    rollingUpdate:           \n      maxSurge: 1            \n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        project: diy\n    spec: \n      containers:\n      - name: qsk-pod\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        image: nesaboz/qsk-course:1.1 \nNow if we re-apply deployment:\nkubectl apply -f deploy.yml\ndeployment.apps/qsk-deploy configured\nwe can monitor the rollout status:\nkubectl rollout status deployment qsk-deploy\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\ndeployment \"qsk-deploy\" successfully rolled out\nSoon, the App will be updated:\n\n\n\nimage-2.png\n\n\n\n\nAppendix: Apply service locally\nFor local application let’s use local scv-local.yml file:\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-local\nspec:\n  type: NodePort    <== this was LoadBalancer for cloud servers, now is NodePort\n  ports:\n  - port: 8080\n    protocol: TCP       <== this is a new line\n    targetPort: 8080\n    nodePort: 31111     <== this is a new line\n  selector:\n    project: qsk-course    <== this must match with the pod project"
  },
  {
    "objectID": "mini-projects/linear_regression.html",
    "href": "mini-projects/linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Inspiration by Daniel Voigt Godoy’s books\n\nLinear regression\n\nimport platform\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.linear_model import LinearRegression\nfrom pytorched.step_by_step import StepByStep\n\nfrom torchviz import make_dot\nplt.style.use('fivethirtyeight')\n\n\n\nGenerate some data\nwe’ll use numpy for this, and also need to split the data, can also use numpy for this\n\nnp.random.seed(43)\n\nb_true = 2.\nw_true = -0.5\nN = 100\n\nx = np.random.rand(N,1)\nepsilon = 0.05 * np.random.randn(N,1)\ny = w_true*x + b_true + epsilon\n\nplt.plot(x,y,'.')\nplt.show()\n\n\n\n\n\n\nLinear regression with sklearn\nOf course we can make a fit using sklearn:\n\nreg = LinearRegression().fit(x, y)\nr2_coef = reg.score(x, y)\nprint(reg.coef_, reg.intercept_, r2_coef)\n\n[[-0.52894853]] [2.01635764] 0.9014715901595961\n\n\nbut the point is to learn PyTorch and solve much bigger problems.\n\n\nCreate datasets, data loaders\n\ndata set is the object that holds features and labels together,\nsplit the data into train and valid,\nconvert to pytorch tensors,\ncreate datasets,\ncreate data_loaders.\n\n\nnp.random.seed(43)\nindices = np.arange(N)\nnp.random.shuffle(indices)\ntrain_indices = indices[:int(0.8*N)]\nval_indices = indices[int(0.8*N):]\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_x = torch.tensor(x[train_indices], dtype=torch.float32, device=device)\ntrain_y = torch.tensor(y[train_indices], dtype=torch.float32, device=device)\nval_x = torch.tensor(x[val_indices], dtype=torch.float32, device=device)\nval_y = torch.tensor(y[val_indices], dtype=torch.float32, device=device)\n\ntrain_dataset = TensorDataset(train_x, train_y)\nval_dataset = TensorDataset(val_x, val_y)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n\n\nModel, loss, and optimizer\n\ntorch.random.manual_seed(42)\nmodel = torch.nn.Linear(1,1, bias=True, device=device)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\n\n\nTrain\n\nmodel.reset_parameters()\nsbs = StepByStep(model, optimizer, loss_fn)\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(30)\n\n\nsbs.model.state_dict()\n\nOrderedDict([('weight', tensor([[-0.5267]])), ('bias', tensor([2.0177]))])\n\n\n\nsbs.plot_losses()\n\n\n\n\nNote btw that alex and sbs.model are the same object:\n\nassert id(sbs.model) == id(model)\n\n\n\nPredict\n\ntest = np.random.rand(100,1)\ntest_predictions = sbs.predict(test)\nplt.plot(x,y,'.')\nplt.plot(test,test_predictions,'.')\nplt.show()\n\n\n\n\n\n\nSave/load model\n\nsbs.save_checkpoint('pera.pth')\n\n\nsbs.load_checkpoint('pera.pth')\n\n\n\nVisualize model\nOne can use make_dot(yhat) locally. I can’t make graphviz work on GitHub, but the output looks like this:\n\n\n\nSet up tensorboard\nOne can add tensorboard to monitor losses, this will be important when having long training. We can start tensorboard from terminal using tensorboard --logdir runs (or from notebook if using extension via %load_ext tensorboard). The tensorboard should be running at http://localhost:6006/ (ignore \"TensorFlow installation not found\" message, we don’t need it). Make sure path is right, tensorboard will be empty if it can’t find the runs folder."
  },
  {
    "objectID": "mini-projects/multiclass_image_classification.html",
    "href": "mini-projects/multiclass_image_classification.html",
    "title": "Multiclass image classification",
    "section": "",
    "text": "Here we’ll modify binary image classification from previous example to multiclass image classification by detecting left diagonal and right diagonal separately.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom pytorched.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start > 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets > 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30, per_row=10):\n    n_rows = n_plot // per_row + ((n_plot % per_row) > 0)\n    fig, axes = plt.subplots(n_rows, per_row, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // per_row, i % per_row    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 8})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\nData preparation\nWe prepare data similary as in the previous exercize thought notable difference is that y_tensor shape is (N), not (N,1) as previously. This is due to loss function (CrossEntropyLoss) requiremnts (it takes class indices).\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels).long()\n\n\nx_tensor.shape\n\ntorch.Size([1000, 1, 10, 10])\n\n\n\ny_tensor.shape\n\ntorch.Size([1000])\n\n\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\n\ntorch.manual_seed(42)\nN = len(x_tensor)\nn_train = int(.8*N)\ntrain_subset, val_subset = random_split(x_tensor, [n_train, N - n_train])\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\nWe do not apply augmentation since it would mess up the labels:\n\ntrain_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\ny_val_tensor\n\ntensor([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n        2, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 0, 0, 0, 2,\n        0, 2, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0,\n        2, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2,\n        2, 2, 0, 2, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 2, 0, 2, 2, 2, 1, 1,\n        1, 1, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1,\n        0, 1, 0, 2, 0, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 2,\n        2, 1, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0,\n        0, 0, 0, 2, 2, 2, 2, 2])\n\n\n\n\nDeep model\nA typical architecture uses a sequence of one or more typical convolutional blocks, with each block consisting of three operations:\n\nConvolution\nActivation function\nPooling\n\nAnd for multiclass problems we need to use appropriate loss functions depending if we have Sigmoid/LogSoftmax as the last layer:\n\nLet’s build a model:\n\ntorch.manual_seed(13)\nmodel_cnn1 = nn.Sequential()\n\n# Featurizer\n# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\nn_channels = 1\nmodel_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\nmodel_cnn1.add_module('relu1', nn.ReLU())\nmodel_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n# Flattening: n_channels * 4 * 4\nmodel_cnn1.add_module('flatten', nn.Flatten())\n\n# Classification\n# Hidden Layer\nmodel_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\nmodel_cnn1.add_module('relu2', nn.ReLU())\n# Output Layer\nmodel_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n\nlr = 0.1\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)\n\nLet’s just run one batch to get the sense for loss:\n\nx, y = next(iter(train_loader))\ny_pred = model_cnn1(x)\nprint(y.shape)\nprint(y_pred.shape)\nnn.CrossEntropyLoss()(y_pred,y)\n\ntorch.Size([16])\ntorch.Size([16, 3])\n\n\ntensor(0.2768, grad_fn=<NllLossBackward0>)\n\n\nOne important observation: y shape is 16, while y_pred shape is 16x3. CrossEntropyLoss expects this (see docs).\n\nsbs = StepByStep(model_cnn1, optimizer_cnn1, multi_loss_fn)\n# sbs.set_seed()\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(20)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  7.16it/s]\n\n\n\nfig = sbs.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\nCorrect categories:\ntensor([[60, 71],\n        [46, 56],\n        [73, 73]])\n\n\n\n\nCode\nprint(f'Accuracy: {sbs.accuracy}%')\n\n\nAccuracy: 89.5%\n\n\nThis is not the greatest accuracy for label 0 (parallel) and label 1 (counter-diagonal). Once can probably find a better model (TODO) but for now let’s inspect what failed.\n\n\nVisualize error outputs\n\n# will predict all points at once here, no batches:\nlogits = sbs.predict(val_loader.dataset.x)\npredicted = np.argmax(logits, 1)\n\n\nlogits.shape\n\n(200, 3)\n\n\n\nval_loader.dataset.x.shape\n\ntorch.Size([200, 1, 10, 10])\n\n\n\nlogits.shape\n\n(200, 3)\n\n\n\npredicted\n\narray([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n       0, 1, 2, 1, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1,\n       0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0,\n       1, 2, 1, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 0, 2, 1,\n       2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1,\n       1, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2,\n       0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 2, 0,\n       2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1,\n       0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2,\n       2, 2])\n\n\n\nnot_equal = torch.ne(val_loader.dataset.y, torch.as_tensor(predicted))\nimages_tensor = val_loader.dataset.x[not_equal]\nactual_labels_tensor = val_loader.dataset.y[not_equal]\npred_labels_tensor = predicted[not_equal]\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nstart_idx = 0\nbatch_size = 10\nimages_batch = images_tensor[start_idx:start_idx+batch_size]\nlabels_batch = actual_labels_tensor[start_idx:start_idx+batch_size]\n\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)\n\n\n\n\n\n\n\nAnd we see that lots of predictions are for class label 2. For images 2,3,7,8, filters failed to register anything.\n\n\nOrdinary batch visualization:\n\nfig_filters = sbs.visualize_filters('conv1', cmap='gray')\n\n\n\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nimages_batch, labels_batch = next(iter(val_loader))\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
  },
  {
    "objectID": "mini-projects/transfer_learning.html",
    "href": "mini-projects/transfer_learning.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Set the environment (for Google Colab):\nIf this is GoogleColab we download config and use it to download necesseary folders and files for this project (defined in config.py). Here we’ll need: - pytorched.step_by_step.py - data_preparation.rps.py\n\n\nCode\ntry:\n    import google.colab\n    !pip install numpy pandas matplotlib torchviz scikit-learn tensorboard torchvision torch tqdm torch-lr-finder\n\n    import requests\n    url = 'https://raw.githubusercontent.com/nesaboz/pytorched/main/config.py'\n    r = requests.get(url, allow_redirects=True)\n    open('config.py', 'wb').write(r.content)    \nexcept ModuleNotFoundError:\n    print('Not Google Colab environment.')\n\n\nfrom config import config_project\nconfig_project('transfer_learning')\n\n\nWe are now ready for imports:\n\n\nCode\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nfrom torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop, InterpolationMode\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import alexnet, resnet18, inception_v3\nfrom torchvision.models.alexnet import model_urls\nfrom torchvision.models import Inception_V3_Weights, AlexNet_Weights\nfrom torch.hub import load_state_dict_from_url\nfrom torchviz import make_dot\n\nfrom data_generation.rps import download_rps\nfrom pytorched.step_by_step import StepByStep, freeze_model, print_trainable_parameters\n\nplt.style.use('fivethirtyeight')\n\n\n\nAlexNet\nLet’s try to use AlexNet model first to help us in the Rock-Paper-Scissors problem. We’ll need to load AlexNet, with it’s weights, then make a feature-extractor on our data loaders, modify the last layer, and train. Let’s get to it:\nWe can get AlexNet from torchvision.models:\n\nweights = AlexNet_Weights.IMAGENET1K_V1\nalex = alexnet(weights=weights)\nalex\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\nWe need an original transform:\n\ntransform = weights.transforms()\n\nLet’s now create new data loaders based on this transform:\n\n\nCode\ndownload_rps()\n\n\n\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\n\n\ntrain_dataset.classes\n\n['paper', 'rock', 'scissors']\n\n\n\ntrain_loader = DataLoader(train_dataset, 16, shuffle=True)\nval_loader = DataLoader(val_dataset, 16)\n\nLet’s also define an optimizer and loss:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(alex.parameters(), 3e-4)\nloss_fn = nn.CrossEntropyLoss()\nsbs = StepByStep(alex, optimizer, loss_fn)\nsbs.set_loaders(train_loader, val_loader)\n\n\n\nFeature-extractor\nThese are current trainable parameters:\n\nsbs.print_trainable_parameters()\n\nfeatures.0.weight\nfeatures.0.bias\nfeatures.3.weight\nfeatures.3.bias\nfeatures.6.weight\nfeatures.6.bias\nfeatures.8.weight\nfeatures.8.bias\nfeatures.10.weight\nfeatures.10.bias\nclassifier.1.weight\nclassifier.1.bias\nclassifier.4.weight\nclassifier.4.bias\nclassifier.6.weight\nclassifier.6.bias\n\n\nWe must freeze the model and replace the last layer. These are suggestions what layers to change (of course more layers can be left unfrozen but this requires more training data and longer times):\n\nso let’s change the last layer first to Identity to make feature extractor:\n\nsbs.model.classifier[6] = nn.Identity()\n\n\nfreeze_model(sbs.model)\n\n\nsbs.print_trainable_parameters()\n\nNo trainable parameters.\n\n\nWe now go throught the loader batch by batch and pass the data through the model in order to generate new preprocessed datasets:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef preprocessed_dataset(model, loader, device=None):\n    \"\"\"\n    Runs all data in the loader through the model and returns a dataset.\n    \"\"\"\n    \n    features = torch.Tensor()\n    labels = torch.Tensor().type(torch.long)\n\n    if device is None:\n        device = next(model.parameters()).device\n\n    for i, (x_batch, y_batch) in enumerate(loader):\n        model.eval()\n        output = model(x_batch.to(device))\n        features = torch.cat([features, output.detach().cpu()])\n        labels = torch.cat([labels, y_batch.cpu()])\n\n    return TensorDataset(features, labels)\n\n\ndef test_preprocessed_dataset():\n    x = torch.rand(10,3,244,244)\n    y = torch.rand(10,1)\n    ds = TensorDataset(x,y)\n    dl = DataLoader(ds, 16, False)\n    tpp = preprocessed_dataset(alex, dl)\n    assert tpp.tensors[0].shape == torch.Size([10, 4096])\n\n\ntrain_preproc = preprocessed_dataset(alex, train_loader)\nval_preproc = preprocessed_dataset(alex, val_loader)\n\nmake sure that the tensort types are correct:\n\nassert next(iter(train_preproc))[0].type() == 'torch.FloatTensor'\nassert next(iter(train_preproc))[1].type() == 'torch.LongTensor'\n\nwe now build new DataLoaders:\n\nnew_train_loader = DataLoader(train_preproc, 16, True)\nnew_val_loader = DataLoader(val_preproc, 16)\n\n\n\nTop layer\nWith features extracted we can now create a brand new simple model using a fully-connected layer per table suggestions nn.Linear(4096, num_classes):\nLet’s create a new model:\n\ntorch.manual_seed(17)\ntop_layer = nn.Linear(4096, 3)\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_top = optim.Adam(top_layer.parameters(), lr=3e-4)\nsbs_top = StepByStep(top_layer, optimizer_top, multi_loss_fn)\nsbs_top.set_loaders(new_train_loader, new_val_loader)\n\n\nsbs_top.train(10)\n\n100%|██████████| 10/10 [00:01<00:00,  5.11it/s]\n\n\n\n_ = sbs_top.plot_losses()\n\n\n\n\n\nsbs_top.accuracy\n\n95.97\n\n\n\nsbs_top.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\nAnd this is pretty good, and very fast too!\nFor any new images that need to be evaluated thought, we will have to go through the whole model, so let’s insert this new top_layer into a sbs. Important: be very careful when changing the model layers AFTER creating StepByStep object. Model change that changes parameters must be reflected in optimizer as well (I added some check for this via sbs.check_consistency that checks number of parameters).\n\nsbs.model.classifier[6] = top_layer\nsbs.check_consistency()\nsbs.model.classifier\n\nSequential(\n  (0): Dropout(p=0.5, inplace=False)\n  (1): Linear(in_features=9216, out_features=4096, bias=True)\n  (2): ReLU(inplace=True)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=4096, out_features=4096, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=4096, out_features=3, bias=True)\n)\n\n\nLet’s evaluate on the old val_loader to see if we get the same result:\n\nsbs.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\n\nsbs.accuracy\n\n95.97\n\n\nyup, exactly the same.\n\n\nInception Model\nLet’s try Inception model. Inception has these 2 layers so we can not run the feature extraction, we have to use the full model. First we prep the data:\n\nweights = Inception_V3_Weights.IMAGENET1K_V1\ntransform = weights.transforms()\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\ntrain_loader = DataLoader(train_dataset, 16, True)\nval_loader = DataLoader(val_dataset, 16)\n\n\ntransform\n\nImageClassification(\n    crop_size=[299]\n    resize_size=[342]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\n\nnext we set the model and freeze it:\n\ninception = inception_v3(weights=weights)\n\n\ninception\n\nInception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\ninception = inception_v3(weights=weights)\nfreeze_model(inception)\nprint_trainable_parameters(inception)\n\nNo trainable parameters.\n\n\nreplace the top layers per advice:\n\ninception.fc = nn.Linear(2048, 3)\ninception.AuxLogits.fc = nn.Linear(768, 3)\nprint_trainable_parameters(inception)\n\nAuxLogits.fc.weight\nAuxLogits.fc.bias\nfc.weight\nfc.bias\n\n\nWe need to create special loss function that handles 2 output losses (one main and one auxilary) and combines them (with weight of 0.4 for auxilary):\n\ndef inception_loss(outputs, labels):\n    try:\n        main, aux = outputs  # this is a KEY difference from other models with single output\n    except ValueError:\n        main, aux = outputs, None  # this is a typical loss with no auxilairy layers\n        \n    main_loss = nn.CrossEntropyLoss()(main, labels)\n    aux_loss = nn.CrossEntropyLoss()(aux, labels) if aux is not None else 0\n    return main_loss + 0.4 * aux_loss\n\nWe are now ready to create sbs object:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(inception.parameters(), lr=3e-4)\nsbs_inception = StepByStep(inception, optimizer, inception_loss)\nsbs_inception.set_loaders(train_loader, val_loader)\n\nIt is training time:\n\nsbs_inception.train(10)\n\n100%|██████████| 10/10 [05:08<00:00, 30.85s/it]\n\n\n\nprint(sbs_inception.accuracy_per_class)\nprint(sbs_inception.accuracy)\nsbs_inception.plot_losses()\n\ntensor([[112, 124],\n        [118, 124],\n        [113, 124]])\n92.2\n\n\n\n\n\n\n\nInception model from scratch\n\nscissors = Image.open('rps/scissors/scissors01-001.png')\nimage = ToTensor()(scissors)[:3, :, :].view(1, 3, 300, 300)\nweights = torch.tensor([0.2126, 0.7152, 0.0722]).view(1, 3, 1, 1)\nconvolved = F.conv2d(input=image, weight=weights)\nconverted = ToPILImage()(convolved[0])\n\ngrayscale = scissors.convert('L')\n\n\ngrayscale\n\n\n\n\nOne must define all layers in the __init__ in order to initialize them. Remeber that forward method will be called during training so no layers with parameters should be defined there:\n\nclass Inception(nn.Module):\n    def __init__(self, in_channels):\n        super(Inception, self).__init__()\n        # in_channels@HxW -> 2@HxW\n        self.branch1x1_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n        # in_channels@HxW -> 2@HxW -> 3@HxW\n        self.branch5x5_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch5x5_2 = nn.Conv2d(2, 3, kernel_size=5, padding=2)\n\n        # in_channels@HxW -> 2@HxW -> 3@HxW\n        self.branch3x3_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch3x3_2 = nn.Conv2d(2, 3, kernel_size=3, padding=1)\n\n        # in_channels@HxW -> in_channels@HxW -> 1@HxW\n        self.branch_pool_1 = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch_pool_2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n    def forward(self, x):\n        # Produces 2 channels\n        branch1x1 = self.branch1x1_1(x)\n        # Produces 3 channels\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n        # Produces 3 channels\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n        # Produces 2 channels\n        branch_pool = self.branch_pool_1(x)\n        branch_pool = self.branch_pool_2(branch_pool)\n        # Concatenates all channels together (10)\n        outputs = torch.cat([branch1x1, branch5x5, branch3x3, branch_pool], 1)\n        return outputs\n\n\ninception = Inception(in_channels=3)\noutput = inception(image)\noutput.shape\n\ntorch.Size([1, 10, 300, 300])"
  },
  {
    "objectID": "mini-projects/15_head_pose/head_pose.html",
    "href": "mini-projects/15_head_pose/head_pose.html",
    "title": "Image Regression",
    "section": "",
    "text": "In this project we’ll detect the center of the person’s face in the image. We’ll use the Biwi Kinect Head Pose dataset for this task. The dataset contains images of 24 people and coresponding obj files that won’t be needed. Each image (*_rgb.jpg) is annotated with the center of the person’s face and the pose of the head (_pose.txt*). The pose is defined by the rotation of the head around the three axes.\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\npath.ls()\n\n(#50) [Path('/notebooks/fastai/data/biwi_head_pose/03.obj'),Path('/notebooks/fastai/data/biwi_head_pose/06'),Path('/notebooks/fastai/data/biwi_head_pose/21'),Path('/notebooks/fastai/data/biwi_head_pose/16.obj'),Path('/notebooks/fastai/data/biwi_head_pose/08'),Path('/notebooks/fastai/data/biwi_head_pose/19'),Path('/notebooks/fastai/data/biwi_head_pose/15'),Path('/notebooks/fastai/data/biwi_head_pose/02.obj'),Path('/notebooks/fastai/data/biwi_head_pose/15.obj'),Path('/notebooks/fastai/data/biwi_head_pose/04')...]\n\n\nThere are total of 15678 images:\n\nimg_files = get_image_files(path)  # same as L(path.glob('**/*_rgb.jpg')) but get_image_files is not implemented as a glob)\nimg_files\n\n(#15678) [Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00554_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00069_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00262_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00324_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00093_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00313_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00129_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00187_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00278_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00117_rgb.jpg')...]\n\n\n\nlen(list(path.glob('**/*_rgb.jpg')))\n\n15678\n\n\nAn image look like this:\n\nim_path = (path / '01').ls()[0]\nim = PILImage.create(im_path).to_thumb(300)  # same as PIL.Image.open\nim\n\n\n\n\nto find the annotation for that image we need to look at the corresponding *_pose.txt* file:\n\ndef img2pose(x):\n    return Path(f'{str(x)[:-7]}pose.txt')\n\n\nff = img2pose(img_files[0])\nwith open(ff, 'r') as f:\n    print(f.read())\n\n0.969581 -0.0117593 0.244489 \n-0.0614535 0.955158 0.28965 \n-0.236931 -0.295864 0.92538 \n\n22.4063 138.81 1058.77 \n\n\n\n\nThe following is the code to extract data from the file, it uses np.genfromtxt:\n\nwith open(path / '01' / 'rgb.cal', 'r') as f:\n    print(f.read())\n\n517.679 0 320 \n0 517.679 240.5 \n0 0 1 \n\n0 0 0 0 \n\n0.999947 0.00432361 0.00929419 \n-0.00446314 0.999877 0.0150443 \n-0.009228 -0.015085 0.999844 \n\n-24.0198 5.8896 -13.2308 \n\n640 480\n\n\n\nThe following is the code to extract data from the file, it uses np.genfromtxt:\n\ncal = np.genfromtxt(path / '01' / 'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0]*cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1]*cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1, c2])\n\n\nimg_files[0]\n\nPath('/notebooks/fastai/data/biwi_head_pose/06/frame_00554_rgb.jpg')\n\n\n\nidx = 4\nprint(img_files[idx])\nget_ctr(img_files[idx])\n\n/notebooks/fastai/data/biwi_head_pose/06/frame_00093_rgb.jpg\n\n\ntensor([367.3590, 311.6036])\n\n\nWe’ll use images from person #15 as a validation dataset:\n\ndef splitter(name):\n    # True if directory name is 15\n    return name.parent.name=='15'\n\nWhen running with original size it takes ~2.5 minutes per epoch on P5000, which is not super fast. So let’s rescale the images to smaller size like half of both width and height, which is [640, 480] / 4 = [160, 120]:\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(splitter),\n    batch_tfms=[*aug_transforms(size=[120, 160]), Normalize.from_stats(*imagenet_stats)]\n    )\n\n\ndls = biwi.dataloaders(path)\n\n\ndls.show_batch(nrows=3, ncols=3)\n\n\n\n\n\nTraining\nLet’s look at one batch:\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 120, 160]), torch.Size([64, 1, 2]))\n\n\nLet’s create a Learner. Because we are dealing with the coordinates as targets, we should rescale them to (-1, 1). If y_range is defined then sigmoid is added as a last layer to the model:\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nlearn.model.state_dict\n\n<bound method Module.state_dict of Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n    (9): fastai.layers.SigmoidRange(low=-1, high=1)\n  )\n)>\n\n\nYup, there it is as a last layer.\nFastAI decided on a loss function:\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nthis makes sense, we are looking for L2 distance from the target.\nLet’s take a look at learning rate finder:\n\nlearn.lr_find()\n\nSuggestedLRs(valley=0.0012022644514217973)\n\n\n\n\n\nLet’s train:\n\nlearn.fine_tune(5, base_lr=1e-2)\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n  \n\n\n    \n      \n      0.00% [0/234 00:00<?]\n    \n    \n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nShow predictions on a validation dataset (ds_idx=1, i.e. the validation dataset):\n\nlearn.show_results(ds_idx=1, max_n=4, figsize=(6,8))\n\n\n\n\n\nlearn.final_record\n\n(#1) [0.00045044912258163095]\n\n\nThe error or 0.00039 is a mean square error, and y_range was (-1, 1) due to sigmoid, where -1 and 1 are the edges of the image (we can’t predict outside the images), so this means the corresponds average distance is:\n\nmath.sqrt(0.00039)\n\n0.019748417658131498\n\n\nThis corresponds to an average being 1.5 pixels off from the target (2 is the iamge width : 0.019 = 160 pixels : x):\n\n0.019*160/2\n\n1.52\n\n\n\n\nInference\nLet’s try to evaluate new images. We need to load RGB images, apparently learn.predict applies the same transformation as in DataLoaders. It even returns original image if needed (good post for more info):\n\nim_list = [] \nfor i in [1,3,4]:\n    im_path = f'/notebooks/nbs/mini-projects/15_head_pose/Photo{i}.jpg'\n    im_orig, pred, _, _ = learn.predict(im_path, with_input=True)\n    point = (pred[0]).detach().cpu().numpy()  # convert predictions to coordinates\n    im = ToPILImage()(im_orig/255)\n    draw = ImageDraw.Draw(im)\n    d = 2\n    draw.ellipse((point[0] - d, point[1] - d, point[0] + d, point[1] +d), fill='red')\n    im_list.append(im)\n_ = plot_pil_images(im_list)\n\n\n\n\n\n\n\n\nAnd this is pretty good, even if the head is shifted left and right with respect to the image center."
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html",
    "title": "Pet Breed Classification",
    "section": "",
    "text": "Let’s first download the data:\nRunning on Quadro P5000 16GB."
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data-preparation",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data-preparation",
    "title": "Pet Breed Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\n\npath = untar_data(URLs.PETS)\n\n\n    \n      \n      100.00% [811712512/811706944 00:09<00:00]\n    \n    \n\n\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/oxford-iiit-pet/images'),Path('/root/.fastai/data/oxford-iiit-pet/annotations')]\n\n\n\n(path/'images').ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_56.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_61.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_66.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_55.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_68.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_153.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Ragdoll_57.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_182.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_100.jpg')...]\n\n\n\nfname = (path/'images').ls()[0]\nfname\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_56.jpg')\n\n\nIf we want to extract the breed itself from the name we can use regex:\n\nm = re.match(r\"(.+)_\\d+.jpg\", fname.name)\nbreed = m.group(1)\nbreed\n\n'american_bulldog'\n\n\nitem_tfms is applied to all images. Here it resizes images to some large value first.\nbatch_tfms is applied only on mini-batches (on GPU if set as device). Here it crops and scales images. Note that validation set does not get augmented, only gets resized.\n\npets = DataBlock(blocks= (ImageBlock, CategoryBlock),\n                 get_items=get_image_files,\n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r\"(.+)_\\d+.jpg$\"), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\npets.summary(path/'images')\n\nSetting-up type transforms pipelines\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/pug_130.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=300x225\n  Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/pug_130.jpg\n    applying partial gives\n      pug\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(29)\n\nFinal sample: (PILImage mode=RGB size=300x225, TensorCategory(29))\n\n\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -> Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0} -> ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0} -> ToTensor\n    starting from\n      (PILImage mode=RGB size=300x225, TensorCategory(29))\n    applying Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0} gives\n      (PILImage mode=RGB size=460x460, TensorCategory(29))\n    applying ToTensor gives\n      (TensorImage of size 3x460x460, TensorCategory(29))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n    starting from\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n\n\n\ndls = pets.dataloaders(path/\"images\")\n\n\ndls.show_batch(nrows=1, ncols=5)\n\n\n\n\nLet’s create learner, and define dataloaders, model, and metrics (optimizer and loss are deducted automatically)."
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#training",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#training",
    "title": "Pet Breed Classification",
    "section": "Training",
    "text": "Training\n\nlearner = vision_learner(dls, resnet34, metrics=error_rate)\nlearner.fine_tune(2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.505711\n      0.344845\n      0.109608\n      00:35\n    \n    \n      1\n      0.342231\n      0.221326\n      0.073072\n      00:34\n    \n  \n\n\n\n\nlearner.recorder.plot_loss()\n\n\n\n\nWe can definitely train more using fine_tune but the error rate seem small so let’s take a look at some predictions:\n\nx, y = dls.one_batch()\n\n\ny\n\nTensorCategory([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25,\n                34, 30, 13, 21, 15, 24,  0,  9,  0, 27, 31,  2, 18, 25,  4, 14,\n                27, 35, 19, 33, 21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35,\n                14, 12, 18,  7,  8, 31, 15, 20, 16, 13, 29,  1, 16,  9, 32, 10],\n               device='cuda:0')\n\n\n\npreds, class_preds = learner.get_preds(dl=[(x, y)])\n\n\n\n\n\nprint(preds.shape)\nprint(preds)\nclass_preds\n\ntorch.Size([64, 37])\nTensorBase([[1.2718e-02, 7.3877e-01, 1.2443e-05,  ..., 1.4487e-05,\n             2.1402e-05, 2.4068e-06],\n            [2.1086e-04, 3.4171e-05, 2.3038e-05,  ..., 1.0070e-01,\n             3.7866e-04, 1.8155e-05],\n            [2.2906e-07, 6.6254e-08, 2.3534e-07,  ..., 1.0171e-07,\n             2.4211e-07, 3.6889e-07],\n            ...,\n            [2.1525e-06, 7.6821e-08, 2.3279e-08,  ..., 1.8256e-07,\n             1.4531e-08, 5.4367e-10],\n            [4.5686e-10, 1.6017e-08, 2.5129e-09,  ..., 7.3533e-09,\n             1.6970e-05, 1.6900e-07],\n            [1.7999e-08, 7.1642e-11, 2.4699e-06,  ..., 2.8352e-10,\n             1.9007e-09, 1.4708e-09]])\n\n\ntensor([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25, 34, 30,\n        13, 21, 15, 24,  0,  9,  0, 27, 31,  2, 18, 25,  4, 14, 27, 35, 19, 33,\n        21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35, 14, 12, 18,  7,  8, 31,\n        15, 20, 16, 13, 29,  1, 16,  9, 32, 10])\n\n\nThere are 64 samples in a batch, each having a probability of a certain class. The class_preds is just argmax of preds:\n\npreds.argmax(dim=1)\n\nTensorBase([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25, 34,\n            30, 13, 21, 15, 24,  0,  9,  1, 27, 31,  2, 18, 25,  4, 14, 27, 35,\n            19, 33, 21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35, 14, 12, 19,\n             7,  8, 31, 15, 20, 16, 13, 29,  1, 16,  9, 32, 10])\n\n\nSo learner deducted this is a multi-category problem, and have decided on nn.CrossEntropyLoss (which is combo of nn.LogSoftmax and nn.NLLLoss). What’s important is to apply nn.CrossEntropyLoss on logits, not on probabilites, so our model should not have softmax layer at the end.\nSoftmax is a multi-category equivalent of sigmoid. We use it any time when we want to convert logits into probabilites, and we want them to sum up to 1.\nLog is important because it’s easier to optimize, since difference between, say, 0.99 and 0.999 is 10 fold, not negligible."
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#model-interpretation",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#model-interpretation",
    "title": "Pet Breed Classification",
    "section": "Model interpretation",
    "text": "Model interpretation\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)\n\n\n\n\n\n\n\n\ninterp.most_confused(min_val=3)\n\n\n\n\n[('Ragdoll', 'Birman', 6),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 6),\n ('Russian_Blue', 'British_Shorthair', 5),\n ('Siamese', 'Birman', 5),\n ('staffordshire_bull_terrier', 'american_bulldog', 5),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 5),\n ('Bengal', 'Egyptian_Mau', 4),\n ('american_bulldog', 'staffordshire_bull_terrier', 4),\n ('basset_hound', 'beagle', 4),\n ('Egyptian_Mau', 'Bengal', 3),\n ('american_pit_bull_terrier', 'american_bulldog', 3),\n ('boxer', 'american_bulldog', 3),\n ('yorkshire_terrier', 'havanese', 3)]"
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#learning-rate-finder",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#learning-rate-finder",
    "title": "Pet Breed Classification",
    "section": "Learning rate finder",
    "text": "Learning rate finder\nLet’s train with some large learning rate (run it for 1 epoch every time with base_lr):\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlearner.fine_tune(1, base_lr=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.900633\n      1.380544\n      0.409337\n      00:25\n    \n  \n\n\n\n\nlearner.fine_tune(1, base_lr=0.1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.599579\n      1.268538\n      0.391746\n      00:26\n    \n  \n\n\n\n\nlearner.fine_tune(1, base_lr=0.2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      5.310261\n      4.817607\n      0.616373\n      00:25\n    \n  \n\n\n\nSo clearly we are diverging. We can use learning rate finder to deduct the good learning rate:\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlr_min = learner.lr_find()\n\n\n\n\n\n\n\n\nprint(f'{lr_min.valley:.2e}')\n\n5.75e-04"
  },
  {
    "objectID": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#transfer-learning",
    "href": "mini-projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#transfer-learning",
    "title": "Pet Breed Classification",
    "section": "Transfer learning",
    "text": "Transfer learning\nThe idea here is the same as before, we replace the last layer, freeze all but that last layer, then train. A version of this is done with the following:\n\nlearner.fine_tune??\n\nSignature:\nlearner.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    *,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/callback/schedule.py\nType:      method\n\n\nWhere learner.freeze will freeze up to a last layer:\n\nlearner.freeze??\n\nSignature: learner.freeze()\nDocstring: Freeze up to last parameter group\nSource:   \n@patch\ndef freeze(self:Learner): self.freeze_to(-1)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/learner.py\nType:      method\n\n\nlearner.fit_one_cycle trains whatever is unfrozen with some scheduler (will study it later):\n\nlearner.fit_one_cycle??\n\nSignature:\nlearner.fit_one_cycle(\n    n_epoch,\n    lr_max=None,\n    div=25.0,\n    div_final=100000.0,\n    pct_start=0.25,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nSource:   \n@patch\ndef fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n                  moms=None, cbs=None, reset_opt=False, start_epoch=0):\n    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/callback/schedule.py\nType:      method\n\n\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlearner.fit_one_cycle(3, 5e-4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.346319\n      0.579111\n      0.176590\n      00:22\n    \n    \n      1\n      1.001505\n      0.389489\n      0.131258\n      00:21\n    \n    \n      2\n      0.690984\n      0.360593\n      0.121786\n      00:22\n    \n  \n\n\n\n\nlearner.unfreeze()\n\n\nlearner.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.0002290867705596611)\n\n\n\n\n\nLoss is flat for small LRs because we already trained for 3 epochs."
  },
  {
    "objectID": "mini-projects/1_binary_image_classification.html",
    "href": "mini-projects/1_binary_image_classification.html",
    "title": "Binary image classification",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }</style>\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom pytorched.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\nWe’ll use generated data, where images with horizontal and vertical lines are considered label 0, while diagonal have label 1.\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start > 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets > 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 6 + ((n_plot % 6) > 0)\n    fig, axes = plt.subplots(n_rows, 6, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 6, i % 6    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=5, n_images=300, binary=True, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\nData preparation\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels.reshape(-1, 1)).float()  # reshaped this to (N,1) tensor\n\nPyTorch has Dataset class, TensorDataset as a subclass, and we can create custom subclasses too that can handle data augmentation:\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\nA torch.utils.data.random_split method can split indices into train and valid (it requires exact number of images to split):\n\ntorch.manual_seed(13)\nN = len(x_tensor)\nn_train = int(.8*N)\nn_val = N - n_train\ntrain_subset, val_subset = random_split(x_tensor, [n_train, n_val])\ntrain_subset\n\n<torch.utils.data.dataset.Subset>\n\n\nwe just need indices:\n\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\n\ntrain_idx[:10]\n\n[118, 170, 148, 239, 226, 146, 168, 195, 6, 180]\n\n\n\n\nData augmentation\nFor data augmentation we only augment training data, so we create training and validation Composer:\n\ntrain_composer = Compose([RandomHorizontalFlip(p=.5),\n                          Normalize(mean=(.5,), std=(.5,))])\n\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\nWe could stop here and just make loaders:\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\nor we can even used WeightedRandomSampler if we want to balance datasets:\n\ndef make_balanced_sampler(y):\n    # Computes weights for compensating imbalanced classes\n    classes, counts = y.unique(return_counts=True)\n    weights = 1.0 / counts.float()\n    sample_weights = weights[y.squeeze().long()]\n    # Builds sampler with compute weights\n    generator = torch.Generator()\n    sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        generator=generator,\n        replacement=True\n    )\n    return sampler\n\nNote that we don’t need a val_sampler anymore since we already split datasets:\n\ntrain_sampler = make_balanced_sampler(y_train_tensor)\n\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\n\nLogistic Regression Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_logistic = nn.Sequential()\nmodel_logistic.add_module('flatten', nn.Flatten())\nmodel_logistic.add_module('output', nn.Linear(25, 1, bias=True))\nmodel_logistic.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_logistic = optim.SGD(model_logistic.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_logistic = StepByStep(model_logistic, optimizer_logistic, binary_loss_fn)\nsbs_logistic.set_seed()\nsbs_logistic.set_loaders(train_loader, val_loader)\nsbs_logistic.train(200)\n\nFailed to set loader seed.\n\n\n100%|██████████| 200/200 [00:08<00:00, 23.24it/s]\n\n\n\nfig = sbs_logistic.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [34, 36]])\n\n\nAfter 200 epoch it’s almost 100%. Let’s add 400 more:\n\nsbs_logistic.train(400)\n\n100%|██████████| 400/400 [00:13<00:00, 29.00it/s]\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nso after 600 epoch model is 100% accurate (at least on 60 samples).\n\n\nDeeper Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_deeper = nn.Sequential()\nmodel_deeper.add_module('flatten', nn.Flatten())\nmodel_deeper.add_module('linear1', nn.Linear(25, 10, bias=True))\nmodel_deeper.add_module('relu', nn.ReLU())\nmodel_deeper.add_module('linear2', nn.Linear(10, 1, bias=True))\nmodel_deeper.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_deeper = optim.SGD(model_deeper.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_deeper = StepByStep(model_deeper, optimizer_deeper, binary_loss_fn)\nsbs_deeper.set_seed()\nsbs_deeper.set_loaders(train_loader, val_loader)\nsbs_deeper.train(20)\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 28.33it/s]\n\n\n\nfig = sbs_deeper.plot_losses()\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_deeper.loader_apply(sbs_deeper.val_loader, sbs_deeper.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nand even after 20 epoch it’s 100% accurate. We can train more to flatten the loss though which will surely generalize model:\n\nsbs_deeper.train(200)\nfig = sbs_deeper.plot_losses()\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 30.26it/s]\n\n\n\n\n\nAnd that’s it."
  },
  {
    "objectID": "mini-projects/quadratic_function_fit.html",
    "href": "mini-projects/quadratic_function_fit.html",
    "title": "Quadratic function fitting",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nfrom pytorched.step_by_step import StepByStep\n\nfrom torchviz import make_dot\nplt.style.use('fivethirtyeight')\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nLet’s make up some data:"
  },
  {
    "objectID": "mini-projects/quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "href": "mini-projects/quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "title": "Quadratic function fitting",
    "section": "Same but using sklearn.pipeline",
    "text": "Same but using sklearn.pipeline\nOne can make the process more streamlined using Pipeline:\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                  ('linear', LinearRegression(fit_intercept=False))])\n# fit to an order-2 polynomial data\nmodel = model.fit(x, y)\nprint(model.named_steps['linear'].coef_)\nprint(f'Real values {a0}, {a1}, {a2}')\n\n[[ 4.98812164 -1.61954639  4.02342307]]\nReal values 5.0, -1.5, 4.0"
  },
  {
    "objectID": "mini-projects/quadratic_function_fit.html#data-preparation",
    "href": "mini-projects/quadratic_function_fit.html#data-preparation",
    "title": "Quadratic function fitting",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nnp.random.seed(42)\nN = len(x)\nidx = list(range(N))\nnp.random.shuffle(idx)\n\n\nsplit_idx = int(.8*N)\ntrain_idx = idx[:split_idx]\nval_idx = idx[split_idx:]\ntrain_x = torch.as_tensor(x[train_idx], device=device).float()\ntrain_y = torch.as_tensor(y[train_idx], device=device).float()\nval_x = torch.as_tensor(x[val_idx], device=device).float()\nval_y = torch.as_tensor(y[val_idx], device=device).float()\n\n\ntrain_dataset = TensorDataset(train_x, train_y)\nval_dataset = TensorDataset(val_x, val_y)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)"
  },
  {
    "objectID": "mini-projects/quadratic_function_fit.html#training",
    "href": "mini-projects/quadratic_function_fit.html#training",
    "title": "Quadratic function fitting",
    "section": "Training",
    "text": "Training\n\nmodel=nn.Sequential(\n        nn.Linear(1,10),\n        nn.ReLU(),\n        nn.Linear(10,1)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\nsbs = StepByStep(model, optimizer, loss_fn)\n\nLet’s train for 200 epoch and plot losses:\n\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(200)\n\n\nsbs.plot_losses()\n\n\n\n\nLet’s make predictions:\n\ntest =np.linspace(-4.,4.,num=N).reshape(-1,1)\ntest_predictions = sbs.predict(test)\nplt.plot(x,y,'.')\nplt.plot(test,test_predictions,'.')\nplt.show()\n\n\n\n\nThis is good though, unfortunatelly, the true values of quadratic function are now lost in the sea of weights of the the two linear layers:\n\nsbs.model.state_dict()\n\nOrderedDict([('0.weight',\n              tensor([[ 1.3475],\n                      [-2.2383],\n                      [-2.1243],\n                      [ 2.0004],\n                      [-1.9875],\n                      [-2.2052],\n                      [ 0.1436],\n                      [-1.8479],\n                      [ 2.6974],\n                      [ 2.1781]])),\n             ('0.bias',\n              tensor([-1.2300, -3.2117,  0.8249, -1.5303, -0.2013, -2.3025,  1.3949, -0.0182,\n                       0.2817, -3.1922])),\n             ('2.weight',\n              tensor([[0.7446, 2.5052, 1.1556, 1.2103, 1.3438, 1.6768, 0.8039, 1.2448, 1.4132,\n                       2.6946]])),\n             ('2.bias', tensor([1.5188]))])"
  },
  {
    "objectID": "mini-projects/rock_paper_scissors.html",
    "href": "mini-projects/rock_paper_scissors.html",
    "title": "Rock paper scissors",
    "section": "",
    "text": "We’ll use Rock, Paper, Scissors dataset created by Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com) and can be found in his site: Rock Paper Scissors Dataset."
  },
  {
    "objectID": "mini-projects/rock_paper_scissors.html#temporary-dataset",
    "href": "mini-projects/rock_paper_scissors.html#temporary-dataset",
    "title": "Rock paper scissors",
    "section": "Temporary dataset",
    "text": "Temporary dataset\nWe need to calculate normalization parameters (mean and std) for all training images first. This is important step as we will use these normalization parameters not only for training images but for all the validation, and any future, predictions. Since we need to only calculate normalization parameters, we can also scale images to smaller size, just so the calculation is faster.\n\ncomposer = Compose([Resize(28),\n                    ToTensor()])\ntemp_dataset = ImageFolder(root='rps', transform=composer)\ntemp_loader = DataLoader(temp_dataset, batch_size=32)\nnormalizer = StepByStep.make_normalizer(temp_loader)\nnormalizer"
  },
  {
    "objectID": "mini-projects/rock_paper_scissors.html#real-dataset",
    "href": "mini-projects/rock_paper_scissors.html#real-dataset",
    "title": "Rock paper scissors",
    "section": "Real dataset",
    "text": "Real dataset\n\ncomposer = Compose([Resize(28), ToTensor(), normalizer])\ntrain_dataset = ImageFolder(root='rps', transform=composer)\nval_dataset = ImageFolder(root='rps-test-set', transform=composer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n\nimages, labels = next(iter(train_loader))\n\n\ntrain_loader\n\n<torch.utils.data.dataloader.DataLoader>\n\n\n\nlabels\n\ntensor([2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1])"
  },
  {
    "objectID": "mini-projects/09_bear_classifier/bear_classifier.html",
    "href": "mini-projects/09_bear_classifier/bear_classifier.html",
    "title": "Bear classifier",
    "section": "",
    "text": "See a deployed app here.\nThere will be two parts to it:\n- first part generates a model\n- second part deploys it as an app on HuggingFace (see code)"
  },
  {
    "objectID": "mini-projects/09_bear_classifier/bear_classifier.html#from-data-to-dataloaders",
    "href": "mini-projects/09_bear_classifier/bear_classifier.html#from-data-to-dataloaders",
    "title": "Bear classifier",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = bears.dataloaders(path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "mini-projects/09_bear_classifier/bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "mini-projects/09_bear_classifier/bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "Bear classifier",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\n\n\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.027743\n      0.179141\n      0.062500\n      01:22\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.114028\n      0.170333\n      0.026786\n      01:35\n    \n    \n      1\n      0.096802\n      0.224060\n      0.026786\n      01:28\n    \n    \n      2\n      0.081567\n      0.242517\n      0.026786\n      01:22\n    \n    \n      3\n      0.065939\n      0.244289\n      0.026786\n      01:20\n    \n  \n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=2, figsize=(10, 10))\n\n\n\n\n\n\n\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n#hide\nfor idx in cleaner.delete(): \n    cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): \n    shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "mini-projects/09_bear_classifier/bear_classifier.html#turning-your-model-into-an-online-application",
    "href": "mini-projects/09_bear_classifier/bear_classifier.html#turning-your-model-into-an-online-application",
    "title": "Bear classifier",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\n\nUsing the Model for Inference\n\nlearn.export('bears.pkl')\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('bears.pkl')]\n\n\n\nlearn_inf = load_learner(path/'bears.pkl')\n\n\nlearn_inf.predict('bear_images/grizzly.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([2.1257e-06, 1.0000e+00, 2.7545e-08]))\n\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\n\n\nCreating a Notebook App from the Model\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nbtn_upload = SimpleNamespace(data = ['bear_images/grizzly.jpg'])\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\n\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\n\n\n\n\n\n\n\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\n\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\n\nAll together:\n\npath = Path()\nlearn_inf = load_learner(path/'bears.pkl')\n\nout_pl = widgets.Output()\nbtn_upload = widgets.FileUpload()\nlbl_pred = widgets.Label()\nbtn_run = widgets.Button(description='Classify')\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nbtn_run.on_click(on_click_classify)\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])"
  },
  {
    "objectID": "mini-projects/binary_classification.html",
    "href": "mini-projects/binary_classification.html",
    "title": "Binary Classification",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom pytorched.step_by_step import StepByStep, RUNS_FOLDER_NAME\nimport platform\n\nimport datetime\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nWe’ll use Scikit-Learn’s make_moons to generate a toy dataset with 1000 data points and two features."
  },
  {
    "objectID": "mini-projects/binary_classification.html#data-generation",
    "href": "mini-projects/binary_classification.html#data-generation",
    "title": "Binary Classification",
    "section": "Data Generation",
    "text": "Data Generation\n\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=11)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n\nWe first use Scikit-Learn’s StandardScaler to standardize datasets:\n\nsc = StandardScaler()\nsc.fit(X_train)  # always fit only on X_train\nX_train_scalled = sc.transform(X_train)\nX_val_scalled = sc.transform(X_val)  # DO NOT use fit or fit_transform on X_val, it causes data leak\nm = sc.mean_\nv = sc.var_\nprint(m, v)\nassert ((X_train_scalled[0] - m)/np.sqrt(v) - X_train[0] < np.finfo(float).eps).all()\n\n[0.4866699  0.26184213] [0.80645937 0.32738853]\n\n\n\nX_train = X_train_scalled\nX_val = X_val_scalled\n\n\nfrom matplotlib.colors import ListedColormap\n\ndef figure1(X_train, y_train, X_val, y_val, cm_bright=None):\n    if cm_bright is None:\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)#, edgecolors='k')\n    ax[0].set_xlabel(r'$X_1$')\n    ax[0].set_ylabel(r'$X_2$')\n    ax[0].set_xlim([-2.3, 2.3])\n    ax[0].set_ylim([-2.3, 2.3])\n    ax[0].set_title('Generated Data - Train')\n\n    ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright)#, edgecolors='k')\n    ax[1].set_xlabel(r'$X_1$')\n    ax[1].set_ylabel(r'$X_2$')\n    ax[1].set_xlim([-2.3, 2.3])\n    ax[1].set_ylim([-2.3, 2.3])\n    ax[1].set_title('Generated Data - Validation')\n    fig.tight_layout()\n    \n    return fig\n\n\nfig = figure1(X_train, y_train, X_val, y_val)"
  },
  {
    "objectID": "mini-projects/binary_classification.html#data-preparation",
    "href": "mini-projects/binary_classification.html#data-preparation",
    "title": "Binary Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe preparation of data starts by converting the data points from Numpy arrays to PyTorch tensors and sending them to the available device:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Builds tensors from numpy arrays\nx_train_tensor = torch.as_tensor(X_train).float()\ny_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()  # reshape makes shape from (80,) to (80,1)\n\nx_val_tensor = torch.as_tensor(X_val).float()\ny_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n\n\ntrain_data = TensorDataset(x_train_tensor, y_train_tensor)\nval_data = TensorDataset(x_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_data, 64, shuffle=True)\nval_loader = DataLoader(val_data, 64)"
  },
  {
    "objectID": "mini-projects/binary_classification.html#linear-model",
    "href": "mini-projects/binary_classification.html#linear-model",
    "title": "Binary Classification",
    "section": "Linear model",
    "text": "Linear model\n\ntorch.manual_seed(42)\n\nlr = 0.01\n\nmodel = nn.Sequential()\nmodel.add_module('linear', nn.Linear(2,1))\nmodel.add_module('sigmoid', nn.Sigmoid())\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\nloss_fn = nn.BCELoss()\n\n\nsbs_lin = StepByStep(model, optimizer, loss_fn)\nsbs_lin.set_loaders(train_loader, val_loader)\nsbs_lin.train(100)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 42.69it/s]\n\n\n\n_ = sbs_lin.plot_losses()\n\n\n\n\nLet’s predict the values for X_train (y_train_predicted) and X_val (y_val_predicted) and plot them. Also let’s see how good of a job linear regression did using confusion matrix:\n\ndef predict_plot_count(sbs):\n    y_train_predicted = sbs.predict(X_train)\n    y_val_predicted = sbs.predict(X_val)\n    fig = figure1(X_train, y_train_predicted, X_val, y_val_predicted)\n    print('Confusion matrix:')\n    print(confusion_matrix(y_val, list(map(int, (y_val_predicted > 0.5).ravel()))))\n    print('Correct categories:')\n    print(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\npredict_plot_count(sbs_lin)\n\nConfusion matrix:\n[[82 14]\n [14 90]]\nCorrect categories:\ntensor([[ 82,  96],\n        [ 90, 104]])\n\n\n\n\n\nand we see there are some false positives and false negatives (off-diagonal elements)."
  },
  {
    "objectID": "mini-projects/binary_classification.html#two-layer-model",
    "href": "mini-projects/binary_classification.html#two-layer-model",
    "title": "Binary Classification",
    "section": "Two-layer model",
    "text": "Two-layer model\nLet’s make a better model.\n\nmodel_nonlin = nn.Sequential()\nmodel_nonlin.add_module('linear1', nn.Linear(2,10))\nmodel_nonlin.add_module('relu', nn.ReLU())\nmodel_nonlin.add_module('linear2', nn.Linear(10,1))\nmodel_nonlin.add_module('sigmoid', nn.Sigmoid())\noptimizer = optim.Adam(model_nonlin.parameters(), lr=lr)\n\nsbs_nonlin = StepByStep(model_nonlin, optimizer, loss_fn)\nsbs_nonlin.set_loaders(train_loader, val_loader)\nsbs_nonlin.train(100)\n_ = sbs_nonlin.plot_losses()\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.74it/s]\n\n\n\n\n\n\npredict_plot_count(sbs_nonlin)\n\nConfusion matrix:\n[[86 10]\n [10 94]]\nCorrect categories:\ntensor([[ 86,  96],\n        [ 94, 104]])\n\n\n\n\n\nAnd this is better (we could calculate precision and recall)."
  },
  {
    "objectID": "tips.html",
    "href": "tips.html",
    "title": "Tips",
    "section": "",
    "text": "Some people might call it cheat-sheet, utilities, TILs (“today-I-learned”), what matters is that these saved me a ton of time.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAugmentations\n\n\n\n\n\n\nAzure\n\n\n\n\n\n\nConvolutions\n\n\n\n\n\n\nCustom domain\n\n\n\n\n\n\nDatetime\n\n\n\n\n\n\nFastAI\n\n\n\n\n\n\nFileIO\n\n\n\n\n\n\nFunctools\n\n\n\n\n\n\nGit\n\n\n\n\n\n\nHuggingFace\n\n\n\n\n\n\nImages\n\n\n\n\n\n\nJarvislabs.ai\n\n\n\n\n\n\nJupyter General\n\n\n\n\n\n\nKaggle\n\n\n\n\n\n\nKernels\n\n\n\n\n\n\nLatex\n\n\n\n\n\n\nLoss\n\n\n\n\n\n\nMacOS\n\n\n\n\n\n\nMatplotlib\n\n\n\n\n\n\nMetrics\n\n\n\n\n\n\nModels\n\n\n\n\n\n\nNumpy\n\n\n\n\n\n\nPIL\n\n\n\n\n\n\nPaperspace.com\n\n\n\n\n\n\nPathlib\n\n\n\n\n\n\nPyData\n\n\n\n\n\n\nPyTest\n\n\n\n\n\n\nPython General\n\n\n\n\n\n\nRegex\n\n\n\n\n\n\nSciPy\n\n\n\n\n\n\nSet operations\n\n\n\n\n\n\nSet up env\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\nTensor\n\n\n\n\n\n\nVisualizations\n\n\n\n\n\n\nnbdev\n\n\n\n\n\n\nzsh\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "mini-projects.html",
    "href": "mini-projects.html",
    "title": "Mini-Projects",
    "section": "",
    "text": "Following mini-projects cover my journey through some excellent resources I’ve learned so much from:\n\nPyTorch: Step by Step books (3 of them) by Daniel Voigt Godoy\n\nDeep Learning course by Yann LeCun & Alfredo Canziani\n\nPractical Deep Learning for Coders course by Jeremy Howard.\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nImage Regression\n\n\n\n\n\nBiwi Kinect Head Pose dataset\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMulti-label classification\n\n\n\n\n\nPASCAL 2007 dataset\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPet Breed Classification\n\n\n\n\n\nMulti-class classification using FastAI\n\n\n\n\n\n\nMar 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKubernets\n\n\n\n\n\nOn Linode Kubernets Engine (LKE)\n\n\n\n\n\n\nMar 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBear classifier\n\n\n\n\n\nUsing FastAI\n\n\n\n\n\n\nFeb 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLabeling\n\n\n\n\n\nUsing LabelMe\n\n\n\n\n\n\nJan 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTransfer learning\n\n\n\n\n\nUsing pre-trained AlexNet\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRock paper scissors\n\n\n\n\n\nMulticlass image classification\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMulticlass image classification\n\n\n\n\n\nOn a simple image dataset\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTorchvision, Conversions, Samplers\n\n\n\n\n\nOn conversions and transforms\n\n\n\n\n\n\nNov 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBinary image classification\n\n\n\n\n\nOn a simple image dataset\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBinary Classification\n\n\n\n\n\nOn Scikit-Learn’s make_moons\n\n\n\n\n\n\nOct 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nQuadratic function fitting\n\n\n\n\n\nUsing deep learning\n\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\nUsing deep learning\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nenad Bozinovic",
    "section": "",
    "text": "Hi there and welcome to my website! Here you will find my work on deep learning projects and some smaller projects too. You can browse some publications, patents and media covers of my work, and some general software tips I accumulated over time. Feel free to contact me if you have any questions. Enjoy!"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#gz-files",
    "href": "tips/030_Terminal/zsh.html#gz-files",
    "title": "zsh",
    "section": "gz files",
    "text": "gz files\nTo zip/unzip gz:\nInstall gnu-tar if seeing warning with the tar, then use gtar:\nbrew install gnu-tar\nTo zip:\ngtar -zcvf myfolder.tar.gz myfolder\ngzip filename  # zip it back\ngzip -k filename  # to zip it and keep original\nto see the content of a zipped file:\ngtar -tf myfolder.tar.gz\nto unzip:\ntar -xf labeled_data.tar.gz\ntar -xf labeled_data.tar.gz -C /home/user/destination"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#zip-files",
    "href": "tips/030_Terminal/zsh.html#zip-files",
    "title": "zsh",
    "section": "zip files",
    "text": "zip files\nzip filename.zip file1.txt file2.txt file3.txt\nunzip -vl titanic.zip  # to see content without unzipping\nunzip filename.zip\nunzip filename.zip -d /home/user/destination"
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#rename-filesfolders",
    "href": "tips/030_Terminal/zsh.html#rename-filesfolders",
    "title": "zsh",
    "section": "Rename files/folders",
    "text": "Rename files/folders\nmv source dst # to move and/or rename directory"
  },
  {
    "objectID": "tips/010_Python/scipy.html",
    "href": "tips/010_Python/scipy.html",
    "title": "SciPy",
    "section": "",
    "text": "import numpy as np\nnp.random.seed(42)\na = np.random.randn(1,10)\nprint(a)\nprint(a.max())\nprint(a.dtype)\na = a.astype(np.uint8)\nprint(a)\nprint(a.max())\nprint(a.dtype)\nprint(a.shape)\na = a.squeeze()\nprint(a.shape)\nTo scale tensor from 0, 255, convert to uint8\nb = (np.array(mask_tensor) / np.array(mask_tensor).max() * 255).astype(np.uint8).squeeze()\nb.max()\nb = (np.array(mask_tensor) / np.array(mask_tensor).max()).astype(np.float32).squeeze()\nb.max()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial import polynomial as P\n\nM = 100;\nN = 4;\nx = np.linspace(-20, 20, M)\nX = np.fliplr(np.vander(x , N + 1))\n# for i in range(N+1):\n#     X[:,i] = x**i\nnoise = 20 * np.random.randn(M)\nbeta = [18, -12, 2, 0.1, 0.1];\ny = np.dot(X, beta) + noise\n\nbeta_r = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\ndef plot_something(p):\n    fig, ax = plt.subplots()\n    ax.plot(p, 'o')\n    ax.set_title('Random')\n    plt.show()\n\n\nbeta_r, stats = P.polyfit(x, y, 4, full=True)\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\nhelp(P.polyfit)\n\n\n\n\n\n\n\nHelp on function polyfit in module numpy.polynomial.polynomial:\n\npolyfit(x, y, deg, rcond=None, full=False, w=None)\n    Least-squares fit of a polynomial to data.\n    \n    Return the coefficients of a polynomial of degree `deg` that is the\n    least squares fit to the data values `y` given at points `x`. If `y` is\n    1-D the returned coefficients will also be 1-D. If `y` is 2-D multiple\n    fits are done, one for each column of `y`, and the resulting\n    coefficients are stored in the corresponding columns of a 2-D return.\n    The fitted polynomial(s) are in the form\n    \n    .. math::  p(x) = c_0 + c_1 * x + ... + c_n * x^n,\n    \n    where `n` is `deg`.\n    \n    Parameters\n    ----------\n    x : array_like, shape (`M`,)\n        x-coordinates of the `M` sample (data) points ``(x[i], y[i])``.\n    y : array_like, shape (`M`,) or (`M`, `K`)\n        y-coordinates of the sample points.  Several sets of sample points\n        sharing the same x-coordinates can be (independently) fit with one\n        call to `polyfit` by passing in for `y` a 2-D array that contains\n        one data set per column.\n    deg : int or 1-D array_like\n        Degree(s) of the fitting polynomials. If `deg` is a single integer\n        all terms up to and including the `deg`'th term are included in the\n        fit. For NumPy versions >= 1.11.0 a list of integers specifying the\n        degrees of the terms to include may be used instead.\n    rcond : float, optional\n        Relative condition number of the fit.  Singular values smaller\n        than `rcond`, relative to the largest singular value, will be\n        ignored.  The default value is ``len(x)*eps``, where `eps` is the\n        relative precision of the platform's float type, about 2e-16 in\n        most cases.\n    full : bool, optional\n        Switch determining the nature of the return value.  When ``False``\n        (the default) just the coefficients are returned; when ``True``,\n        diagnostic information from the singular value decomposition (used\n        to solve the fit's matrix equation) is also returned.\n    w : array_like, shape (`M`,), optional\n        Weights. If not None, the weight ``w[i]`` applies to the unsquared\n        residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n        chosen so that the errors of the products ``w[i]*y[i]`` all have the\n        same variance.  When using inverse-variance weighting, use\n        ``w[i] = 1/sigma(y[i])``.  The default value is None.\n    \n        .. versionadded:: 1.5.0\n    \n    Returns\n    -------\n    coef : ndarray, shape (`deg` + 1,) or (`deg` + 1, `K`)\n        Polynomial coefficients ordered from low to high.  If `y` was 2-D,\n        the coefficients in column `k` of `coef` represent the polynomial\n        fit to the data in `y`'s `k`-th column.\n    \n    [residuals, rank, singular_values, rcond] : list\n        These values are only returned if ``full == True``\n    \n        - residuals -- sum of squared residuals of the least squares fit\n        - rank -- the numerical rank of the scaled Vandermonde matrix\n        - singular_values -- singular values of the scaled Vandermonde matrix\n        - rcond -- value of `rcond`.\n    \n        For more details, see `numpy.linalg.lstsq`.\n    \n    Raises\n    ------\n    RankWarning\n        Raised if the matrix in the least-squares fit is rank deficient.\n        The warning is only raised if ``full == False``.  The warnings can\n        be turned off by:\n    \n        >>> import warnings\n        >>> warnings.simplefilter('ignore', np.RankWarning)\n    \n    See Also\n    --------\n    numpy.polynomial.chebyshev.chebfit\n    numpy.polynomial.legendre.legfit\n    numpy.polynomial.laguerre.lagfit\n    numpy.polynomial.hermite.hermfit\n    numpy.polynomial.hermite_e.hermefit\n    polyval : Evaluates a polynomial.\n    polyvander : Vandermonde matrix for powers.\n    numpy.linalg.lstsq : Computes a least-squares fit from the matrix.\n    scipy.interpolate.UnivariateSpline : Computes spline fits.\n    \n    Notes\n    -----\n    The solution is the coefficients of the polynomial `p` that minimizes\n    the sum of the weighted squared errors\n    \n    .. math:: E = \\sum_j w_j^2 * |y_j - p(x_j)|^2,\n    \n    where the :math:`w_j` are the weights. This problem is solved by\n    setting up the (typically) over-determined matrix equation:\n    \n    .. math:: V(x) * c = w * y,\n    \n    where `V` is the weighted pseudo Vandermonde matrix of `x`, `c` are the\n    coefficients to be solved for, `w` are the weights, and `y` are the\n    observed values.  This equation is then solved using the singular value\n    decomposition of `V`.\n    \n    If some of the singular values of `V` are so small that they are\n    neglected (and `full` == ``False``), a `RankWarning` will be raised.\n    This means that the coefficient values may be poorly determined.\n    Fitting to a lower order polynomial will usually get rid of the warning\n    (but may not be what you want, of course; if you have independent\n    reason(s) for choosing the degree which isn't working, you may have to:\n    a) reconsider those reasons, and/or b) reconsider the quality of your\n    data).  The `rcond` parameter can also be set to a value smaller than\n    its default, but the resulting fit may be spurious and have large\n    contributions from roundoff error.\n    \n    Polynomial fits using double precision tend to \"fail\" at about\n    (polynomial) degree 20. Fits using Chebyshev or Legendre series are\n    generally better conditioned, but much can still depend on the\n    distribution of the sample points and the smoothness of the data.  If\n    the quality of the fit is inadequate, splines may be a good\n    alternative.\n    \n    Examples\n    --------\n    >>> np.random.seed(123)\n    >>> from numpy.polynomial import polynomial as P\n    >>> x = np.linspace(-1,1,51) # x \"data\": [-1, -0.96, ..., 0.96, 1]\n    >>> y = x**3 - x + np.random.randn(len(x))  # x^3 - x + Gaussian noise\n    >>> c, stats = P.polyfit(x,y,3,full=True)\n    >>> np.random.seed(123)\n    >>> c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1\n    array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286]) # may vary\n    >>> stats # note the large SSR, explaining the rather poor results\n     [array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316, # may vary\n              0.28853036]), 1.1324274851176597e-014]\n    \n    Same thing without the added noise\n    \n    >>> y = x**3 - x\n    >>> c, stats = P.polyfit(x,y,3,full=True)\n    >>> c # c[0], c[2] should be \"very close to 0\", c[1] ~= -1, c[3] ~= 1\n    array([-6.36925336e-18, -1.00000000e+00, -4.08053781e-16,  1.00000000e+00])\n    >>> stats # note the minuscule SSR\n    [array([  7.46346754e-31]), 4, array([ 1.38446749,  1.32119158, # may vary\n               0.50443316,  0.28853036]), 1.1324274851176597e-014]\n\n\n\n\n\n\nSome random image manipulations:\n\nimport cv2\n\nvideo = cv2.VideoCapture(0)\n\nfirst_frame = None\n\nwhile True:\n\n    check, frame = video.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    if first_frame is None:\n        first_frame = gray\n        continue\n\n\n    delta_frame = cv2.absdiff(first_frame, gray)\n    thresh_frame = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n    thresh_frame = cv2.dilate(thresh_frame, None, iterations=2)\n\n    ctns, _ = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n\n    for contour in ctns:\n        if cv2.contourArea(contour) < 1000:\n            continue\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n\n    cv2.imshow(\"Gray frame\", gray)\n    cv2.imshow(\"Delta frame\", delta_frame)\n    cv2.imshow(\"Threshold frame\", thresh_frame)\n    cv2.imshow(\"Color frame\", frame)\n\n    key = cv2.waitKey(10)\n\n    if key == ord('q'):\n        break\n\nvideo.release()\n\n\n\n\nTo read the csv file:\n\ndf = pd.read_csv(file_str, header=1, names=['time', 'a', 'b', 'c', 'd', 'e'])"
  }
]