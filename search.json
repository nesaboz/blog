[
  {
    "objectID": "tips/030_Terminal/environments.html",
    "href": "tips/030_Terminal/environments.html",
    "title": "Environments",
    "section": "",
    "text": "First and foremost, add AWS access keys to env variables to local machine, ideally in ~/.zsh (or whatver your $SHELL is).\nexport AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nthese can be found in the AWS console under IAM -&gt; Users -&gt; Security credentials.",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/030_Terminal/environments.html#ssh-and-connect",
    "href": "tips/030_Terminal/environments.html#ssh-and-connect",
    "title": "Environments",
    "section": "ssh and connect",
    "text": "ssh and connect\nOptionally, create and assign elastic IP to the instance (cost is $3.5 per month) for easier access.\nIf you have a pem key you can just log in with:\nssh -i my-aws-gpu-machine.pem ubuntu@54.151.70.169\nor if you make a ~/.ssh/config file like this:\nHost my-aws-gpu-instance \n  HostName 54.151.70.169\n  User ubuntu\n  IdentityFile ~/.ssh/my-aws-gpu-instance.pem   # if you lose this you can use the trick below\nthen just ssh my-aws-gpu-instance.\nIf you happen to lose pem file, just: - create a key pair run this on LOCAL machine (skip this if already exists and just copy it: cat ~/.ssh/id_rsa.pub) bash     ssh-keygen -t rsa -b 4096 -C \"&lt;pick a name of a machine you are running this command on&gt;\" - log into remote machine using ec2 instance connect - copy LOCAL public key to authorized_keys: echo public_key_string &gt;&gt; ~/.ssh/authorized_keys or use nano editor. - change permissions on REMOTE machine: bash     chmod 700 ~/.ssh     chmod 600 ~/.ssh/authorized_keys",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/030_Terminal/environments.html#add-github-access",
    "href": "tips/030_Terminal/environments.html#add-github-access",
    "title": "Environments",
    "section": "Add github access",
    "text": "Add github access\nTake a public key from any machine and copy/paste into GitHub - Settings - SSH and GPG keys - New SSH key.",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/030_Terminal/environments.html#anything-gpu",
    "href": "tips/030_Terminal/environments.html#anything-gpu",
    "title": "Environments",
    "section": "Anything GPU",
    "text": "Anything GPU\nTo see recommended drivers:\nsudo apt install ubuntu-drivers-common\nubuntu-drivers devices\nfor T4 GPU, the recommended driver is 440. To install the latest driver, run the following commands:\nsudo apt update\nsudo apt-get upgrade   # conservative upgrade\nsudo apt-get dist-upgrade   # aggressive upgrade, be careful\nsudo apt install nvidia-driver-545\nsudo reboot  # don't forget to reboot, might even need to purge old drivers via `sudo apt-get purge nvidia-driver-XXX`\nYou can now see stats:\nlspci | grep -i nvidia\nnvidia-smi\nyou should see something like this:\nNVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/030_Terminal/environments.html#install-aws-cli",
    "href": "tips/030_Terminal/environments.html#install-aws-cli",
    "title": "Environments",
    "section": "Install AWS CLI",
    "text": "Install AWS CLI\nsudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nsource ~/.bashrc\nrm awscliv2.zip\naws --version\naws-cli/2.15.41 Python/3.11.8 Linux/6.5.0-1018-aws exe/x86_64.ubuntu.22 prompt/off",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/030_Terminal/environments.html#if-you-must-get-conda",
    "href": "tips/030_Terminal/environments.html#if-you-must-get-conda",
    "title": "Environments",
    "section": "If you must get Conda",
    "text": "If you must get Conda\nGet mamba which is a faster version of conda from here:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\nor just get miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh\nInstall it via:\nsource &lt;file you want to run&gt;.sh\nUpdate mamba/conda:\nconda update -n base -c conda-forge conda\nInit conda:\nconda init zsh  # Must restart terminal if running this one.\n\nNote for Windows\nInstall WSL for Windows:\nwsl --install Ubuntu\nwill need to add both conda and mamba to PATH by editing ~/.bashrc file and adding these two lines to the end:\nsource \"${CONDA PATH}/etc/profile.d/conda.sh\"\nsource \"${CONDA PATH}/etc/profile.d/mamba.sh\"\nAlso to set up default terminal in VSCode follow quick instructions here.\nWindows C hard drive will be by default mounted in /mnt/c/ folder.\nUse PowerToys/Keyboard Manager to remap the keyboard.",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "Environments"
    ]
  },
  {
    "objectID": "tips/050_FastAI/azure.html",
    "href": "tips/050_FastAI/azure.html",
    "title": "Azure",
    "section": "",
    "text": "To see keys that one can use to run Bing API go here",
    "crumbs": [
      "Projects",
      "TILs",
      "FastAI",
      "Azure"
    ]
  },
  {
    "objectID": "tips/050_FastAI/huggingface.html",
    "href": "tips/050_FastAI/huggingface.html",
    "title": "HuggingFace",
    "section": "",
    "text": "Couldn’t upload large files without git-lfs (lfs stands for large file storage)\nbrew install git-lfs\ngit lfs install\nHugging space doesn’t allow binary files in repo, so the file extensions needs to be added to .gitattributes.\nWe must have requirements.txt file in the hugging space repo:\n\"If you need other Python packages to run your app, add them to a requirements.txt file at the root of the repository. The Spaces runtime engine will create a custom environment on-the-fly.\"",
    "crumbs": [
      "Projects",
      "TILs",
      "FastAI",
      "HuggingFace"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/general.html",
    "href": "tips/020_Jupyter/general.html",
    "title": "Jupyter General",
    "section": "",
    "text": "To see parameters: SHIFT + TAB For help use ? before or after the command. For autocompletion: TAB + TAB Merge cells, select cells then: Shift + M `To see all available functions in numpy that havesinin their name:np.sin?`",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Jupyter General"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/general.html#help",
    "href": "tips/020_Jupyter/general.html#help",
    "title": "Jupyter General",
    "section": "",
    "text": "To see parameters: SHIFT + TAB For help use ? before or after the command. For autocompletion: TAB + TAB Merge cells, select cells then: Shift + M `To see all available functions in numpy that havesinin their name:np.sin?`",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Jupyter General"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/general.html#magic-commands",
    "href": "tips/020_Jupyter/general.html#magic-commands",
    "title": "Jupyter General",
    "section": "Magic commands",
    "text": "Magic commands\nTo autoreload edited python files (doesn’t include class signatures changes, these you have to re-import):\n%load_ext autoreload\n%autoreload 2",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Jupyter General"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/matplotlib.html",
    "href": "tips/020_Jupyter/matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n\nnp.random.randn()\n\n-2.5106811179598463\n\n\n\nx = np.random.rand(10,1)\ny = np.random.rand(10,1)\nplt.plot(x,y,'.')\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_means_and_stdevs(x1, y1, x2, y2):\n    fig, axs = plt.subplots(1,3, figsize=(10,3))\n    for i in range(3):\n        axs[i].plot(x1[:,i], y1[:,i], '.')\n        axs[i].plot(x2[:,i], y2[:,i], '.')\n        axs[i].set_xlabel('means')\n        axs[i].set_ylabel('stdev')\n        axs[i].set_title(f'Channel #{i+1}')\n        axs[i].legend(['eval', 'train'])\n    plt.show()\n\n\nx1 = np.random.rand(10,3)\ny1 = np.random.rand(10,3)\nx2 = np.random.rand(10,3)\ny2 = np.random.rand(10,3)\nplot_means_and_stdevs(x1, y1, x2, y2)\n\n\n\n\n\n\n\n\nA somewhat quicker method without using axes and figsize is to use plt.subplot (note that index is 1-based):\n\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.plot(x1[:,i], y1[:,i], '.')\n    plt.plot(x2[:,i], y2[:,i], '.')\n    plt.xlabel('means')\n    plt.ylabel('stdev')\n    plt.title(f'Channel #{i+1}')\n    plt.legend(['eval', 'train'])\nplt.show()\n\n\n\n\n\n\n\n\nTo remove axes use:\n\nplt.axis('off')\n\n\n\n\n\n\n\n\nTo save image:\n\nplt.savefig(\"test.png\", bbox_inches='tight')\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Matplotlib"
    ]
  },
  {
    "objectID": "tips/OOP/elevator.html",
    "href": "tips/OOP/elevator.html",
    "title": "Design Elevator system",
    "section": "",
    "text": "from IPython.display import display, Image\nimport plantuml\n\nPlantUML\n\ndef generate_uml_diagram():\n    uml_text = \"\"\"\n    @startuml\n    class Elevator {\n        +capacity: Int\n        +inside_panel\n        +current_floor\n        current_state: Enum(going_up, going_down, idle)\n        go_to_floor()\n        open_door()\n        close_door()\n    }\n\n    class Button {\n        \n    }\n\n    class Floor {\n        number\n        outside_panel\n        up_pressed()\n        down_pressed()\n    }\n\n    class OutsidePanel {\n        buttons: dict()\n        up_pressed()\n        down_pressed()\n    }\n    \n    class InsidePanel {\n        buttons: dict()\n        floor_pressed()\n        open_door_pressed()\n        close_door_pressed()\n        emergency_pressed()\n    }\n    \n    class Orchestrator {\n        elevators: list(Elevator)\n        floors: list(floors)\n        passengers: list(int)\n        + outside_panel_button_pressed(floor, direction)\n        + inside_panel_button_pressed(elevator_id, button)\n        dispatch_elevator: (direction, floor) : elevator \n        }\n\n    class User {\n        press_up()\n        press_down()\n        }\n\n        \n    Elevator \"1\" *-- \"1\" InsidePanel\n    Elevator \"1\" *-- \"1\" OutsidePanel\n    \n    \n    \n    \n    @enduml\n    \"\"\"\n    # note right of Orchestrator : this is where all the logic happens, this is complex optimization problem\n    # @enduml\n    # \"\"\"\n    plantuml_obj = plantuml.PlantUML(url='http://www.plantuml.com/plantuml/img/')\n    result = plantuml_obj.processes(uml_text)\n    return result\n\n# Call the function and save or display the diagram\ndiagram_image = generate_uml_diagram()\n\ndisplay(Image(data=diagram_image))"
  },
  {
    "objectID": "tips/015_Cloud/aws/ec2_image_builder.html",
    "href": "tips/015_Cloud/aws/ec2_image_builder.html",
    "title": "Creating an Image Pipeline with EC2 Image Builder",
    "section": "",
    "text": "Credits Educative.io\nLet’s create our own image that we can use to create EC2 instances with pre-installed Python and boto3. We will do this via EC2 Image Builder.\n\nWe create a SNS topic and a create an email subscriber to it (arn:aws:sns:us-east-1:332498689295:CL_SNS_Topic)\nThen we create a IAM role for EC2 (under “AWS service”) and assign “EC2InstanceProfileForImageBuilder” and “EC2InstanceProfileForImageBuilder” policies.\nWe create build component that’s used to specify the image operating system, configure settings, and perform any tasks that are necessary while creating an image. We do this via “EC2 Image Builder” with the following definition document:\n\nname: LinuxWithBoto3\nschemaVersion: 1.0\nphases:\n  - name: build\n    steps:\n      - name: Boto3\n        action: ExecuteBash\n        inputs:\n          commands:\n            - 'sudo yum install python3 -y'\n            - 'sudo yum install gcc openssl-devel bzip2-devel libffi-devel zlib-devel -y'\n            - 'cd /opt'\n            - 'sudo wget https://www.python.org/ftp/python/3.8.12/Python-3.8.12.tgz'\n            - 'sudo tar xzf Python-3.8.12.tgz'\n            - 'cd Python-3.8.12'\n            - 'sudo ./configure --enable-optimizations'\n            - 'sudo make altinstall'\n            - 'pip3.8 install pip --upgrade'\n            - 'pip3.8 install boto3'\n\nNow we create image pipeline, also under “EC2 Image Builder”, with Amazon Machine Image (AMI) (and not Docker Image). This process has 6 steps:\n\nselect build component (the one we created above),\nselect recipe (that will also select SNS topic we create above to receive notifications about Image Builder process)\ninfrastructure configurations (optional)\ndistribution settings (optional) We then build and “Run pipeline” which might take 15-20 minutes to build. When it’s done we now have Image that has installed Python and boto3 (and of course can have anything we need as well).\n\n\nWe can now go to EC2, create instance, and select image that we created in a previous step. If we now connect to the instance we can see that Python, boto3 are pre-installed, which was the goal.\nTo add more packages, we can add them to the build component definition document (save it as a new version), and then rebuild the image.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Creating an Image Pipeline with EC2 Image Builder"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_rds.html",
    "href": "tips/015_Cloud/aws/aws_rds.html",
    "title": "AWS RDS",
    "section": "",
    "text": "Credits Educative.io\nBefore we can start working with RDS, we’ll have to set up other resources that are required while creating an RDS DB instance. The first of those resources is VPC (virtual private cloud). An RDS DB instance must be created inside a VPC. This is to enhance security and provide network isolation.\nWhen setting VPC name will autotag -vpc. Set “Number of Availability Zones (AZs)”. “None” for the “VPC” endpoints.\nNow we create RDS, from the “Templates” section, select the “Dev/Test” option because it’ll allow you to select the deployment options. Choose: “Single DB instance”, “Multi-AZ DB instance”, or “Multi-AZ DB cluster” Burstable classes (includes t classes) We can now check the status of the RDS using EC2 instance:\nchmod 400 usercode/RDS-client-key-pair.pem\nssh -i usercode/RDS-client-key-pair.pem ubuntu@{public-ipv4-address}\nsudo apt update\nsudo apt install mysql-client-core-8.0\nmysql -h {endpoint} -P 3306 -u admin -p\naws rds describe-db-instances --db-instance-identifier mysql8",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS RDS"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/certificates/practitioner/cloud_practitioner.html",
    "href": "tips/015_Cloud/aws/certificates/practitioner/cloud_practitioner.html",
    "title": "4. IAM",
    "section": "",
    "text": "EC2 User Data script can bootstrap the instance. It is executed only once when the instance is launched.\nExample: m5.xlarge: m is instance class, 5 is the generation, xlarge is the size.\n\nc class: compute optimized\nR, X, High Memory, z1d type: RAM optimized, for in memory databases, cache, real time big data\n‘I’, ‘D’, ‘H1’: storage intensive (databases, cache, etc.)\nand many more: ec2instances info\n\n\n\n\nsecurity group is a firewall, regulate ports, IP ranges, inbound and outbound traffic\n0.0.0.0/0 means all IP addresses\ncan be attached to multiple instances\nlocekd to the region\nlives outside the instance\ngood to maintain a security group for SSH access\nany timeout is security group issue\nconnection refused is instance issue\nby default all inbound is closed, all outbound is open\nsecurity group can reference another security group, this is good to allow bunch of instances to talk to - each other in groups\n22 is SSH\n21 is FTP\n80 is HTTP unsecured websites\n443 is HTTPS secured websites\n3389 is RDP (for Windows)\n\nIf you lose Key pair pem file you will need to create a new public one from it like:\nssh-keygen -y -f Test.pem &gt; Test.pub\ncat Test.pub\nthen connect to the instance using browser console and add the public key to the authorized_keys file:\nnano ~/.ssh/authorized_keys\nwith that you can connect to the instance from local machine::\nchmod 400 Test.pem  # to change the permissions of the key\nssh -i Test.pem ubuntu@&lt;public IP&gt;  # for for Ubuntu and for Amazon Linux AMI use `ec2-user`\nAlways attach roles to the instance to give access to other AWS services.\n\nOn-demand, Reserved, Spot, and Dedicated Hosts are the pricing models for EC2 instances."
  },
  {
    "objectID": "tips/015_Cloud/aws/certificates/practitioner/cloud_practitioner.html#autoscaling-group-asg",
    "href": "tips/015_Cloud/aws/certificates/practitioner/cloud_practitioner.html#autoscaling-group-asg",
    "title": "4. IAM",
    "section": "Autoscaling group (ASG)",
    "text": "Autoscaling group (ASG)\nASG embodies elasticity, across multiple AZ. There is minimum, desired, and maximum number of instances. We need to set up template, type, security group, EBS, the usual. The ASG will now always try to keep desired number of instances even if they crash. There are scaling options (Manual, Dynamic(Simple, Target, Schedule), and Predictive)."
  },
  {
    "objectID": "tips/015_Cloud/aws/queues.html",
    "href": "tips/015_Cloud/aws/queues.html",
    "title": "AWS SQS, SNS, and Kafka",
    "section": "",
    "text": "Credits Educative.io\nHere we demo some message queue and pub/sub platforms: - AWS SQS - AWS SNS - Kafka\nWe first create a standard SQS, then we send a message to it, and print message ID. Then we create AWS SNS which is great for prototyping and smaller projects. Finally, we demo Kafka which is fast, large scale industry standard.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS SQS, SNS, and Kafka"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/queues.html#sns",
    "href": "tips/015_Cloud/aws/queues.html#sns",
    "title": "AWS SQS, SNS, and Kafka",
    "section": "SNS",
    "text": "SNS\nLambda will be publisher (and will trigger it using EventBridge scheduler), and we’ll have two subscribers, SQS queue and an email. Next IAM role (add SNSPublishManagedPolicy to the IAM role and assign to Lambda).\nimport json\nimport boto3\n\ndef lambda_handler(event, context):\n   client = boto3.client('sns')\n   Topic_ARN = \"&lt;SNS Topic ARN Here&gt;\"  # looks like this: arn:aws:sns:us-east-1:&lt;XYZ&gt;:ReminderTopic\n   response_one = client.publish (\n      TargetArn = Topic_ARN,\n      Message = json.dumps({'Reminder-Type': \"Weekly Reminder\", 'Reminder': \"Reminder 1\", 'Destination': \"Email\" }),\n   )\n   response_two = client.publish (\n      TargetArn = Topic_ARN,\n      Message = json.dumps({'Reminder-Type': \"Weekly Reminder\", 'Reminder': \"Reminder 2\", 'Destination': \"Email\"}),\n   )\n   response_three = client.publish (\n      TargetArn = Topic_ARN,\n      Message = json.dumps({'Reminder-Type': \"Daily Reminder\", 'Reminder': \"Reminder 3\", 'Destination': \"SQS\"}),\n   )\n   return {\n      'statusCode': 200,\n      'body': json.dumps({'response_one': response_one, 'response_two': response_two, 'response_three': response_three })\n   }\n\nPublish\nBy default, only the owner of the SNS topic can publish messages to the topic, so we need to change SNS access policy:\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"ReminderID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"SNS topic policy\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Action\": \"SNS:Publish\",\n      \"Resource\": \"&lt;Provide Lambda Function ARN&gt;\",   # arn:aws:lambda:us-east-1:XYZ:function:ReminderFunction\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:SourceAccount\": \"&lt;Provide Account ID without Dashes&gt;\"\n        }\n      }\n    }\n  ]\n}",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS SQS, SNS, and Kafka"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/queues.html#subscribe",
    "href": "tips/015_Cloud/aws/queues.html#subscribe",
    "title": "AWS SQS, SNS, and Kafka",
    "section": "Subscribe",
    "text": "Subscribe\nWe will subscribe SQS and Email. Add their ARNs under SNS subscribers tab.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS SQS, SNS, and Kafka"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/queues.html#schedule-eventbridge-to-invoke-lambda",
    "href": "tips/015_Cloud/aws/queues.html#schedule-eventbridge-to-invoke-lambda",
    "title": "AWS SQS, SNS, and Kafka",
    "section": "Schedule EventBridge to invoke Lambda",
    "text": "Schedule EventBridge to invoke Lambda\nWe can add a trigger to set messages regularly:\n\n\n\n\n\n\n\n\ndatetime\ncron\n\n\n\n\n\ncron(minutes hours “day of the month” month “day of the week” year)\n\n\n27th December, 2023. AT 5:00 PM\ncron(0 17 27 DEC ? 2023)\n\n\nEvery 30 minutes on every Sunday\ncron(30 * ? * SUN *)\n\n\nEvery Monday to Friday, At 8:30 PM\ncron(30 20 ? * MON-FRI *)",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS SQS, SNS, and Kafka"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/cloudwatch.html",
    "href": "tips/015_Cloud/aws/cloudwatch.html",
    "title": "Nenad Bozinovic",
    "section": "",
    "text": "Install stress library (not complete commands maybe):\nsudo apt-get install stress\nsudo apt-get install htop\nRun stress:\nstress --cpu 6 --vm-bytes $(awk '/MemAvailable/{printf \"%d\\n\", $2 * 0.75;}' &lt; /proc/meminfo)k --vm-keep -m 1"
  },
  {
    "objectID": "tips/015_Cloud/aws/lambda.html",
    "href": "tips/015_Cloud/aws/lambda.html",
    "title": "AWS Lambdas with layers and from containers",
    "section": "",
    "text": "Credits Educative.io\nAWS Lambda function is great to run something small and fast. In case where you need to use some external libraries, you can use layers or the whole container. Layers are zip files that contain libraries, custom runtimes, or other dependencies. You can upload and use your own layers with your function or use layers from AWS Lambda layers library. We’ll now try all three options.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Lambdas with layers and from containers"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/lambda.html#mix-code-and-libraries",
    "href": "tips/015_Cloud/aws/lambda.html#mix-code-and-libraries",
    "title": "AWS Lambdas with layers and from containers",
    "section": "1. Mix code and libraries",
    "text": "1. Mix code and libraries\nFirst we will directly incorporate the requests library into our lambda function:\nInstall virtual env and requests:\npython3.11 -m venv virtual_env && source virtual_env/bin/activate && pip install requests && deactivate\nCreate zip file:\ncd virtual_env/lib/python3.11/site-packages && zip -r requests_package.zip . && cp requests_package.zip /usercode/LambdaFolder && cd /usercode/LambdaFolder\nNow add lambda function and HTML template to the requests_package.zip:\nzip requests_package.zip lambda_function.py template.html\nWe can now deploy requests_package.zip, that again has mixed code and dependencies:\naws lambda update-function-code --function-name function_call_api --zip-file fileb://requests_package.zip  --no-cli-pager\nLambda function:\nimport requests\n\ndef lambda_handler(event, context):\n    api_endpoint = 'XYZ'\n    api_key = 'XYZ'\n\n    try:\n        header_parameters = {\n            'Content-Type': 'application/json',\n            'x-api-key': api_key,\n        }\n\n        response = requests.get(api_endpoint, headers=header_parameters)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        data = response.json()[0]  # Reading the first item in the list\n        \n        # Read the HTML template from the file\n        with open('template.html', 'r') as template_file:\n            html_template = template_file.read()\n\n        # Replace placeholders with actual quote data and author name\n        html_response = html_template.replace('{{QUOTE}}', data['content'])\n        html_response = html_response.replace('{{AUTHOR}}', data['author'])\n        \n        return {\n            'statusCode': 200,\n            'headers': {\n                'Content-Type': 'text/html',\n            },\n            'body': html_response\n        }\n    except requests.exceptions.RequestException as error:\n        print(error)\n        raise Exception('An error occurred while fetching data')",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Lambdas with layers and from containers"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/lambda.html#use-layers",
    "href": "tips/015_Cloud/aws/lambda.html#use-layers",
    "title": "AWS Lambdas with layers and from containers",
    "section": "2. Use layers",
    "text": "2. Use layers\nNow we are going to do the same but instead of mixing code and libraries, we are going to push libraries to a layer:\npip install requests -t ./requestsFolder/python\ncd requestsFolder\nzip -r requests_layer.zip .\naws lambda publish-layer-version --layer-name requests_layer --zip-file fileb://requests_layer.zip --compatible-runtimes python3.11 --no-cli-pager\n# or by specifying region, one can do the same via aws configure\naws lambda publish-layer-version --layer-name requests_layer --zip-file fileb://requests_layer.zip --compatible-runtimes python3.11 --no-cli-pager --region us-east-1\nNow, we attach the layer to the lambda function. That’s it. We now have pre-installed libraries (like requests) available to the lambda function.\nSide note: to get some random quote:\ncurl \"http://api.forismatic.com/api/1.0/?method=getQuote&lang=en&format=json\"",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Lambdas with layers and from containers"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/lambda.html#use-container",
    "href": "tips/015_Cloud/aws/lambda.html#use-container",
    "title": "AWS Lambdas with layers and from containers",
    "section": "3. Use container",
    "text": "3. Use container\nLet’s create Amazon Elastic Container Registry (ECR), then we install Docker in Cloud9 and push our image to ECR.\nsudo apt-get update\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nsudo systemctl start docker\nsudo systemctl enable docker\ndocker --version\nWe now create application.py and Dockerfile and any Lambda will automatically have installed libraries specified in a Dockerfile, and code from application.py.\nNow we push, this is simple using view push commands in ECR console.\nWe can now create lambda function from container image.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Lambdas with layers and from containers"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/step_functions.html",
    "href": "tips/015_Cloud/aws/step_functions.html",
    "title": "Step Function project",
    "section": "",
    "text": "The goal is to have a webapp where user enters any language and we detect the sentiment. We also want to store the input and sentiment to DynamoDB as well as send an email.\nWe’ll host the app on EC2, it will be a simle node.js app, when a user passes input we’ll call a API. Side thing: we also create a SNS subscription that we’ll inform us to email when user enters something and create a DynamoDB table that will store the inputs, we just need to provide the primary key (which we can use Time as a string to keep things simple).",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Step Function project"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/step_functions.html#aws-lambda",
    "href": "tips/015_Cloud/aws/step_functions.html#aws-lambda",
    "title": "Step Function project",
    "section": "AWS Lambda",
    "text": "AWS Lambda\nNow let’s create a Lambda function that will do the “heavier lifting”, namely calling AWS Comprehend to get the sentiment.\nimport boto3\nimport json\n\ndef lambda_handler(event, context):\n    step_functions = boto3.client('stepfunctions')\n    comprehend = boto3.client('comprehend')\n    \n    input_text = event['Review']\n    response = comprehend.detect_sentiment(Text=input_text, LanguageCode='en')\n    sentiment_result = response['Sentiment']\n    \n    task_token = event['MyTaskToken']\n    \n    # Send the result in a format that matches the expected path\n    result_payload = {\"Payload\": {\"Sentiment\": sentiment_result}}\n    print(result_payload)\n        \n    step_functions.send_task_success(\n        taskToken=task_token,\n        output=json.dumps(result_payload)\n    )",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Step Function project"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/step_functions.html#step-function",
    "href": "tips/015_Cloud/aws/step_functions.html#step-function",
    "title": "Step Function project",
    "section": "Step Function",
    "text": "Step Function\nAnd we are all set to make FeedbackStateMachine StepFunction. This is not as straightforward as this is written in a completely new language, UI is there to help but still, lot’s of tricks. the ultimate code looks like this:\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"DetectDominantLanguage\",\n  \"States\": {\n    \"DetectDominantLanguage\": {\n      \"Type\": \"Task\",\n      \"Parameters\": {\n        \"Text.$\": \"$.body\"\n      },\n      \"Resource\": \"arn:aws:states:::aws-sdk:comprehend:detectDominantLanguage\",\n      \"Next\": \"Choice\",\n      \"ResultPath\": \"$.New\"\n    },\n    \"Choice\": {\n      \"Type\": \"Choice\",\n      \"Choices\": [\n        {\n          \"Variable\": \"$.New.Languages[0].LanguageCode\",\n          \"StringMatches\": \"en\",\n          \"Next\": \"Pass\"\n        }\n      ],\n      \"Default\": \"TranslateText\"\n    },\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"Lambda Invoke\"\n    },\n    \"Lambda Invoke\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke.waitForTaskToken\",\n      \"Parameters\": {\n        \"FunctionName\": \"arn:aws:lambda:us-east-1:106785241709:function:SentimentAnalysisFunction:$LATEST\",\n        \"Payload\": {\n          \"Review.$\": \"$.body\",\n          \"MyTaskToken.$\": \"$$.Task.Token\"\n        }\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\",\n            \"Lambda.TooManyRequestsException\"\n          ],\n          \"IntervalSeconds\": 1,\n          \"MaxAttempts\": 3,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"ResultPath\": \"$.Result\",\n      \"Next\": \"Parallel\"\n    },\n    \"Parallel\": {\n      \"Type\": \"Parallel\",\n      \"End\": true,\n      \"Branches\": [\n        {\n          \"StartAt\": \"SNS Publish\",\n          \"States\": {\n            \"SNS Publish\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::sns:publish\",\n              \"Parameters\": {\n                \"TopicArn\": \"arn:aws:sns:us-east-1:106785241709:EmailNotificationTopic\",\n                \"Message\": {\n                  \"Review.$\": \"$.body\",\n                  \"Sentiment.$\": \"$.Result.Payload.Sentiment\"\n                }\n              },\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"DynamoDB PutItem\",\n          \"States\": {\n            \"DynamoDB PutItem\": {\n              \"Type\": \"Task\",\n              \"Resource\": \"arn:aws:states:::dynamodb:putItem\",\n              \"Parameters\": {\n                \"TableName\": \"ReviewsTable\",\n                \"Item\": {\n                  \"Time\": {\n                    \"S.$\": \"$$.State.EnteredTime\"\n                  },\n                  \"Review\": {\n                    \"S.$\": \"$.body\"\n                  },\n                  \"Sentiment\": {\n                    \"S.$\": \"$.Result.Payload.Sentiment\"\n                  }\n                }\n              },\n              \"End\": true\n            }\n          }\n        }\n      ]\n    },\n    \"TranslateText\": {\n      \"Type\": \"Task\",\n      \"Parameters\": {\n        \"SourceLanguageCode.$\": \"$.New.Languages[0].LanguageCode\",\n        \"TargetLanguageCode\": \"en\",\n        \"Text.$\": \"$.body\"\n      },\n      \"Resource\": \"arn:aws:states:::aws-sdk:translate:translateText\",\n      \"ResultSelector\": {\n        \"body.$\": \"$.TranslatedText\"\n      },\n      \"Next\": \"Lambda Invoke\"\n    }\n  }\n}\n\n\n\nimage.png",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Step Function project"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/step_functions.html#conclusion",
    "href": "tips/015_Cloud/aws/step_functions.html#conclusion",
    "title": "Step Function project",
    "section": "Conclusion",
    "text": "Conclusion\nIn this Cloud Lab, we created resources to be integrated with the state machine, including an SNS topic, Lambda function, REST API, and a DynamoDB table. Then, we set up an application where a user can submit feedback. The state machine we created would be triggered on each submission, translating the feedback to English when required. Then, the state machine performed a sentiment analysis on the feedback, stored the feedback and the detected sentiment in DynamoDB, and sent a notification to the email address we provided.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Step Function project"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html",
    "href": "tips/015_Cloud/aws/host_web_app.html",
    "title": "Host Web App",
    "section": "",
    "text": "Connect to the instance using the following command. Replace &lt;Your Public DNS(IPv4) Address&gt; with your instance’s public DNS address.\nchmod 400 myGPT.pem\nssh -i \"myGPT.pem\" ubuntu@ec2-54-151-28-127.us-west-1.compute.amazonaws.com\nNow we install miniconda:\nsudo apt-get update\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\nbash ~/miniconda.sh -b -p ~/miniconda\necho \"PATH=$PATH:$HOME/miniconda/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nGet the code from our repository and install the requirements.\ngit clone XYZ\npip install -r requirements.txt",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html#connect-to-the-instance",
    "href": "tips/015_Cloud/aws/host_web_app.html#connect-to-the-instance",
    "title": "Host Web App",
    "section": "",
    "text": "Connect to the instance using the following command. Replace &lt;Your Public DNS(IPv4) Address&gt; with your instance’s public DNS address.\nchmod 400 myGPT.pem\nssh -i \"myGPT.pem\" ubuntu@ec2-54-151-28-127.us-west-1.compute.amazonaws.com\nNow we install miniconda:\nsudo apt-get update\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\nbash ~/miniconda.sh -b -p ~/miniconda\necho \"PATH=$PATH:$HOME/miniconda/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nGet the code from our repository and install the requirements.\ngit clone XYZ\npip install -r requirements.txt",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html#tmux",
    "href": "tips/015_Cloud/aws/host_web_app.html#tmux",
    "title": "Host Web App",
    "section": "Tmux",
    "text": "Tmux\nsudo apt-get install tmux\ntmux new -s StreamSession  # create a new session called StreamSession\ntmux list-sessions  # to discover sessions\ntmux attach -t StreamSession  # to reatach\ntmux kill-session -t StreamSession  # to kill session\nTo leave tmux session, press Ctrl+b and then d.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html#add-dns-name",
    "href": "tips/015_Cloud/aws/host_web_app.html#add-dns-name",
    "title": "Host Web App",
    "section": "Add DNS name",
    "text": "Add DNS name\nOne option is to add subdomain, for example mygpt.nenadbozinovic.com will now go to IP address of the EC2 instance. The port will be 80 by default.\n\n\n\ndns",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html#prep-instance-for-remote-control-via-aws-ssm",
    "href": "tips/015_Cloud/aws/host_web_app.html#prep-instance-for-remote-control-via-aws-ssm",
    "title": "Host Web App",
    "section": "Prep instance for remote control via aws ssm",
    "text": "Prep instance for remote control via aws ssm\nFirst we need to assume user role, which is done by specifying AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. And we need to make sure this user has the necessary permissions to send command, by attaching SendCommand policy to the user.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ssm:SendCommand\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\nWe need to create keygen and add it to GitHub, checkout blog and add it the safe list:\ngit clone git@github.com:nesaboz/blog.git\nsudo timedatectl set-timezone America/Los_Angeles  # set the timezone\ntimedatectl status\nTZ='America/Los_Angeles' date\nfind . -name output.log\noptional, we can add our repo directory (in my case blog) to the safe list:\nsudo git config --system --add safe.directory /home/ubuntu/blog\nssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts  # add github.com to known hosts is obtained from output of: git remote -v\nWe also need to make sure instance can accept commands, this should be default and done by snap, double check it is installed, enabled, and active:\nsnap services amazon-ssm-agent\nWe need to run streamlit so we need to execute these commands:\ngit -C ~/blog pull\nstreamlit run ~/chatbot/myGPT.py --server.enableCORS false --server.enableXsrfProtection false\nNote that --server.enableCORS false --server.enableXsrfProtection false reduces security but allows app to run on servers. One can get away running without the flags too.\nAt this point we should verify myGPT is running fine.\nHowever, to run these commands remotely, it might seem easy, but there are few more things to do: - when we call git command remotely it will execute with different user, so we need to specify user, in this case ubuntu, - we need to kill the previous streamlit process, if it is running, since only one version counts when accessing URL (other apps would run at ports 8502 that are not even exposed), - we need nohup call so the process can run in the background (similar to tmux above), - we need to specify path to streamlit, since it is not in the PATH\n\nwe should also redirect output to a file so we can see what is happening\n\nSo the command looks like this:\nsudo -u ubuntu git -C /home/ubuntu/blog pull\nsudo pkill -f streamlit\nnohup /home/ubuntu/miniconda/bin/streamlit run /home/ubuntu/chatbot/myGPT.py --server.enableCORS false --server.enableXsrfProtection false\nWe should also append everything to a log file, and add a date to the beggining:\ndate &gt; /home/ubuntu/output.log\nsudo -u ubuntu git -C /home/ubuntu/blog pull &gt;&gt; /home/ubuntu/output.log 2&gt;&1\nsudo pkill -f streamlit &gt;&gt; /home/ubuntu/output.log 2&gt;&1\nnohup /home/ubuntu/miniconda/bin/streamlit run /home/ubuntu/chatbot/myGPT.py --server.enableCORS false --server.enableXsrfProtection false &gt;&gt; /home/ubuntu/output.log 2&gt;&1\nThe &gt; rewrites output.log and &gt;&gt; appends while 2&gt;&1 redirects both stderr to stdout:\n&gt;&gt; /home/ubuntu/output.log 2&gt;&1",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_web_app.html#put-it-all-together-in-aws-ssm",
    "href": "tips/015_Cloud/aws/host_web_app.html#put-it-all-together-in-aws-ssm",
    "title": "Host Web App",
    "section": "Put it all together in aws ssm",
    "text": "Put it all together in aws ssm\nSo now that we have commands, let’s execute remote command using AWS Systems Manager (or aws ssm):\naws ssm send-command \\\n    --document-name \"AWS-RunShellScript\" \\\n    --targets \"Key=instanceids,Values=i-08b8b6691ed2e1b6d\" \\\n    --parameters commands=\"date &gt; /home/ubuntu/output.log && sudo -u ubuntu git -C /home/ubuntu/blog pull &gt;&gt; /home/ubuntu/output.log 2&gt;&1 && sudo pkill -f streamlit &gt;&gt; /home/ubuntu/output.log 2&gt;&1 && nohup /home/ubuntu/miniconda/bin/streamlit run /home/ubuntu/chatbot/myGPT.py --server.enableCORS false --server.enableXsrfProtection false &gt;&gt; /home/ubuntu/output.log 2&gt;&1\" \\\n    --region us-west-1\nWe can now go to the remote machine and check the output.log:\n(base) ubuntu@ip-172-31-18-61:~$ cat output.log \nThu Feb  8 17:56:43 PST 2024\nAlready up to date.\n  Stopping...\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n\n\n  You can now view your Streamlit app in your browser.\n\n  Network URL: http://172.31.18.61:8501\n  External URL: http://54.151.28.127:8501\nwe can see what is running on the instance:\nps aux | grep -E \"streamlit|PID\"\nkill &lt;PID&gt;  # if you need to\nthe output looks like this:\n(base) ubuntu@ip-172-31-18-61:~$ ps aux | grep -E \"streamlit|PID\"\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot       33774  0.1 13.9 623640 135188 ?       Sl   00:55   0:01 /home/ubuntu/miniconda/bin/python /home/ubuntu/miniconda/bin/streamlit run /home/ubuntu/blog/nbs/projects/myGPT/myGPT.py --server.enableCORS false --server.enableXsrfProtection false\nubuntu     33856  0.0  0.2   7004  2304 pts/0    S+   01:15   0:00 grep --color=auto -E streamlit|PID\nthe process with user root is the one that we started remotely, it is taking 0.1% of CPU and 13.9% of the memory.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Host Web App"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/application_integration.html",
    "href": "tips/015_Cloud/aws/application_integration.html",
    "title": "Application Integration",
    "section": "",
    "text": "Credits: Educative.io\nAmazon Simple Notification Service (SNS) helps us send notifications to several types of endpoints/destinations, such as email addresses, text messages, and so on. SNS topic is the channel from where the communication occurs. SNS subscriptions are the endpoint for a published message on an SNS topic; subscribers can be other services of AWS, email addresses, mobile phone numbers, and so on. SNS filters enable topics to publish messages to subscribers based on certain predefined criteria.\nAmazon Simple Queue Service (SQS) is a message queuing service that can store messages for a period of time—up to 14 days, during which a consumer can try to receive the message from the queue. In case of any failure on the consumer’s side, the message won’t be lost and the consumer can retrieve the message later on. Furthermore, SQS supports dead-letter queues; if the consumer fails to receive a message a specified number of times, the message is transferred to a dead-letter queue for debugging purposes.\nOne of the major differences it has with an SNS topic is that the consumer is required to receive (pull) messages from an SQS queue, while an SNS topic publishes (pushes) messages to all its subscribers.\nWe create standard SNS topic: arn:aws:sns:us-east-1:437550197980:TopicForQueues\nand we subscribe (no encryption) 3 queues to the topic: https://sqs.us-east-1.amazonaws.com/437550197980/JohnQueue\nwe then add “Subscription filter policy” (so we can provide message with some attributes (in this case that attribute is “Option”)):\nLet’s create a new topic TopicForEmail: arn:aws:sns:us-east-1:437550197980:TopicForEmail this time we create subscription with email protocol and provide email address.\nWe now set up CloudWatch alarms for all 3 queues, so that if there are more then 5 messages in each of the queues, we send an email with message,“Queue X has reached enough messages”, and we send this alarm to queue TopicForEmail we created in the previous step. An alarm metric we’ll use is: SQS &gt; Queue Metrics &gt; ApproximateNumberOfMessagesVisible.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Application Integration"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/application_integration.html#set-api",
    "href": "tips/015_Cloud/aws/application_integration.html#set-api",
    "title": "Application Integration",
    "section": "Set API",
    "text": "Set API\nLets create a regional REST API called clab-API.\n\nPOST API\nWe’ll need a LabRole: arn:aws:iam::546114071089:role/LabRole\nWe first define POST method Publish that will publish some message to SNS topic TopicForQueues. We will use AWS API Gateway service, using AWS SNS service and IAM Role “LabRole” (ARN: arn:aws:iam::437550197980:role/LabRole). To configure the API so “method request” needs to “validate query string and headers” and that URL query string parameter “message” is required. “Integration request” needs 5 URL query string parameters: - “Message”: ‘Option Chosen’ - “MessageAttributes.entry.1.Name”: ‘Option’ - “MessageAttributes.entry.1.Value.DataType”: ‘String’ - “MessageAttributes.entry.1.Value.StringValue”: method.request.querystring.message - “TopicArn”: ‘arn:aws:sns:us-east-1:437550197980:TopicForQueues’\nWhen we send a “message=Alexa”, we in fact create this endpoint:\nEndpoint request URI: https://sns.us-east-1.amazonaws.com/?Action=Publish&Message=Option+Chosen&TopicArn=arn:aws:sns:us-east-1:437550197980:TopicForQueues&MessageAttributes.entry.1.Name=Option&MessageAttributes.entry.1.Value.DataType=String&MessageAttributes.entry.1.Value.StringValue=Alexa\nWe finally deploy API (id: l27ksqvoud)\n\n\nGET API\nWe now create a GET method for the same REST API, named “GetQueueAttributes”. We’ll set up 3 stages, one for each queue, and use Integration request set up as: - “QueueUrl”: ‘https://sqs.us-east-1.amazonaws.com/437550197980/AlexaQueue’ (this will change for each stage) - “AttributeNames”: ‘ApproximateNumberOfMessages’\nand we modify integration response as:\n#set($inputRoot = $input.path('$'))\n$inputRoot.GetQueueAttributesResponse.GetQueueAttributesResult.Attributes[0].Value\nso we get only a simple value.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Application Integration"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_basics.html",
    "href": "tips/015_Cloud/aws/aws_basics.html",
    "title": "AWS basics",
    "section": "",
    "text": "Here I’ll show some basic functionality when working with AWS SageMaker, like: - Installing aws cli (configuring aws credentials, setting up conda environment, cloning GitHub repo) - Run SageMaker in VS-Code web UI - S3 access using s3fs and boto3 - How to restore deleted files on S3 bucket",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS basics"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_basics.html#aws-cli",
    "href": "tips/015_Cloud/aws/aws_basics.html#aws-cli",
    "title": "AWS basics",
    "section": "1. AWS CLI",
    "text": "1. AWS CLI\nFirst we install awscli which is aws API (follow Link to install).\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nwhich aws\naws --version\nCommands that expect aws credential will first look for KEY_ID and SECRET_ACCESS env variables, then local/global .credentials set up by\naws configure  # configures the credentials\naws s3 ls      # list all S3 buckets\naws sts get-caller-identity  # gets identity of a user\n\nSet up new conda environment\nSageMaker comes with many built-in Images that have many preloaded packages. Each image can support different Jupyter kernels (or equivalent conda environments).\nFirst access Image terminal directly (navigate to the Launcher (click on “Amazon SageMaker Studio in the top left corner) and”Open Image terminal”)\nconda env list\nwill print opt/conda only.\nI had to install pip (for some reason it is not preinstalled):\napt-get update\napt-get install python-pip\nLet’s create a new environment:\nconda create -n  python=3.9\nconda activate new_env\nconda install jupyter\nLet’s also add this environment as a Jupyter kernel:\njupyter kernelspec list\nipython kernel install --name new_env --user\nNow our custom environment is showing up.\nLet’s install required packages in the new_env env:\npip install streamlit pandas polars\npip install torch torchvision torchaudio\npip install jupyter methodtools pytorch-lightning scikit-learn colorama libtmux onnxruntime openpyxl xlsxwriter matplotlib pulp\nmost of these are already preinstalled.\nThat’s it. Our kernel is now ready to be used.\nThere are more ways this can be done and I dabbled in Lifecycle Configuration but haven’t had success.\n\n\nClone GitHub repository\nBest way to do this is via terminal (SageMaker has GitHub tab option but couldn’t make it run). We’ll show here one of the easy ways to do this is via Personal Access Token (there is ssh-key option as well). First, In Github, navigate to Personal Access Tokens, then in SageMaker Studio terminal clone the repo:\ngit clone &lt;repo-URL&gt;\nwhen asked for password enter the token that you just created!\nTo set up global git credentials, edit and run following lines:\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\nthat way all the git changes will have your credentials.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS basics"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_basics.html#run-sagemaker-in-vs-code-web-ui",
    "href": "tips/015_Cloud/aws/aws_basics.html#run-sagemaker-in-vs-code-web-ui",
    "title": "AWS basics",
    "section": "2. Run SageMaker in VS-Code web UI",
    "text": "2. Run SageMaker in VS-Code web UI\nSageMaker Studio is the AWSs’ UI. If you prefer the VSCode look then you need to install somewhat awkwardly named “Code Server” (see installation instructions). You will now be able to run VS Code in a browser.\n\n\n\nAlt Text",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS basics"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_basics.html#s3-access-using-s3fs-and-boto3",
    "href": "tips/015_Cloud/aws/aws_basics.html#s3-access-using-s3fs-and-boto3",
    "title": "AWS basics",
    "section": "3. S3 access using s3fs and boto3",
    "text": "3. S3 access using s3fs and boto3\nFor s3 access one must set up credentials.\nAccessing S3 files is not as straightforward as if having them locally since interaction has to go via AWS APIs. boto3 and s3fs are two libraries that have their own APIs. I find s3fs to be better since “S3Fs is a Pythonic file interface to S3. It builds on top of botocore”. Pandas is an exception and pd.read_csv just works. There is s3fs-fuse library (and apparently one with even higher performance called goofys, neither in pip) that kind of offer mounting s3 option, however, I ran into issues when trying to install them. Following are some examples:\n\nimport boto3\nimport s3fs\nimport pandas as pd\nimport os\n\nbucket_name = 'X'\nfile_key = 'Y'\n\n\nRead into DataFrame\n\ndf = pd.read_csv(f's3://{bucket_name}/{file_key}')\n\n\n\ns3fs\n\ns3 = s3fs.S3FileSystem(anon=False)  # Set anon to True for public buckets\n\n\n\nls\n\ncontents = s3.ls(bucket_name)\nprint(contents)\n\n\n\nglob\n\ns3.glob(f's3://{bucket_name}/**/a*.csv')\n\n\n\ndownload\n\n# Specify local directory for downloaded file\nlocal_directory = '/tmp/'\nlocal_file_path = os.path.join(local_directory, 'Z')\n\n# Ensure the local directory exists\nos.makedirs(local_directory, exist_ok=True)\n\n# Download CSV file\ns3.download(f's3://{bucket_name}/{file_key}', local_file_path)\n\n# Load CSV into Pandas DataFrame\ndf = pd.read_csv(local_file_path)\n\n# Clean up downloaded file\n# os.remove(local_file_path)\n\n\n\nboto3\n\ns3_client = boto3.client('s3')\n\n\n\ndownload\n\n# Specify local directory for downloaded file\nlocal_directory = '/tmp/'\nlocal_file_path = os.path.join(local_directory, 'Z')\n\n# Ensure the local directory exists\nos.makedirs(local_directory, exist_ok=True)\n\n# Download CSV file\ns3_client.download_file(bucket_name, file_key, local_file_path)\n\n# Load CSV into Pandas DataFrame\ndf = pd.read_csv(local_file_path)\n\n# Clean up downloaded file\nos.remove(local_file_path)\n\n\n\nls\n\ncontents = s3_client.list_objects(Bucket=bucket_name, Prefix='X')['Contents']  \nfor f in contents:  \n    print(f['Key'])",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS basics"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_basics.html#restore-deleted-files",
    "href": "tips/015_Cloud/aws/aws_basics.html#restore-deleted-files",
    "title": "AWS basics",
    "section": "4. Restore deleted files",
    "text": "4. Restore deleted files\nIf Version Control is enabled in S3 then deleted files will just have a tag Deleted Marker or similar. The idea is to remove these tags (link). To do this for many files at once first create a script that will list all the files to be restored:\necho '#!/bin/bash' &gt; undeleteScript.sh\n\naws --output text s3api list-object-versions --bucket &lt;bucket name&gt; --prefix \"Flagging Tool/\" | grep -E \"^DELETEMARKERS\" | awk '{FS = \"[\\t]+\"; print \"aws s3api delete-object --bucket &lt;bucket name&gt;  --key \\42\"$3\"\\42 --version-id \"$5\";\"}' &gt;&gt; undeleteScript.sh\nThen just run the\nchmod +x undeleteScript.sh\n./undeleteScript.sh\nfinally remove it:\nrm -f undeleteScript.sh",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS basics"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/beanstalk.html",
    "href": "tips/015_Cloud/aws/beanstalk.html",
    "title": "AWS Beanstalk",
    "section": "",
    "text": "Credits Educative.io course.\nElastic Beanstalk is designed to host web apps by creating the whole stack various resources for us such as, an EC2 instance, an EC2 security group, an S3 bucket, CloudWatch alarms, and CloudFormation stack. Elastic Beanstalk also provides us with a domain name for our application. It is also auto-scalable. It works well with Flask, Django, Node.js, PHP, Python, Ruby, Tomcat, and .NET.\nTake home messages: - Create IAM role that allows BeanStack to access EC2, policy: AWSElasticBeanstalkWebTier - Elastic Beanstalk looks for zip file, that has requirements.txt, and it looks for our code in the application.py file (in case of Flask app)."
  },
  {
    "objectID": "tips/015_Cloud/paperspace.html",
    "href": "tips/015_Cloud/paperspace.html",
    "title": "Paperspace",
    "section": "",
    "text": "Paperspace offers very affordable remote machines with GPU, and allows for 150Gb permanent storage (that is mounted as /notebook folder). I find it easier to set up then any other platform. It has many pre-installed deployments like PyTorch, FastAI, Stable Diffusion, and more. It does require monthly subscription that is at the time of writting $8 per month.\n\nSSH commands\nTo make your life easier before creating any remote instances, copy ssh public key of computers you will be using to access paperspace (like your laptop) to Paperspace SSH keys section (under your account). See this to set up SSH access.\n\n\nGradient ML platform\nFor most of my needs I use free machines on their Gradient ML platform:\n\n\n\nimage.png\n\n\nDon’t forget to set auto-shutdown time depending on you need, 6 hours is the max:\n\n\n\nimage.png\n\n\nOnce the machine is running, the VSCode icon allows to get the URL of the kernel:\n \n  Copy URL and on the local machine in VSCode navigate to: “Select Kernel”-&gt;“Select another Kernel”-&gt;“Existing Jupyter Kernel”-&gt;“Enter the URL of the running Jupyter server”-&gt;“Python3 (ipykernel)”:\n \nThat’s pretty much it; the kernel is now set in the upper right corner of the VSCode:\n\n\n\nimage.png\n\n\nLink describes this too.\nThis is great for majority of my needs, for longer durations one can always restart the machine.\nNote again that only files in the /notebook folder are persistent.\n\n\nCore machines\nCore machines are virtual machines that one can SSH into. More on setting SSH connection in VSCode can be found here.\n \nOne must install Python as a VSCode extension:\n\n\n\nimage.png\n\n\nFor setting up the environment see here.\n\n\nShared drive\nShared drive is an good way to share drive across multiple machines HOWEVER, BEWARE(!), shared drive speed is 10-50X SLOWER then the local hard drive. So while convenient since no extra copies of data exist, loading data in batches from shared drive will be a bottleneck to your training (for large models and smaller batch sizes). So ideally data should be copied locally before any training.\nMachines do have to be on the same private network, which is created from a network tab on Paperspace Core.\nOnce private network is created instances MUST be assigned to that private noetwork, this is not a default.\nNote there might be a delay for all of this to set up (likely &lt;10 minutes) to be able to ssh into a machine.\nOnce you ssh, you will need several pieces of information, follow this, these are the commands you will be using:\nI’ve used SHARE_FOLDERNAME as /home/paperspace/share:\nsudo vim /etc/fstab  # use any editor of your choce btw, add the following line at the end: \n# //YOUR_SHARED_DRIVE_IP/YOURSHARE   SHARE_FOLDERNAME  cifs  user=USERNAME,password=PASSWORD,rw,uid=1000,gid=1000,users 0 0\nmkdir SHARE_FOLDERNAME  # if doesn't exist\nsudo chown paperspace:paperspace SHARE_FOLDERNAME  # this is needed ONLY if SHARE_FOLDERNAME is outside your home directory \nsudo apt-get update\nsudo apt install cifs-utils\nsudo mount SHARE_FOLDERNAME  # MUST use sudo here!\ndf\nAlso you will have to re-mount the shared drive after the instance is restarted, that takes a second though.\nTo unmount a drive use umount command:\nsudo umount SHARE_FOLDERNAME\n\n\nDocker container\nA docker contain is a great way to set up an environment. Once the docker container is built and running, one can attach to it from VSCode and use it as a remote machine. To do this, install “Remote Development” extension pack, then one can attach the container in VSCode by going to a command pallette (Cmd+Shift+P) and typing “Attach to running container”. If running into a user issue, you need to add user to docker group:\nsudo usermod -aG docker $USER\nand (maybe, not always) restart the machine.\nTo see running containers’ info from paperspace machine use:\ndocker ps\nTo stop all the docker containers:\ndocker container stop ID_or_NAME\ndocker container stop $(docker container ls -aq)\n\n\nGPU stats\nUse nvidia-smi to see GPU stats.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Paperspace"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/loss.html",
    "href": "tips/040_PyTorch/loss.html",
    "title": "Loss",
    "section": "",
    "text": "CrossEntropyloss\nIf logit shape is [N, C, d1, d2] (where N is the number of images and C is the number of classes to predict), then target (i.e. label) shape must be [N, d1, d2].\nThen the loss will be calculated as: nn.CrossEntropyLoss(weight)(logit, target)\nweight is a tensor for unbalanced datasets. Must be tensor.float.\nWhen using masking, one should use masked:\n\ndef masked_cross_entropy_loss_fn(y_pred, y_true):\n    out_of_bounds_mask = (y_true == out_of_bounds_value) # find out the out-of-bounds\n    return nn.CrossEntropyLoss(weight=weights)(\n        y_pred.masked_fill(out_of_bounds.unsqueeze(axis=1), 0), \n        y_true.masked_fill(out_of_bounds, 0)\n    )",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Loss"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/tensor.html",
    "href": "tips/040_PyTorch/tensor.html",
    "title": "Tensor",
    "section": "",
    "text": "import torch\nfrom torchvision.transforms import Normalize, ToPILImage, Resize\nfrom shared.utils import plot_pil_images\nx = torch.randn([1,256,256])\nx = x.squeeze()\nx.shape\n\ntorch.Size([256, 256])\nx = torch.randn([3,256,256])\nx.shape\n\ntorch.Size([3, 256, 256])\ntorch.unsqueeze(x, 0).shape\n\ntorch.Size([1, 3, 256, 256])\nx.flatten(1,2).shape\n\ntorch.Size([3, 65536])\nx.flatten(1,2).mean(axis=1).shape\n\ntorch.Size([3])\nx = torch.randn([1,256,256])\nx.transpose(0,1).shape\n\ntorch.Size([256, 1, 256])\ntorch.manual_seed(12)\nx = torch.randn([1,3])\nx.max(), x.argmax()\n\n(tensor(-0.0546), tensor(2))\nOne can index a matrix using notation similar to numpy:\nor using the gather function:",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Tensor"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/tensor.html#cat-vs-stack",
    "href": "tips/040_PyTorch/tensor.html#cat-vs-stack",
    "title": "Tensor",
    "section": "cat vs stack",
    "text": "cat vs stack\n\nassert (torch.Tensor([1,2,3]) == torch.tensor([1,2,3])).all()\n\n\na = torch.Tensor([1])\nb = torch.Tensor([2])\ntorch.cat([a,b], dim=0)\n\ntensor([1., 2.])\n\n\n\ntorch.stack([a,b], dim=0)\n\ntensor([[1.],\n        [2.]])\n\n\n\ntorch.stack([a,b], dim=1)\n\ntensor([[1., 2.]])",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Tensor"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html",
    "href": "tips/040_PyTorch/convolutions.html",
    "title": "Convolutions",
    "section": "",
    "text": "Code\nfrom torch.nn.functional import conv2d\nimport torch\nimport numpy as np\nNote: torch.nn.Conv2d is a Module that initializes all parameters i.e. kernel will be learned. Only functional form can take kernel as input.",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Convolutions"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#conv2d",
    "href": "tips/040_PyTorch/convolutions.html#conv2d",
    "title": "Convolutions",
    "section": "Conv2d",
    "text": "Conv2d\nWe explore functional form of conv2d. Let’s make some inputs and kernels:\n\ntorch.manual_seed(14)\nx = torch.randn(16, 3, 16, 16)\nkernel = torch.randn(10, 3, 3, 3)\n\nbasic Conv2 does what we expect:\n\ny = conv2d(x, kernel)\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 14, 14])\n\n\nFormula for output size for Convolutions is: \\([(W−K+2P)/S]+1)\\), where [] is the np.floor:\n\ndef get_output_size(w,k,p,s):\n    return int(((w-k+2*p)/s)+1)\n\nIn above example:\n\nget_output_size(16, 3, 0, 1)\n\n14\n\n\nif we want the same size, we could pass padding='same':\n\ny = conv2d(x, kernel, padding='same')\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 16, 16])\n\n\nThis works ok for stride=1, but for stride &gt;1 ‘same’ doesn’t make sense anymore in PyTorch (raises RuntimeError: padding='same' is not supported for strided convolutions). In TensorFlow however ‘same’ assumes one needs \\(width/stride\\) size, which in our case should be 8, instead we get expected 7:\n\ny = conv2d(x, kernel, stride=2)\nprint(x.shape)\nprint(y.shape)\n\ntorch.Size([16, 3, 16, 16])\ntorch.Size([16, 10, 7, 7])\n\n\nformula checks out:\n\nget_output_size(16, 3, 0, 2)\n\n7\n\n\nSo what’s the padding that we need? We need to do some math:\no = output size w = width p = padding s = stride if we solve the equation for p: o = int((w-k+2p)/s) + 1 o - 1 = int((w-k+2p)/s) and here we have a range: o - 1 = int((w-k+2p_min)/s) o - 1 = int((w-k+2p_max)/s) solving this: (o - 1) * s = min = w-k+2p_min &lt;= w-k+2p &lt;= w-k+2p_max = max &lt; o  s Note that inequality is not symmetric, the top bound is exclusive. Then: p_min = ((o-1)s - w + k) / 2 p_max = (os - w + k) / 2 which can both be decimal. at this point we can just get the ceil of p_min and be happy: \\(p = np.ceil(((o-1)*s - w + k) / 2)\\)\n\ndef get_lowest_padding(w,k,s):\n    o = np.ceil(w/s)\n    p = (np.ceil((k - (w - (o-1)*s)) / 2)).astype(int)\n    return p\n\n\nget_lowest_padding(90,15,10)\n\n3\n\n\nIt is quite annoying that padding depends on the width of the input image (this is not always the case though), here is a re-write of the formula where width is defaulted:\n\ndef get_padding(kernel_size, stride, width=256):\n    \"\"\"\n    Padding that ensures the output_size of ceil(input_size/stride). Assuming square images:\n    \"\"\"\n    output_width = np.ceil(width / stride)\n    padding = int(np.ceil(((output_width-1) * stride - width + kernel_size) / 2))\n    return padding",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Convolutions"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#padding-visualizations",
    "href": "tips/040_PyTorch/convolutions.html#padding-visualizations",
    "title": "Convolutions",
    "section": "Padding visualizations",
    "text": "Padding visualizations\nLet’s plot padding for many image widths:\n\nimport nbimporter\nfrom Python.functools import partial, plot_function\n\n\npartial_get_padding = partial(get_lowest_padding, k=5, s=2)\nx = np.arange(200, 300, 1)\nplot_function(partial_get_padding, x)\n\n\n\n\n\n\n\n\n\nnp.unique(partial_get_padding(np.arange(200,300,1)))\n\narray([2])\n\n\n\npartial_get_padding = partial(get_lowest_padding, k=3, s=1)\nx = np.arange(200, 300, 1)\nplot_function(partial_get_padding, x)\n\n\n\n\n\n\n\n\nSo this is funny, seems to be independent of the width. Let’s look into math again:\no = can be even or odd p = (np.ceil((k - (1 or 2)) / 2)) # generaly speaking p = (np.ceil((k - (1 to s) / 2)) p = (np.ceil((5 - (1 or 2)) / 2)) p = (np.ceil((4 or 3)/ 2)) p = 2\nso that’s the reason.\nWe can also look into limits of p:\no = np.ceil(w/s) p = (np.ceil(((o-1)*s - w + k) / 2)) note that p is maximized when (o-1)*s - w is highest, i.e. w - (o-1)*s is lowest, which is has lowest value of 1 (by definition of o it can’t be 0): np.ceil((k-s)/2) &lt;= p &lt;= np.ceil((k-1)/2)\n\nprint(get_lowest_padding(90,16,10))  # np.ceil((k-s)/2) = (ceil((16-10)/2) = 3\nprint(get_lowest_padding(91,16,10))   # np.ceil((k-1)/2) = ceil(15/2) = 8\n\n3\n8\n\n\nFinally, for a special, but common case, w = s*o:\ngeneral equations: o = np.ceil(w/s) p = (np.ceil(((o-1)*s - w + k) / 2))\nthen: p = (np.ceil((k-s) / 2))\nso padding doesn’t depend on the width in this case. We will stick with this formula since it makes it simple not to worry about the width, so long our images are divisible by s.",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Convolutions"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/convolutions.html#conv2dtranspose",
    "href": "tips/040_PyTorch/convolutions.html#conv2dtranspose",
    "title": "Convolutions",
    "section": "Conv2dTranspose",
    "text": "Conv2dTranspose\nnn.ConvTranspose2d is used to upsample the input (for example in UNet). These have additional output_padding that fills only one-side (useful for those ‘same’ paddings). It is a non-symmetric padding of the output image that enables us to get an even size image.",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Convolutions"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/backprop.html",
    "href": "tips/040_PyTorch/backprop.html",
    "title": "Backpropagation",
    "section": "",
    "text": "According to many smart people backpropagation is one of the main reasons why is AI so successful today. What’s even better from backpropagation is the automated backpropagation, or autograd. This is just an example showing how it works in pytorch:\n\nimport torch\n\n\na = torch.randn(2, 3, 6, 6)\na.requires_grad_()\nloss = (a**2).sum()\nloss.backward()\nassert (a.grad == 2 * a).all()",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Backpropagation"
    ]
  },
  {
    "objectID": "tips/010_Python/numpy.html",
    "href": "tips/010_Python/numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "Excellent summary.\nimport numpy as np",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Numpy"
    ]
  },
  {
    "objectID": "tips/010_Python/numpy.html#broadcasting",
    "href": "tips/010_Python/numpy.html#broadcasting",
    "title": "Numpy",
    "section": "Broadcasting",
    "text": "Broadcasting\n\na = np.zeros([3,4])\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\nrows = np.array([0,2])\nvec = np.array([1,2,3]).reshape(1,-1).T\n\n\na[:,rows] += vec\na\n\narray([[1., 0., 1., 0.],\n       [2., 0., 2., 0.],\n       [3., 0., 3., 0.]])",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Numpy"
    ]
  },
  {
    "objectID": "tips/010_Python/datetime.html",
    "href": "tips/010_Python/datetime.html",
    "title": "Datetime",
    "section": "",
    "text": "import datetime\n\n\ndatetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n\n'20230119_112149'\n\n\n\ndatetime.datetime.now().strftime('%Y_%m_%d')\n\n'2023_01_19'\n\n\n\ndef get_elapsed_days(b):\n    a = datetime.datetime.now()\n    return (a-b).days\n\nb = datetime.datetime(2018, 1, 10, 14, 37)\nprint(get_elapsed_days(b))\n\n1845",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Datetime"
    ]
  },
  {
    "objectID": "tips/010_Python/setoperations.html",
    "href": "tips/010_Python/setoperations.html",
    "title": "Set operations",
    "section": "",
    "text": "a = set([1,2,3])\nb = set([1,2,4])\n\n\na | b  # union\n\n{1, 2, 3, 4}\n\n\n\na & b  # intersections\n\n{1, 2}\n\n\n\na ^ b  # symmetric intersection\n\n{3, 4}\n\n\n\na - b\n\n{3}\n\n\n\nb - a\n\n{4}",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Set operations"
    ]
  },
  {
    "objectID": "tips/010_Python/pathlib.html",
    "href": "tips/010_Python/pathlib.html",
    "title": "Pathlib",
    "section": "",
    "text": "from pathlib import Path\nassert Path(&lt;some_file&gt;).is_file()\nTo get the Path of the current file, or a parent:\nfrom pathlib import Path\na = Path('/one/two/three.jpg')\na.parent, a.name, a.stem\n\n(PosixPath('/one/two'), 'three.jpg', 'three')",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Pathlib"
    ]
  },
  {
    "objectID": "tips/010_Python/pathlib.html#platform",
    "href": "tips/010_Python/pathlib.html#platform",
    "title": "Pathlib",
    "section": "Platform",
    "text": "Platform\n\nimport platform\nif 'macOS' in platform.platform():\n    print('This is Mac')\n\nThis is Mac\n\n\n\n&lt;some_path&gt;.mkdir(parents=True, exist_ok=True)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Pathlib"
    ]
  },
  {
    "objectID": "tips/010_Python/parser_example.html",
    "href": "tips/010_Python/parser_example.html",
    "title": "Parser example",
    "section": "",
    "text": "Usage in bash: python parser_example.py -a1 \"hello\" -a2 42\n\nimport argparse\n\ndef main():\n    # Create the parser\n    parser = argparse.ArgumentParser(description=\"Example script.\")\n    \n    # Add arguments with both short and long options\n    parser.add_argument('-a1', '--arg1', type=str, required=True, help=\"First argument (string).\")\n    parser.add_argument('-a2', '--arg2', type=int, required=True, help=\"Second argument (integer).\")\n    \n    # Parse the arguments\n    args = parser.parse_args()\n    \n    # Use the arguments\n    print(f\"arg1: {args.arg1}\")\n    print(f\"arg2: {args.arg2}\")\n\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Parser example"
    ]
  },
  {
    "objectID": "tips/010_Python/general.html",
    "href": "tips/010_Python/general.html",
    "title": "Python General",
    "section": "",
    "text": "To see what modules have been imported use:\nimport sys\nmodulenames = set(sys.modules) & set(globals())\nallmodules = [sys.modules[name] for name in modulenames]\nallmodules\nTo see if a particular module is imported:\n[i for i, x in enumerate(allmodules) if \"'fastai'\" in str(x)] != []\nTo see memory stats (CPU/GPU/top variables):\nimport psutil\nimport torch\n\n\ndef print_cpu_memory(verbose=True):\n\n    # Get the current memory usage\n    memory_info = psutil.virtual_memory()\n\n    # Extract the memory information\n    total_memory = memory_info.total\n    available_memory = memory_info.available\n    used_memory = memory_info.used\n    percent_memory = memory_info.percent\n\n    # Convert bytes to megabytes\n    total_memory_mb = total_memory / 1024**2\n    available_memory_mb = available_memory / 1024**2\n    used_memory_mb = used_memory / 1024**2\n\n    if verbose:\n        # Print the memory information\n        print(f\"Total CPU memory: {total_memory_mb:.2f} MB\")\n        print(f\"Available CPU memory: {available_memory_mb:.2f} MB\")\n        print(f\"Used CPU memory: {used_memory_mb:.2f} MB\")\n        print(f\"Percentage of used CPU memory: {percent_memory}%\")\n    else:\n        print(f\"Percentage of used CPU memory: {percent_memory}%\")\n\n\ndef print_gpu_memory():\n    # Check if CUDA is available\n    if torch.cuda.is_available():\n        # Get the default CUDA device\n        device = torch.cuda.current_device()\n\n        # Get the total memory and currently allocated memory on the device\n        total_memory = torch.cuda.get_device_properties(device).total_memory\n        allocated_memory = torch.cuda.memory_allocated(device)\n\n        # Convert bytes to megabytes\n        total_memory_mb = total_memory / 1024**2\n        allocated_memory_mb = allocated_memory / 1024**2\n\n        # Print the memory information\n        print(f\"Total GPU memory: {total_memory_mb:.2f} MB\")\n        print(f\"Allocated GPU memory: {allocated_memory_mb:.2f} MB\")\n    else:\n        print(\"CUDA is not available\")\n\n\ndef print_top_memory_variables(local_vars, var_number_to_print=5):\n    \"\"\"Prints top variables in terms of memory. \n    Usage: `print_top_memory_variables(locals().copy())` can't call locals() in the function itself.\n\n    Args:\n        local_vars (dict): pass `locals().copy()` \n        var_number_to_print(int): \n    \"\"\"\n\n    # Get the local variables\n    memory = {}\n\n    # Iterate over the local variables and print their sizes\n    for var_name, var_value in local_vars.items():\n        var_size = sys.getsizeof(var_value)\n        memory[var_name] = var_size\n        \n    memory_sorted = sorted(memory.items(), key=lambda x: x[1], reverse=True)[:var_number_to_print]\n    \n    for (var_name, var_size) in memory_sorted:\n        print(f\"Variable: {var_name}, Size: {var_size} bytes\")",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Python General"
    ]
  },
  {
    "objectID": "tips/010_Python/general.html#profiling",
    "href": "tips/010_Python/general.html#profiling",
    "title": "Python General",
    "section": "Profiling",
    "text": "Profiling\nProfiling code is useful in many ways. Python has a built-in module called cProfile. One can also visualize the profile file using snakeviz. Here is an example:\nimport cProfile\nimport pstats\nimport time\n\ndef sum(a,b):\n    return a+b\n\ndef print_many():\n    for i in range(100000):\n        print(i)\n                \ndef main():\n    print_many()\n    print(sum(1,2))\n    just_wait()\n    \ndef just_wait():\n    time.sleep(2)\n    \nif __name__ == \"__main__\":\n    with cProfile.Profile() as pr:\n        main()\n        \n    results = pstats.Stats(pr)\n    results.sort_stats(pstats.SortKey.TIME)\n    results.print_stats()\n    results.dump_stats(\"speed_test.prof\")  # run `snakeviz speed_test.prof` to see the results in a browser (pip install snakeviz if needed)\nthis gives following output:\n   100008 function calls in 2.240 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    2.005    2.005    2.005    2.005 {built-in method time.sleep}\n   100001    0.220    0.000    0.220    0.000 {built-in method builtins.print}\n        1    0.015    0.015    0.235    0.235 /Users/nenadbozinovic/Documents/chatbot/speed_test.py:10(print_many)\n        1    0.000    0.000    0.000    0.000 /Users/nenadbozinovic/miniconda3/envs/blog/lib/python3.11/cProfile.py:118(__exit__)\n        1    0.000    0.000    2.240    2.240 /Users/nenadbozinovic/Documents/chatbot/speed_test.py:15(main)\n        1    0.000    0.000    2.005    2.005 /Users/nenadbozinovic/Documents/chatbot/speed_test.py:21(just_wait)\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n        1    0.000    0.000    0.000    0.000 /Users/nenadbozinovic/Documents/chatbot/speed_test.py:6(sum)\n(there is some extension too that I haven’t tried yet).",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Python General"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html",
    "href": "tips/010_Python/fileio.html",
    "title": "FileIO",
    "section": "",
    "text": "To dump data to json file:\noutput_path = &lt;some_dir&gt; / \"some_file.json\"\nparams = {'a': 1, 'b': 2}\nwith output_path.open('w') as f:\n    json.dump(params, f, indent=4)  # `indent` writes each new input on a new line\nto load from json file:\nwith input_path.open() as f:\n    result = json.load(f)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html#json",
    "href": "tips/010_Python/fileio.html#json",
    "title": "FileIO",
    "section": "",
    "text": "To dump data to json file:\noutput_path = &lt;some_dir&gt; / \"some_file.json\"\nparams = {'a': 1, 'b': 2}\nwith output_path.open('w') as f:\n    json.dump(params, f, indent=4)  # `indent` writes each new input on a new line\nto load from json file:\nwith input_path.open() as f:\n    result = json.load(f)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html#shutil",
    "href": "tips/010_Python/fileio.html#shutil",
    "title": "FileIO",
    "section": "shutil",
    "text": "shutil\nshutil.copy(src_path, dst_path)\nfrom shutil import copyfile, rmtree, move",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html#csv",
    "href": "tips/010_Python/fileio.html#csv",
    "title": "FileIO",
    "section": "CSV",
    "text": "CSV\nimport csv\ndef read_csv_file_into_dict(filename):\n    with open(filename, newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=',')\n        x = {row[0]: row[1] for row in reader}\n    return x",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html#readline-and-readlines",
    "href": "tips/010_Python/fileio.html#readline-and-readlines",
    "title": "FileIO",
    "section": "Readline and Readlines",
    "text": "Readline and Readlines\nwith open(f, \"r\") as f:\n    a = f.readlines()",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/fileio.html#find-and-delete-filesfolders",
    "href": "tips/010_Python/fileio.html#find-and-delete-filesfolders",
    "title": "FileIO",
    "section": "Find and delete files/folders",
    "text": "Find and delete files/folders\nfrom pathlib import Path\na = list(Path('path_to_the_folder').glob('**/some_file_name'))\nfor x in a:\n    x.unlink()\nTo delete a folder use shutil.rmtree:\nimport shutil\nshutil.rmtree(Path(\"a_directory\"))",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "FileIO"
    ]
  },
  {
    "objectID": "tips/010_Python/pytest.html",
    "href": "tips/010_Python/pytest.html",
    "title": "PyTest",
    "section": "",
    "text": "!pip install pytest\n\nYou will need pytest.ini in the root:\n[pytest]\npythonpath = ''\npythonpath adds a path to sys.path relative to the root so the imports in test files work.\nTo discover and run all tests run in terminal:\npytest\nTo run only specific tests:\npytest filename.py\ndocs",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PyTest"
    ]
  },
  {
    "objectID": "tips/010_Python/regex.html",
    "href": "tips/010_Python/regex.html",
    "title": "Regex",
    "section": "",
    "text": "Suppose we need to extract row and col from a string ‘R1C2_row62_col24.png’:\n\nimport re\nsome_string = 'R1C2_row62_col24.png'\nm = re.match(r\".*row(\\d+)_col(\\d+).*.png\", some_string)\nrow = int(m.group(1))\ncol = int(m.group(2))\nassert row == 62\nassert col == 24\n\nor suppose we want to extract a string from a name ‘some_class_name_123.jpg’:\n\nimport re\nsome_string = 'some_class_name_123.jpg'\nm = re.match(r\"(.+)_\\d+.jpg\", some_string)\nassert m.group(1) == 'some_class_name'\n\nTest here https://regex101.com/\n\n\n\nCharacter\nAction\n\n\n\n\n.\nany character\n\n\n\\d\nonly numbers (if we need decimal point user a backslash (.)\n\n\n?\nZero or one character\n\n\n+\nOne or many\n\n\n*\nAny character count\n\n\n\nGood tutorial.",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Regex"
    ]
  },
  {
    "objectID": "tips/060_Misc/customdomain.html",
    "href": "tips/060_Misc/customdomain.html",
    "title": "Custom domain",
    "section": "",
    "text": "To have custom domain to host github pages follow this stackoverflow post carefullly. It is important to have a file CNAME (that is a simple text file that contains a custom domain) in the same folder with _quarto.yml.\nI’m using google hosting services (GSH) for my website www.nenadbozinovic.com. GHS can be accessed here: https://admin.google.com/.\nTo enable comments on a website I’m using gitcus.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Custom domain"
    ]
  },
  {
    "objectID": "tips/060_Misc/os.html",
    "href": "tips/060_Misc/os.html",
    "title": "OS",
    "section": "",
    "text": "To see hidden files on Mac in finder: `Command + Shift + .\nHome/End: Fn + Left/Right Arrow Beggining/End of the file: Fn + Cmd + Left/Right",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "OS"
    ]
  },
  {
    "objectID": "tips/060_Misc/os.html#macos",
    "href": "tips/060_Misc/os.html#macos",
    "title": "OS",
    "section": "",
    "text": "To see hidden files on Mac in finder: `Command + Shift + .\nHome/End: Fn + Left/Right Arrow Beggining/End of the file: Fn + Cmd + Left/Right",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "OS"
    ]
  },
  {
    "objectID": "tips/060_Misc/os.html#vscode",
    "href": "tips/060_Misc/os.html#vscode",
    "title": "OS",
    "section": "VSCode",
    "text": "VSCode\nTo prevent empty copy/paste go to Settings and search for editor.emptySelectionClipboard, then set to False.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "OS"
    ]
  },
  {
    "objectID": "tips/060_Misc/os.html#windows",
    "href": "tips/060_Misc/os.html#windows",
    "title": "OS",
    "section": "Windows",
    "text": "Windows\nUse PowerToys to remap keys on a keyboard (and quite a bit of other stuff the seem useful).",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "OS"
    ]
  },
  {
    "objectID": "tips/060_Misc/system_design.html",
    "href": "tips/060_Misc/system_design.html",
    "title": "Stream processing",
    "section": "",
    "text": "B-tree\nB-tree is stored on disk, each node has fixed size. It is log(N) fast to read. When writing if there is a a space, then just create a new node and point to the new node. When there are no space, then split the node into 2 nodes.\nIf B-tree dies, you can have write-ahead-log (WAL).\nB-trees are good for range queries.\nIt’s self-balancing, so it’s good for write-heavy workloads.\n\n\n\nimage.png\n\n\n\n\nMapReduce vs Spark\n\nChained jobs don’t know about each other\nEach job needs mapper and reducer - unnecessary sorting\nTons of disk usage (intermediate data is also stored on disk).\n\nSpark writes only input and output to disk the rest is in Resilient Distributed Dataset (RDD) in memory. As for fault tolerance, there is narrow dependency (all computations on one node) and wide dependency (computation on multiple nodes). If a node fails, then the computation is done on another node (occasional write checkpoint to disk).\n\n\nDoordash\nAPI gateway server code is written in Kotlin\nMIS - merchant information system MAS - merchant agent service gRPC - remote procedure call framework - one step above HTTP Protobuf - protocol buffer - language agnostic data interchange format spango jwt token is given and API gateway check again the token Order service - Cassandra, write heavy, LSM tree is super fast (Red black tree) when it accumulates RAM memory it writes to disk via S-Table (immutable read only on disk).\nIndexed BTree - good for reading, write for disk Single tree replication 256Gb RAM je max\nwhen user adds to cart, they are given session_id - unique identifier for the users session when user is ready to checkout, it clicks checkout - calls create cart from shopping session payment goes to Stripe - it returns stripe token\nGoogle Food Ordering Kafka, Flink, Cassandra, Redis\nJordan has no life - Uber - https://www.youtube.com/@jordanhasnolife5163/videos\n\n\n\nimage.png"
  },
  {
    "objectID": "tips/060_Misc/pdf.html",
    "href": "tips/060_Misc/pdf.html",
    "title": "Extract pages from PDF",
    "section": "",
    "text": "import PyPDF2\n\ndef extract_pages(source_pdf_path, output_pdf_path, pages_to_extract):\n    \"\"\"\n    Extracts specific pages from a PDF and saves them into a new PDF file.\n\n    Parameters:\n    - source_pdf_path: The path to the source PDF from which to extract pages.\n    - output_pdf_path: The path to save the new PDF with the extracted pages.\n    - pages_to_extract: A list of page numbers (0-indexed) to extract.\n    \"\"\"\n    # Open the source PDF\n    with open(source_pdf_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        writer = PyPDF2.PdfWriter()\n\n        # Loop through the list of pages to extract and add them to the writer\n        for page_num in pages_to_extract:\n            try:\n                writer.add_page(reader.pages[page_num])\n            except IndexError:\n                print(f\"Page {page_num} is out of range.\")\n                continue\n\n        # Save the pages to a new PDF\n        with open(output_pdf_path, 'wb') as output_pdf:\n            writer.write(output_pdf)\n\n# Example usage\nsource_pdf = \"input.pdf\"  # Replace this with your source PDF file path\noutput_pdf = \"output.pdf\"  # The output file\npages = [19,20,21]  # Example page numbers to extract (0-indexed)\n\nextract_pages(source_pdf, output_pdf, pages)",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Extract pages from PDF"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html",
    "href": "tips/060_Misc/sql/sql.html",
    "title": "SQL examples",
    "section": "",
    "text": "CREATE SCHEMA `new_schema` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\nCREATE TABLE `new_schema`.`users` (\n  `id` INT NOT NULL AUTO_INCREMENT COMMENT 'This is the primary index',\n  `name` VARCHAR(45) NOT NULL DEFAULT 'N/A',\n  PRIMARY KEY (`id`)\n);\n\nALTER TABLE `new_schema`.`users`\nADD COLUMN `age` INT NULL AFTER `name`;\n\nALTER TABLE `new_schema`.`users`\nCHANGE COLUMN `id` `id` INT(11) NOT NULL AUTO_INCREMENT,\nCHANGE COLUMN `name` `user_name` VARCHAR(45) NOT NULL DEFAULT 'No Name';\n\nSHOW FULL COLUMNS FROM `new_schema`.`users`;\n\nRENAME TABLE X TO Y;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#schema-and-table-creation",
    "href": "tips/060_Misc/sql/sql.html#schema-and-table-creation",
    "title": "SQL examples",
    "section": "",
    "text": "CREATE SCHEMA `new_schema` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\nCREATE TABLE `new_schema`.`users` (\n  `id` INT NOT NULL AUTO_INCREMENT COMMENT 'This is the primary index',\n  `name` VARCHAR(45) NOT NULL DEFAULT 'N/A',\n  PRIMARY KEY (`id`)\n);\n\nALTER TABLE `new_schema`.`users`\nADD COLUMN `age` INT NULL AFTER `name`;\n\nALTER TABLE `new_schema`.`users`\nCHANGE COLUMN `id` `id` INT(11) NOT NULL AUTO_INCREMENT,\nCHANGE COLUMN `name` `user_name` VARCHAR(45) NOT NULL DEFAULT 'No Name';\n\nSHOW FULL COLUMNS FROM `new_schema`.`users`;\n\nRENAME TABLE X TO Y;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#where-clause",
    "href": "tips/060_Misc/sql/sql.html#where-clause",
    "title": "SQL examples",
    "section": "WHERE CLAUSE",
    "text": "WHERE CLAUSE\nSELECT * FROM `new_schema`.`users` WHERE height IS NULL; -- this is sometimes relevant to include as null values can be excluded from query, \n-- for example !=2 will exclude not only what is equals to 2 but also null values, i.e. null != 2 is True.\n\nSELECT * FROM `new_schema`.`users` WHERE height IS NOT NULL;\n\nSELECT * FROM `new_schema`.`users` WHERE age &lt; 40 AND height &gt; 160;\n\nSELECT * FROM `new_schema`.`users` WHERE age &lt; 40 OR height &gt; 160;\n\nSELECT * FROM `new_schema`.`users` WHERE id &lt; 4 AND (age &gt; 30 OR height &gt; 175);\n\nSELECT * FROM `new_schema`.`users` WHERE `id` IN (1, 3);\n\nSELECT * FROM `new_schema`.`users` WHERE id NOT IN (1, 4);\n\nSELECT * FROM `new_schema`.`users` WHERE height BETWEEN 160 AND 190;\n\n-- Note: The percent sign (%) will match zero, one, or multiple characters. To match exactly one character we could use an underscore (_).\nSELECT * FROM `new_schema`.`users` WHERE name LIKE '%a%';\n\nSELECT * FROM `new_schema`.`users` WHERE name LIKE 'J%';\n```sql\n\n## JSON\n\n```sql\nALTER TABLE `new_schema`.`users` \nADD COLUMN `contact` JSON NULL AFTER `id`;  -- NULL means it can be null\n\nINSERT INTO `new_schema`.`users` (`id`, `name`, `contact`) VALUES \n  (1, 'John', JSON_OBJECT('phone', '123-456', 'address', 'New York')),\n  (2, 'May', JSON_OBJECT('phone', '888-99', 'address', 'LA')),\n  (3, 'Tim', NULL),\n  (4, 'Jay', JSON_OBJECT('phone', '321-6', 'address', 'Boston'));\n\nSELECT `id`, JSON_UNQUOTE(JSON_EXTRACT(contact, '$.phone')) AS phone\nFROM `new_schema`.`users`;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#delete-from",
    "href": "tips/060_Misc/sql/sql.html#delete-from",
    "title": "SQL examples",
    "section": "DELETE FROM",
    "text": "DELETE FROM\nDELETE FROM US.users WHERE id = 1;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#update",
    "href": "tips/060_Misc/sql/sql.html#update",
    "title": "SQL examples",
    "section": "UPDATE",
    "text": "UPDATE\nUPDATE `new_schema`.`users` SET `contact` = JSON_SET(contact, '$.phone', '6666', '$.phone_2', '888') WHERE `id` = 2;\n\nUPDATE Salary SET sex = CASE WHEN sex = 'f' THEN 'm' WHEN sex = 'm' THEN 'f' END;\n\n---------- DISTINCT, LIMIT, OFFSET, GROUP BY --\n\nSELECT DISTINCT age FROM `new_schema`.`users`;\n\nSELECT * FROM `new_schema`.`users` LIMIT 3 OFFSET 1;\n\nSELECT * FROM `new_schema`.`users` ORDER BY age DESC, height DESC;\n\nSELECT `age` FROM `new_schema`.`users` GROUP BY age;\n\n\nselect Email\nfrom Person\ngroup by Email\nhaving count(Email) &gt; 1;  -- WHERE is applied before data is grouped, making it more efficient for initial data filtration. HAVING, on the other hand, is applied after, making it less efficient for initial filtering.\n\n\nselect\n    actor_id,\n    director_id\nfrom ActorDirector\ngroup by\n    actor_id,\n    director_id\nhaving \n    count(*) &gt;= 3;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#count-sum-avg-min-max-concat",
    "href": "tips/060_Misc/sql/sql.html#count-sum-avg-min-max-concat",
    "title": "SQL examples",
    "section": "COUNT, SUM, AVG, MIN, MAX, CONCAT",
    "text": "COUNT, SUM, AVG, MIN, MAX, CONCAT\nSELECT COUNT(*) AS `user_count` FROM `new_schema`.`users` WHERE id &gt; 1;\nSELECT SUM(`age`) AS `sum_of_user_ages` FROM `new_schema`.`users`;\nSELECT AVG(`height`) AS `avg_user_height` FROM `new_schema`.`users`;\nSELECT MIN(`height`) AS `user_min` FROM `new_schema`.`users`;\nSELECT MAX(`height`) AS `user_max` FROM `new_schema`.`users`;\nSELECT CONCAT(`id`, '-', `name`) AS `identification`, `age` FROM `new_schema`.`users`;\n\nSELECT \n    date_id, \n    make_name, \n    COUNT(DISTINCT lead_id) AS unique_leads, \n    COUNT(DISTINCT partner_id) AS unique_partners\nFROM DailySales\nGROUP BY date_id, make_name\nORDER BY date_id, make_name;\n\nselect (COUNT(CASE WHEN Survived = 1 THEN 1 END) * 1.0 / count(*)) as overall_rate FROM titanic;\n\nselect \n    player_id,\n    MIN(event_date) as first_login\nfrom \n    Activity\ngroup by\n    player_id;\n\n\nSELECT date_id, make_name, COUNT(DISTINCT CONCAT(lead_id, '-', partner_id)) AS num_leads\nFROM CarLeads",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#cte",
    "href": "tips/060_Misc/sql/sql.html#cte",
    "title": "SQL examples",
    "section": "CTE",
    "text": "CTE\n– Common Table Expression (CTE) in SQL is a temporary result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement.\nWITH FirstLogins AS (\n    SELECT \n        player_id,\n        MIN(event_date) AS first_login\n    FROM \n        Activity\n    GROUP BY \n        player_id\n)\nSELECT \n    player_id, \n    first_login\nFROM \n    FirstLogins\nORDER BY \n    player_id;",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#join",
    "href": "tips/060_Misc/sql/sql.html#join",
    "title": "SQL examples",
    "section": "JOIN",
    "text": "JOIN\n– 1-to-1, 1-to-many, many-to-many (requires intermediate table) –\n– Careful! LEFT/RIGHT JOIN in combination with WHERE becomes in fact INNER JOIN!!\nSELECT * FROM `new_schema`.`users`\nLEFT JOIN `new_schema`.`orders` ON `users`.`id` = `orders`.`user_id`;\n\nselect \n    customer_id, \n    count(*) as count_no_trans \nfrom \n    Visits v\nleft join \n    Transactions t on v.visit_id = t.visit_id\nwhere \n    t.transaction_id is NULL\nGROUP BY \n    v.customer_id\n\n\nselect p.firstName, p.lastName, a.city, a.state\nfrom Person p\nleft join Address a on a.personId = p.personId",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#subquery",
    "href": "tips/060_Misc/sql/sql.html#subquery",
    "title": "SQL examples",
    "section": "SUBQUERY",
    "text": "SUBQUERY\nSELECT * FROM `new_schema`.`orders`\nWHERE user_id IN (\n  SELECT id FROM `new_schema`.`users`\n  WHERE name LIKE '%j%'\n);\n\n\nselect \n    name \nfrom \n    SalesPerson\nwhere sales_id not in (\n        select \n            distinct sales_id \n        from \n            Orders \n        left join \n            Company on Orders.com_id = Company.com_id \n        where\n            Company.name = \"RED\"\n)\n\n\nselect name as Customers from Customers where id not in (\nselect customerId from Orders join Customers on Orders.customerId = Customers.id\n)\n\n-- every derived table must have it's own alias\nselect sales_id from (\n    select sales_id \n    from Orders \n    group by sales_id\n    order by count(*) desc \n    limit 1\n) as this_is_mandatory_even_it_is_not_used\n– Differences: SELECT … INTO …: – Used within stored procedures or functions to store the result in a variable. SELECT … AS …: – Used to rename the output column in the result set.\nCOALESCE(column_name, 0) AS column_name – more portable outside MySQL or IFNULL(column_name,0)\nselect u.name, COALESCE(sum(r.distance), 0) as traveled_distance\nfrom Rides as r\nright join Users as u on u.id = r.user_id\ngroup by u.id\norder by traveled_distance desc, name asc; \n\n\n-- rare case where one merges two tables with all combinations possible\nselect o.sales_id, s.name\nfrom Orders as o, SalesPerson as s\norder by s.name, o.sales_id asc;\n\n-- To use the USING clause in a JOIN operation in MySQL, the column specified must have the same name in both tables involved in the join.\nSELECT * \nFROM Employees \nLEFT JOIN Salaries USING(employee_id)",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/sql.html#set-operations",
    "href": "tips/060_Misc/sql/sql.html#set-operations",
    "title": "SQL examples",
    "section": "Set operations",
    "text": "Set operations\n-- UNION: excluding duplicates.\n\nSELECT column_name FROM table1\nUNION\nSELECT column_name FROM table2;\n\n\n-- UNION ALL: including duplicates.\n\nSELECT column_name FROM table1\nUNION ALL\nSELECT column_name FROM table2;\n\n-- INTERSECT: Returns the intersection of two result sets, i.e., rows that are present in both result sets. MySQL does not natively support INTERSECT, but it can be emulated using joins.\nSELECT column_name FROM table1\nWHERE column_name IN (\n  SELECT column_name FROM table2\n);\n\n-- EXCEPT (or MINUS in some databases): Returns the difference between two result sets, i.e., rows that are in the first result set but not in the second. MySQL does not support EXCEPT, but it can be emulated using LEFT JOIN and WHERE clauses.\nSELECT column_name FROM table1\nWHERE column_name NOT IN (\n  SELECT column_name FROM table2\n);\n\n-- Example of Emulating INTERSECT in MySQL\nSELECT column_name FROM table1\nINNER JOIN table2 USING (column_name);\n\n\n-- Example of Emulating EXCEPT in MySQL\nSELECT column_name FROM table1\nLEFT JOIN table2 USING (column_name)\nWHERE table2.column_name IS NULL;\n\n## Create Stored Procedure to get orders by customer_id\n```sql\nCREATE PROCEDURE GetCustomerOrders (IN customer_id INT)\nBEGIN\n    SELECT * FROM Orders1 WHERE customer_id = customer_id;\nEND\n\n-- Create User-Defined Function to get total order amount by customer_id\nCREATE FUNCTION GetTotalOrderAmount(customer_id INT) \nRETURNS DECIMAL(10,2)\nDETERMINISTIC\nBEGIN\n    DECLARE total DECIMAL(10,2);\n    SELECT SUM(amount) INTO total FROM Orders WHERE customer_id = customer_id;\n    RETURN total;\nEND\n\n-- Example usage of the Stored Procedure\nCALL GetCustomerOrders(1);\n\n-- Example usage of the User-Defined Function\nSELECT GetTotalOrderAmount(1);",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "SQL examples"
    ]
  },
  {
    "objectID": "tips/060_Misc/statistics.html",
    "href": "tips/060_Misc/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "import matplotlib.pyplot as plt",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Statistics"
    ]
  },
  {
    "objectID": "tips/060_Misc/statistics.html#correlation",
    "href": "tips/060_Misc/statistics.html#correlation",
    "title": "Statistics",
    "section": "Correlation",
    "text": "Correlation\nPaerson coefficient between columns is a way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables: \\(\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{(X,Y)}{\\sigma_X \\sigma_Y}\\)\n\n\n\nimage.png",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Statistics"
    ]
  },
  {
    "objectID": "tips/060_Misc/labeling.html",
    "href": "tips/060_Misc/labeling.html",
    "title": "Labeling",
    "section": "",
    "text": "from PIL import Image, ImageDraw\nfrom pathlib import Path\nimport json",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Labeling"
    ]
  },
  {
    "objectID": "tips/060_Misc/labeling.html#labelme",
    "href": "tips/060_Misc/labeling.html#labelme",
    "title": "Labeling",
    "section": "LabelMe",
    "text": "LabelMe\nLabelMe is simple-to-use GUI for labeling. It is straightforward to use (pip install labelme, run in terminal as labelme):\n\n\n\nlabelme\n\n\nThe segmentation points will be saved as json files:\n{\n  \"version\": \"5.1.1\",  \n  \"flags\": {},  \n  \"shapes\": [\n    {\n      \"label\": \"1\",\n      \"points\": [\n        [\n          4.029411764705877,\n          1.0882352941176439\n        ],\n        [\n          0.7941176470588189,\n          53.73529411764706\n        ],\n        [\n          0.7941176470588189,\n          221.97058823529412\n        ],\n        [\n          12.852941176470587,\n          1.3823529411764672\n        ]\n      ],\n      \"group_id\": null,\n      \"shape_type\": \"polygon\",\n      \"flags\": {}\n    },\n    ...\nLet’s load some image and generated json file:\n\nim = Image.open(\"images/solar_panels.png\")\nwidth, height = im.size\nprint(width, height)\nim\n\n256 256\n\n\n\n\n\n\n\n\n\nLet’s convert the json into a mask:\n\nwith open('images/solar_panels.json') as f:\n    data = json.load(f)\n\nThere are:\n\npoints = data['shapes']\nlen(points)\n\n20\n\n\n20 data points, let’s make a mask out of those:\n\nmask = Image.new('L', (width, height), 0)\nfor group in points:\n    label_class = group['label']\n    polygon = group['points']\n    polygon = [(round(x), round(y)) for x,y in polygon]\n    ImageDraw.Draw(mask).polygon(polygon, outline=255, fill=255)\nmask\n\n\n\n\n\n\n\n\nand save this as a file:\n\nmask.save('images/solar_panels_mask.png')\n\n\nassert Path('images/solar_panels_mask.png').is_file()",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Labeling"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html",
    "href": "tips/DesignPatterns/structural.html",
    "title": "Structural",
    "section": "",
    "text": "Structural patterns explain how to assemble objects and classes into larger structures, while keeping these structures flexible and efficient.\nTaken fully from refactoring.guru, head there for a more detailed explanation. No code was written by me, I am just able to execute it here.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#bridge",
    "href": "tips/DesignPatterns/structural.html#bridge",
    "title": "Structural",
    "section": "Bridge",
    "text": "Bridge\nBridge is useful when we need to extend a class in several orthogonal (independent) dimensions. It separates the abstraction from the implementation using composition, so that they can be modified independently.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\n\nclass Abstraction:\n    \"\"\"\n    The Abstraction defines the interface for the \"control\" part of the two\n    class hierarchies. It maintains a reference to an object of the\n    Implementation hierarchy and delegates all of the real work to this object.\n    \"\"\"\n\n    def __init__(self, implementation: Implementation) -&gt; None:\n        self.implementation = implementation\n\n    def operation(self) -&gt; str:\n        return (f\"Abstraction: Base operation with:\\n\"\n                f\"{self.implementation.operation_implementation()}\")\n\n\nclass ExtendedAbstraction(Abstraction):\n    \"\"\"\n    You can extend the Abstraction without changing the Implementation classes.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        return (f\"ExtendedAbstraction: Extended operation with:\\n\"\n                f\"{self.implementation.operation_implementation()}\")\n\n\nclass Implementation(ABC):\n    \"\"\"\n    The Implementation defines the interface for all implementation classes. It\n    doesn't have to match the Abstraction's interface. In fact, the two\n    interfaces can be entirely different. Typically the Implementation interface\n    provides only primitive operations, while the Abstraction defines higher-\n    level operations based on those primitives.\n    \"\"\"\n\n    @abstractmethod\n    def operation_implementation(self) -&gt; str:\n        pass\n\n\n\"\"\"\nEach Concrete Implementation corresponds to a specific platform and implements\nthe Implementation interface using that platform's API.\n\"\"\"\n\n\nclass ConcreteImplementationA(Implementation):\n    def operation_implementation(self) -&gt; str:\n        return \"ConcreteImplementationA: Here's the result on the platform A.\"\n\n\nclass ConcreteImplementationB(Implementation):\n    def operation_implementation(self) -&gt; str:\n        return \"ConcreteImplementationB: Here's the result on the platform B.\"\n\n\ndef client_code(abstraction: Abstraction) -&gt; None:\n    \"\"\"\n    Except for the initialization phase, where an Abstraction object gets linked\n    with a specific Implementation object, the client code should only depend on\n    the Abstraction class. This way the client code can support any abstraction-\n    implementation combination.\n    \"\"\"\n\n    # ...\n\n    print(abstraction.operation(), end=\"\")\n\n    # ...\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    The client code should be able to work with any pre-configured abstraction-\n    implementation combination.\n    \"\"\"\n\n    implementation = ConcreteImplementationA()    # KEY POINT\n    abstraction = Abstraction(implementation)  \n    client_code(abstraction)\n\n    print(\"\\n\")\n\n    implementation = ConcreteImplementationB()\n    abstraction = ExtendedAbstraction(implementation)\n    client_code(abstraction)\n\nAbstraction: Base operation with:\nConcreteImplementationA: Here's the result on the platform A.\n\nExtendedAbstraction: Extended operation with:\nConcreteImplementationB: Here's the result on the platform B.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#adapter",
    "href": "tips/DesignPatterns/structural.html#adapter",
    "title": "Structural",
    "section": "Adapter",
    "text": "Adapter\nAdapter is useful as an afterthought solution (unlike Bridge).\n\n\nclass Target:\n    \"\"\"\n    The Target defines the domain-specific interface used by the client code.\n    \"\"\"\n\n    def request(self) -&gt; str:\n        return \"Target: The default target's behavior.\"\n\n\nclass Adaptee:\n    \"\"\"\n    The Adaptee contains some useful behavior, but its interface is incompatible\n    with the existing client code. The Adaptee needs some adaptation before the\n    client code can use it.\n    \"\"\"\n\n    def specific_request(self) -&gt; str:\n        return \".eetpadA eht fo roivaheb laicepS\"\n\n\nclass Adapter(Target, Adaptee):    # KEY POINT\n    \"\"\"\n    The Adapter makes the Adaptee's interface compatible with the Target's\n    interface via multiple inheritance.\n    \"\"\"\n\n    def request(self) -&gt; str:\n        return f\"Adapter: (TRANSLATED) {self.specific_request()[::-1]}\"\n\n\ndef client_code(target: \"Target\") -&gt; None:\n    \"\"\"\n    The client code supports all classes that follow the Target interface.\n    \"\"\"\n\n    print(target.request(), end=\"\")\n\n\nif __name__ == \"__main__\":\n    print(\"Client: I can work just fine with the Target objects:\")\n    target = Target()\n    client_code(target)\n    print(\"\\n\")\n\n    adaptee = Adaptee()\n    print(\"Client: The Adaptee class has a weird interface. \"\n          \"See, I don't understand it:\")\n    print(f\"Adaptee: {adaptee.specific_request()}\", end=\"\\n\\n\")\n\n    print(\"Client: But I can work with it via the Adapter:\")\n    adapter = Adapter()\n    client_code(adapter)\n\nClient: I can work just fine with the Target objects:\nTarget: The default target's behavior.\n\nClient: The Adaptee class has a weird interface. See, I don't understand it:\nAdaptee: .eetpadA eht fo roivaheb laicepS\n\nClient: But I can work with it via the Adapter:\nAdapter: (TRANSLATED) Special behavior of the Adaptee.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#composite",
    "href": "tips/DesignPatterns/structural.html#composite",
    "title": "Structural",
    "section": "Composite",
    "text": "Composite\nComposite is all about tree structures. Typically, only leaf nodes do the actual work, while non-leaf nodes (i.e. composites) delegate to their children and sum up the results.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\n\nclass Component(ABC):\n    \"\"\"\n    The base Component class declares common operations for both simple and\n    complex objects of a composition.\n    \"\"\"\n\n    @property\n    def parent(self) -&gt; Component:\n        return self._parent\n\n    @parent.setter\n    def parent(self, parent: Component):\n        \"\"\"\n        Optionally, the base Component can declare an interface for setting and\n        accessing a parent of the component in a tree structure. It can also\n        provide some default implementation for these methods.\n        \"\"\"\n\n        self._parent = parent\n\n    \"\"\"\n    In some cases, it would be beneficial to define the child-management\n    operations right in the base Component class. This way, you won't need to\n    expose any concrete component classes to the client code, even during the\n    object tree assembly. The downside is that these methods will be empty for\n    the leaf-level components.\n    \"\"\"\n\n    def add(self, component: Component) -&gt; None:\n        pass\n\n    def remove(self, component: Component) -&gt; None:\n        pass\n\n    def is_composite(self) -&gt; bool:\n        \"\"\"\n        You can provide a method that lets the client code figure out whether a\n        component can bear children.\n        \"\"\"\n\n        return False\n\n    @abstractmethod\n    def operation(self) -&gt; str:\n        \"\"\"\n        The base Component may implement some default behavior or leave it to\n        concrete classes (by declaring the method containing the behavior as\n        \"abstract\").\n        \"\"\"\n\n        pass\n\n\nclass Leaf(Component):\n    \"\"\"\n    The Leaf class represents the end objects of a composition. A leaf can't\n    have any children.\n\n    Usually, it's the Leaf objects that do the actual work, whereas Composite\n    objects only delegate to their sub-components.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        return \"Leaf\"\n\n\nclass Composite(Component):\n    \"\"\"\n    The Composite class represents the complex components that may have\n    children. Usually, the Composite objects delegate the actual work to their\n    children and then \"sum-up\" the result.    # KEY POINT\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._children: List[Component] = []\n\n    \"\"\"\n    A composite object can add or remove other components (both simple or\n    complex) to or from its child list.\n    \"\"\"\n\n    def add(self, component: Component) -&gt; None:\n        self._children.append(component)\n        component.parent = self\n\n    def remove(self, component: Component) -&gt; None:\n        self._children.remove(component)\n        component.parent = None\n\n    def is_composite(self) -&gt; bool:\n        return True\n\n    def operation(self) -&gt; str:\n        \"\"\"\n        The Composite executes its primary logic in a particular way. It\n        traverses recursively through all its children, collecting and summing\n        their results. Since the composite's children pass these calls to their\n        children and so forth, the whole object tree is traversed as a result.\n        \"\"\"\n\n        results = []\n        for child in self._children:\n            results.append(child.operation())\n        return f\"Branch({'+'.join(results)})\"\n\n\ndef client_code(component: Component) -&gt; None:\n    \"\"\"\n    The client code works with all of the components via the base interface.\n    \"\"\"\n\n    print(f\"RESULT: {component.operation()}\", end=\"\")\n\n\ndef client_code2(component1: Component, component2: Component) -&gt; None:\n    \"\"\"\n    Thanks to the fact that the child-management operations are declared in the\n    base Component class, the client code can work with any component, simple or\n    complex, without depending on their concrete classes.\n    \"\"\"\n\n    if component1.is_composite():\n        component1.add(component2)\n\n    print(f\"RESULT: {component1.operation()}\", end=\"\")\n\n\nif __name__ == \"__main__\":\n    # This way the client code can support the simple leaf components...\n    simple = Leaf()\n    print(\"Client: I've got a simple component:\")\n    client_code(simple)\n    print(\"\\n\")\n\n    # ...as well as the complex composites.\n    tree = Composite()\n\n    branch1 = Composite()\n    branch1.add(Leaf())\n    branch1.add(Leaf())\n\n    branch2 = Composite()\n    branch2.add(Leaf())\n\n    tree.add(branch1)\n    tree.add(branch2)\n\n    print(\"Client: Now I've got a composite tree:\")\n    client_code(tree)\n    print(\"\\n\")\n\n    print(\"Client: I don't need to check the components classes even when managing the tree:\")\n    client_code2(tree, simple)\n\nClient: I've got a simple component:\nRESULT: Leaf\n\nClient: Now I've got a composite tree:\nRESULT: Branch(Branch(Leaf+Leaf)+Branch(Leaf))\n\nClient: I don't need to check the components classes even when managing the tree:\nRESULT: Branch(Branch(Leaf+Leaf)+Branch(Leaf)+Leaf)",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#decorator",
    "href": "tips/DesignPatterns/structural.html#decorator",
    "title": "Structural",
    "section": "Decorator",
    "text": "Decorator\nWe can create decorated objects without having to change any code of real objects, instead, just decorating them. A user can run code on real object or a decorated one. This way we allows adding new behaviors to objects dynamically.\n\n\nclass Component():\n    \"\"\"\n    The base Component interface defines operations that can be altered by\n    decorators.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        pass\n\n\nclass ConcreteComponent(Component):\n    \"\"\"\n    Concrete Components provide default implementations of the operations. There\n    might be several variations of these classes.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        return \"ConcreteComponent\"\n\n\nclass Decorator(Component):\n    \"\"\"\n    The base Decorator class follows the same interface as the other components.\n    The primary purpose of this class is to define the wrapping interface for\n    all concrete decorators. The default implementation of the wrapping code\n    might include a field for storing a wrapped component and the means to\n    initialize it.\n    \"\"\"\n\n    _component: Component = None\n\n    def __init__(self, component: Component) -&gt; None:\n        self._component = component\n\n    @property\n    def component(self) -&gt; Component:\n        \"\"\"\n        The Decorator delegates all work to the wrapped component.\n        \"\"\"\n\n        return self._component\n\n    def operation(self) -&gt; str:\n        return self._component.operation()\n\n\nclass ConcreteDecoratorA(Decorator):\n    \"\"\"\n    Concrete Decorators call the wrapped object and alter its result in some\n    way.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        \"\"\"\n        Decorators may call parent implementation of the operation, instead of\n        calling the wrapped object directly. This approach simplifies extension\n        of decorator classes.\n        \"\"\"\n        return f\"ConcreteDecoratorA({self.component.operation()})\"   # KEY POINT\n\n\nclass ConcreteDecoratorB(Decorator):\n    \"\"\"\n    Decorators can execute their behavior either before or after the call to a\n    wrapped object.\n    \"\"\"\n\n    def operation(self) -&gt; str:\n        return f\"ConcreteDecoratorB({self.component.operation()})\"\n\n\ndef client_code(component: Component) -&gt; None:\n    \"\"\"\n    The client code works with all objects using the Component interface. This\n    way it can stay independent of the concrete classes of components it works\n    with.\n    \"\"\"\n\n    # ...\n\n    print(f\"RESULT: {component.operation()}\", end=\"\")\n\n    # ...\n\n\nif __name__ == \"__main__\":\n    # This way the client code can support both simple components...\n    simple = ConcreteComponent()\n    print(\"Client: I've got a simple component:\")\n    client_code(simple)\n    print(\"\\n\")\n\n    # ...as well as decorated ones.\n    #\n    # Note how decorators can wrap not only simple components but the other\n    # decorators as well.\n    decorator1 = ConcreteDecoratorA(simple)\n    decorator2 = ConcreteDecoratorB(decorator1)\n    print(\"Client: Now I've got a decorated component:\")\n    client_code(decorator2)\n\nClient: I've got a simple component:\nRESULT: ConcreteComponent\n\nClient: Now I've got a decorated component:\nRESULT: ConcreteDecoratorB(ConcreteDecoratorA(ConcreteComponent))",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#facade",
    "href": "tips/DesignPatterns/structural.html#facade",
    "title": "Structural",
    "section": "Facade",
    "text": "Facade\nFacade delegates work to subsystems. It is a part of the design process and is not applied after the fact like the adapter pattern.\n\n\nfrom __future__ import annotations\n\n\nclass Facade:\n    \"\"\"\n    The Facade class provides a simple interface to the complex logic of one or\n    several subsystems. The Facade delegates the client requests to the\n    appropriate objects within the subsystem. The Facade is also responsible for\n    managing their lifecycle. All of this shields the client from the undesired\n    complexity of the subsystem.\n    \"\"\"\n\n    def __init__(self, subsystem1: Subsystem1, subsystem2: Subsystem2) -&gt; None:\n        \"\"\"\n        Depending on your application's needs, you can provide the Facade with\n        existing subsystem objects or force the Facade to create them on its\n        own.\n        \"\"\"\n\n        self._subsystem1 = subsystem1 or Subsystem1()\n        self._subsystem2 = subsystem2 or Subsystem2()\n\n    def operation(self) -&gt; str:\n        \"\"\"\n        The Facade's methods are convenient shortcuts to the sophisticated\n        functionality of the subsystems. However, clients get only to a fraction\n        of a subsystem's capabilities.\n        \"\"\"\n\n        results = []\n        results.append(\"Facade initializes subsystems:\")\n        results.append(self._subsystem1.operation1())\n        results.append(self._subsystem2.operation1())\n        results.append(\"Facade orders subsystems to perform the action:\")\n        results.append(self._subsystem1.operation_n())\n        results.append(self._subsystem2.operation_z())\n        return \"\\n\".join(results)\n\n\nclass Subsystem1:\n    \"\"\"\n    The Subsystem can accept requests either from the facade or client directly.\n    In any case, to the Subsystem, the Facade is yet another client, and it's\n    not a part of the Subsystem.\n    \"\"\"\n\n    def operation1(self) -&gt; str:\n        return \"Subsystem1: Ready!\"\n\n    # ...\n\n    def operation_n(self) -&gt; str:\n        return \"Subsystem1: Go!\"\n\n\nclass Subsystem2:\n    \"\"\"\n    Some facades can work with multiple subsystems at the same time.\n    \"\"\"\n\n    def operation1(self) -&gt; str:\n        return \"Subsystem2: Get ready!\"\n\n    # ...\n\n    def operation_z(self) -&gt; str:\n        return \"Subsystem2: Fire!\"\n\n\ndef client_code(facade: Facade) -&gt; None:\n    \"\"\"\n    The client code works with complex subsystems through a simple interface\n    provided by the Facade. When a facade manages the lifecycle of the\n    subsystem, the client might not even know about the existence of the\n    subsystem. This approach lets you keep the complexity under control.\n    \"\"\"\n\n    print(facade.operation(), end=\"\")\n\n\nif __name__ == \"__main__\":\n    # The client code may have some of the subsystem's objects already created.\n    # In this case, it might be worthwhile to initialize the Facade with these\n    # objects instead of letting the Facade create new instances.\n    subsystem1 = Subsystem1()\n    subsystem2 = Subsystem2()\n    facade = Facade(subsystem1, subsystem2)    # KEY POINT\n    client_code(facade)\n\nFacade initializes subsystems:\nSubsystem1: Ready!\nSubsystem2: Get ready!\nFacade orders subsystems to perform the action:\nSubsystem1: Go!\nSubsystem2: Fire!",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#flyweight",
    "href": "tips/DesignPatterns/structural.html#flyweight",
    "title": "Structural",
    "section": "Flyweight",
    "text": "Flyweight\nIs all about caching shared data to reduce memory footprint.\n\n\nimport json\nfrom typing import Dict\n\n\nclass Flyweight():\n    \"\"\"\n    The Flyweight stores a common portion of the state (also called intrinsic\n    state) that belongs to multiple real business entities. The Flyweight\n    accepts the rest of the state (extrinsic state, unique for each entity) via\n    its method parameters.\n    \"\"\"\n\n    def __init__(self, shared_state: str) -&gt; None:\n        self._shared_state = shared_state\n\n    def operation(self, unique_state: str) -&gt; None:\n        s = json.dumps(self._shared_state)\n        u = json.dumps(unique_state)\n        print(f\"Flyweight: Displaying shared ({s}) and unique ({u}) state.\", end=\"\")\n\n\nclass FlyweightFactory():\n    \"\"\"\n    The Flyweight Factory creates and manages the Flyweight objects. It ensures\n    that flyweights are shared correctly. When the client requests a flyweight,\n    the factory either returns an existing instance or creates a new one, if it\n    doesn't exist yet.\n    \"\"\"\n\n    _flyweights: Dict[str, Flyweight] = {}\n\n    def __init__(self, initial_flyweights: Dict) -&gt; None:\n        for state in initial_flyweights:\n            self._flyweights[self.get_key(state)] = Flyweight(state)\n\n    def get_key(self, state: Dict) -&gt; str:\n        \"\"\"\n        Returns a Flyweight's string hash for a given state.\n        \"\"\"\n\n        return \"_\".join(sorted(state))\n\n    def get_flyweight(self, shared_state: Dict) -&gt; Flyweight:\n        \"\"\"\n        Returns an existing Flyweight with a given state or creates a new one.\n        \"\"\"\n\n        key = self.get_key(shared_state)\n\n        if not self._flyweights.get(key):    # KEY POINT\n            print(\"FlyweightFactory: Can't find a flyweight, creating new one.\")\n            self._flyweights[key] = Flyweight(shared_state)\n        else:\n            print(\"FlyweightFactory: Reusing existing flyweight.\")\n\n        return self._flyweights[key]\n\n    def list_flyweights(self) -&gt; None:\n        count = len(self._flyweights)\n        print(f\"FlyweightFactory: I have {count} flyweights:\")\n        print(\"\\n\".join(map(str, self._flyweights.keys())), end=\"\")\n\n\ndef add_car_to_police_database(\n    factory: FlyweightFactory, plates: str, owner: str,\n    brand: str, model: str, color: str\n) -&gt; None:\n    print(\"\\n\\nClient: Adding a car to database.\")\n    flyweight = factory.get_flyweight([brand, model, color])\n    # The client code either stores or calculates extrinsic state and passes it\n    # to the flyweight's methods.\n    flyweight.operation([plates, owner])\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    The client code usually creates a bunch of pre-populated flyweights in the\n    initialization stage of the application.\n    \"\"\"\n\n    factory = FlyweightFactory([\n        [\"Chevrolet\", \"Camaro2018\", \"pink\"],\n        [\"Mercedes Benz\", \"C300\", \"black\"],\n        [\"Mercedes Benz\", \"C500\", \"red\"],\n        [\"BMW\", \"M5\", \"red\"],\n        [\"BMW\", \"X6\", \"white\"],\n    ])\n\n    factory.list_flyweights()\n\n    add_car_to_police_database(\n        factory, \"CL234IR\", \"James Doe\", \"BMW\", \"M5\", \"red\")\n\n    add_car_to_police_database(\n        factory, \"CL234IR\", \"James Doe\", \"BMW\", \"X1\", \"red\")\n\n    print(\"\\n\")\n\n    factory.list_flyweights()\n\nFlyweightFactory: I have 5 flyweights:\nCamaro2018_Chevrolet_pink\nC300_Mercedes Benz_black\nC500_Mercedes Benz_red\nBMW_M5_red\nBMW_X6_white\n\nClient: Adding a car to database.\nFlyweightFactory: Reusing existing flyweight.\nFlyweight: Displaying shared ([\"BMW\", \"M5\", \"red\"]) and unique ([\"CL234IR\", \"James Doe\"]) state.\n\nClient: Adding a car to database.\nFlyweightFactory: Can't find a flyweight, creating new one.\nFlyweight: Displaying shared ([\"BMW\", \"X1\", \"red\"]) and unique ([\"CL234IR\", \"James Doe\"]) state.\n\nFlyweightFactory: I have 6 flyweights:\nCamaro2018_Chevrolet_pink\nC300_Mercedes Benz_black\nC500_Mercedes Benz_red\nBMW_M5_red\nBMW_X6_white\nBMW_X1_red",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/structural.html#proxy",
    "href": "tips/DesignPatterns/structural.html#proxy",
    "title": "Structural",
    "section": "Proxy",
    "text": "Proxy\nClient can call both real object or proxy; when calling proxy we can add hooks.\n\n\nfrom abc import ABC, abstractmethod\n\n\nclass Subject(ABC):\n    \"\"\"\n    The Subject interface declares common operations for both RealSubject and\n    the Proxy. As long as the client works with RealSubject using this\n    interface, you'll be able to pass it a proxy instead of a real subject.\n    \"\"\"\n\n    @abstractmethod\n    def request(self) -&gt; None:\n        pass\n\n\nclass RealSubject(Subject):\n    \"\"\"\n    The RealSubject contains some core business logic. Usually, RealSubjects are\n    capable of doing some useful work which may also be very slow or sensitive -\n    e.g. correcting input data. A Proxy can solve these issues without any\n    changes to the RealSubject's code.\n    \"\"\"\n\n    def request(self) -&gt; None:\n        print(\"RealSubject: Handling request.\")\n\n\nclass Proxy(Subject):\n    \"\"\"\n    The Proxy has an interface identical to the RealSubject.\n    \"\"\"\n\n    def __init__(self, real_subject: RealSubject) -&gt; None:\n        self._real_subject = real_subject\n\n    def request(self) -&gt; None:\n        \"\"\"\n        The most common applications of the Proxy pattern are lazy loading,\n        caching, controlling the access, logging, etc. A Proxy can perform one\n        of these things and then, depending on the result, pass the execution to\n        the same method in a linked RealSubject object.\n        \"\"\"\n\n        if self.check_access():   # KEY POINT\n            self._real_subject.request()\n            self.log_access()\n\n    def check_access(self) -&gt; bool:\n        print(\"Proxy: Checking access prior to firing a real request.\")\n        return True\n\n    def log_access(self) -&gt; None:\n        print(\"Proxy: Logging the time of request.\", end=\"\")\n\n\ndef client_code(subject: Subject) -&gt; None:\n    \"\"\"\n    The client code is supposed to work with all objects (both subjects and\n    proxies) via the Subject interface in order to support both real subjects\n    and proxies. In real life, however, clients mostly work with their real\n    subjects directly. In this case, to implement the pattern more easily, you\n    can extend your proxy from the real subject's class.\n    \"\"\"\n\n    # ...\n\n    subject.request()\n\n    # ...\n\n\nif __name__ == \"__main__\":\n    print(\"Client: Executing the client code with a real subject:\")\n    real_subject = RealSubject()\n    client_code(real_subject)\n\n    print(\"\")\n\n    print(\"Client: Executing the same client code with a proxy:\")\n    proxy = Proxy(real_subject)\n    client_code(proxy)\n\nClient: Executing the client code with a real subject:\nRealSubject: Handling request.\n\nClient: Executing the same client code with a proxy:\nProxy: Checking access prior to firing a real request.\nRealSubject: Handling request.\nProxy: Logging the time of request.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Structural"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html",
    "href": "tips/DesignPatterns/behavioral.html",
    "title": "Behavioral",
    "section": "",
    "text": "Behavioral patterns take care of effective communication and the assignment of responsibilities between objects.\nCode taken fully from refactoring.guru with my descriptions. Again, no code was written by me, I am just able to execute it here.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#chain-of-responsibility",
    "href": "tips/DesignPatterns/behavioral.html#chain-of-responsibility",
    "title": "Behavioral",
    "section": "Chain of Responsibility",
    "text": "Chain of Responsibility\nPasses request down the chain, until it is handled.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Optional\n\n\nclass Handler(ABC):\n    \"\"\"\n    The Handler interface declares a method for building the chain of handlers.\n    It also declares a method for executing a request.\n    \"\"\"\n\n    @abstractmethod\n    def set_next(self, handler: Handler) -&gt; Handler:\n        pass\n\n    @abstractmethod\n    def handle(self, request) -&gt; Optional[str]:\n        pass\n\n\nclass AbstractHandler(Handler):\n    \"\"\"\n    The default chaining behavior can be implemented inside a base handler\n    class.\n    \"\"\"\n\n    _next_handler: Handler = None\n\n    def set_next(self, handler: Handler) -&gt; Handler:\n        self._next_handler = handler\n        # Returning a handler from here will let us link handlers in a\n        # convenient way like this:\n        # monkey.set_next(squirrel).set_next(dog)\n        return handler\n\n    @abstractmethod\n    def handle(self, request: Any) -&gt; str:\n        if self._next_handler:\n            return self._next_handler.handle(request)\n\n        return None\n\n\n\"\"\"\nAll Concrete Handlers either handle a request or pass it to the next handler in\nthe chain.\n\"\"\"\n\n\nclass MonkeyHandler(AbstractHandler):\n    def handle(self, request: Any) -&gt; str:\n        if request == \"Banana\":\n            return f\"Monkey: I'll eat the {request}\"\n        else:\n            return super().handle(request)\n\n\nclass SquirrelHandler(AbstractHandler):\n    def handle(self, request: Any) -&gt; str:\n        if request == \"Nut\":\n            return f\"Squirrel: I'll eat the {request}\"\n        else:\n            return super().handle(request)\n\n\nclass DogHandler(AbstractHandler):\n    def handle(self, request: Any) -&gt; str:\n        if request == \"MeatBall\":\n            return f\"Dog: I'll eat the {request}\"\n        else:\n            return super().handle(request)\n\n\ndef client_code(handler: Handler) -&gt; None:\n    \"\"\"\n    The client code is usually suited to work with a single handler. In most\n    cases, it is not even aware that the handler is part of a chain.\n    \"\"\"\n\n    for food in [\"Nut\", \"Banana\", \"Cup of coffee\"]:\n        print(f\"\\nClient: Who wants a {food}?\")\n        result = handler.handle(food)\n        if result:\n            print(f\"  {result}\", end=\"\")\n        else:\n            print(f\"  {food} was left untouched.\", end=\"\")\n\n\nif __name__ == \"__main__\":\n    monkey = MonkeyHandler()\n    squirrel = SquirrelHandler()\n    dog = DogHandler()\n\n    monkey.set_next(squirrel).set_next(dog)  # KEY POINT\n\n    # The client should be able to send a request to any handler, not just the\n    # first one in the chain.\n    print(\"Chain: Monkey &gt; Squirrel &gt; Dog\")\n    client_code(monkey)\n    print(\"\\n\")\n\n    print(\"Subchain: Squirrel &gt; Dog\")\n    client_code(squirrel)\n\nChain: Monkey &gt; Squirrel &gt; Dog\n\nClient: Who wants a Nut?\n  Squirrel: I'll eat the Nut\nClient: Who wants a Banana?\n  Monkey: I'll eat the Banana\nClient: Who wants a Cup of coffee?\n  Cup of coffee was left untouched.\n\nSubchain: Squirrel &gt; Dog\n\nClient: Who wants a Nut?\n  Squirrel: I'll eat the Nut\nClient: Who wants a Banana?\n  Banana was left untouched.\nClient: Who wants a Cup of coffee?\n  Cup of coffee was left untouched.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#command",
    "href": "tips/DesignPatterns/behavioral.html#command",
    "title": "Behavioral",
    "section": "Command",
    "text": "Command\nCommand can either execute simple stuff or delegate to receiver more complex work.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\n\nclass Command(ABC):\n    \"\"\"\n    The Command interface declares a method for executing a command.\n    \"\"\"\n\n    @abstractmethod\n    def execute(self) -&gt; None:\n        pass\n\n\nclass SimpleCommand(Command):\n    \"\"\"\n    Some commands can implement simple operations on their own.\n    \"\"\"\n\n    def __init__(self, payload: str) -&gt; None:\n        self._payload = payload\n\n    def execute(self) -&gt; None:\n        print(f\"SimpleCommand: See, I can do simple things like printing\"\n              f\"({self._payload})\")\n\n\nclass ComplexCommand(Command):\n    \"\"\"\n    However, some commands can delegate more complex operations to other\n    objects, called \"receivers.\"\n    \"\"\"\n\n    def __init__(self, receiver: Receiver, a: str, b: str) -&gt; None:\n        \"\"\"\n        Complex commands can accept one or several receiver objects along with\n        any context data via the constructor.\n        \"\"\"\n\n        self._receiver = receiver\n        self._a = a\n        self._b = b\n\n    def execute(self) -&gt; None:\n        \"\"\"\n        Commands can delegate to any methods of a receiver.\n        \"\"\"\n\n        print(\"ComplexCommand: Complex stuff should be done by a receiver object\", end=\"\")\n        self._receiver.do_something(self._a)    # KEY POINT\n        self._receiver.do_something_else(self._b)\n\n\nclass Receiver:\n    \"\"\"\n    The Receiver classes contain some important business logic. They know how to\n    perform all kinds of operations, associated with carrying out a request. In\n    fact, any class may serve as a Receiver.\n    \"\"\"\n\n    def do_something(self, a: str) -&gt; None:\n        print(f\"\\nReceiver: Working on ({a}.)\", end=\"\")\n\n    def do_something_else(self, b: str) -&gt; None:\n        print(f\"\\nReceiver: Also working on ({b}.)\", end=\"\")\n\n\nclass Invoker:\n    \"\"\"\n    The Invoker is associated with one or several commands. It sends a request\n    to the command.\n    \"\"\"\n\n    _on_start = None\n    _on_finish = None\n\n    \"\"\"\n    Initialize commands.\n    \"\"\"\n\n    def set_on_start(self, command: Command):\n        self._on_start = command\n\n    def set_on_finish(self, command: Command):\n        self._on_finish = command\n\n    def do_something_important(self) -&gt; None:\n        \"\"\"\n        The Invoker does not depend on concrete command or receiver classes. The\n        Invoker passes a request to a receiver indirectly, by executing a\n        command.\n        \"\"\"\n\n        print(\"Invoker: Does anybody want something done before I begin?\")\n        if isinstance(self._on_start, Command):\n            self._on_start.execute()\n\n        print(\"Invoker: ...doing something really important...\")\n\n        print(\"Invoker: Does anybody want something done after I finish?\")\n        if isinstance(self._on_finish, Command):\n            self._on_finish.execute()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    The client code can parameterize an invoker with any commands.\n    \"\"\"\n\n    invoker = Invoker()\n    invoker.set_on_start(SimpleCommand(\"Say Hi!\"))\n    invoker.set_on_finish(ComplexCommand(\n        Receiver(), \"Send email\", \"Save report\"))\n\n    invoker.do_something_important()\n\nInvoker: Does anybody want something done before I begin?\nSimpleCommand: See, I can do simple things like printing(Say Hi!)\nInvoker: ...doing something really important...\nInvoker: Does anybody want something done after I finish?\nComplexCommand: Complex stuff should be done by a receiver object\nReceiver: Working on (Send email.)\nReceiver: Also working on (Save report.)",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#iterator",
    "href": "tips/DesignPatterns/behavioral.html#iterator",
    "title": "Behavioral",
    "section": "Iterator",
    "text": "Iterator\nDefines how to iterate over a collection.\n\n\nfrom __future__ import annotations\nfrom collections.abc import Iterable, Iterator\nfrom typing import Any\n\n\n\"\"\"\nTo create an iterator in Python, there are two abstract classes from the built-\nin `collections` module - Iterable,Iterator.  # KEY POINT\n\nWe need to implement the `__iter__()` method in the iterated object (collection), \nand the `__next__ ()` method in the iterator.\n\"\"\"\n\n\nclass AlphabeticalOrderIterator(Iterator):\n    \"\"\"\n    Concrete Iterators implement various traversal algorithms. These classes\n    store the current traversal position at all times.\n    \"\"\"\n\n    \"\"\"\n    `_position` attribute stores the current traversal position. An iterator may\n    have a lot of other fields for storing iteration state, especially when it\n    is supposed to work with a particular kind of collection.\n    \"\"\"\n    _position: int = None\n\n    \"\"\"\n    This attribute indicates the traversal direction.\n    \"\"\"\n    _reverse: bool = False\n\n    def __init__(self, collection: WordsCollection, reverse: bool = False) -&gt; None:\n        self._collection = collection\n        self._reverse = reverse\n        self._position = -1 if reverse else 0\n\n    def __next__(self) -&gt; Any:\n        \"\"\"\n        The __next__() method must return the next item in the sequence. On\n        reaching the end, and in subsequent calls, it must raise StopIteration.\n        \"\"\"\n        try:\n            value = self._collection[self._position]\n            self._position += -1 if self._reverse else 1\n        except IndexError:\n            raise StopIteration()\n\n        return value\n\n\nclass WordsCollection(Iterable):\n    \"\"\"\n    Concrete Collections provide one or several methods for retrieving fresh\n    iterator instances, compatible with the collection class.\n    \"\"\"\n\n    def __init__(self, collection: list[Any] | None = None) -&gt; None:\n        self._collection = collection or []\n\n\n    def __getitem__(self, index: int) -&gt; Any:\n        return self._collection[index]\n\n    def __iter__(self) -&gt; AlphabeticalOrderIterator:\n        \"\"\"\n        The __iter__() method returns the iterator object itself, by default we\n        return the iterator in ascending order.\n        \"\"\"\n        return AlphabeticalOrderIterator(self)\n\n    def get_reverse_iterator(self) -&gt; AlphabeticalOrderIterator:\n        return AlphabeticalOrderIterator(self, True)\n\n    def add_item(self, item: Any) -&gt; None:\n        self._collection.append(item)\n\n\nif __name__ == \"__main__\":\n    # The client code may or may not know about the Concrete Iterator or\n    # Collection classes, depending on the level of indirection you want to keep\n    # in your program.\n    collection = WordsCollection()\n    collection.add_item(\"First\")\n    collection.add_item(\"Second\")\n    collection.add_item(\"Third\")\n\n    print(\"Straight traversal:\")\n    print(\"\\n\".join(collection))\n    print(\"\")\n\n    print(\"Reverse traversal:\")\n    print(\"\\n\".join(collection.get_reverse_iterator()), end=\"\")\n\nStraight traversal:\nFirst\nSecond\nThird\n\nReverse traversal:\nThird\nSecond\nFirst",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#mediator",
    "href": "tips/DesignPatterns/behavioral.html#mediator",
    "title": "Behavioral",
    "section": "Mediator",
    "text": "Mediator\nEnables communication between objects.\n\n\nfrom __future__ import annotations\nfrom abc import ABC\n\n\nclass Mediator(ABC):\n    \"\"\"\n    The Mediator interface declares a method used by components to notify the\n    mediator about various events. The Mediator may react to these events and\n    pass the execution to other components.\n    \"\"\"\n\n    def notify(self, sender: object, event: str) -&gt; None:\n        pass\n\n\nclass ConcreteMediator(Mediator):\n    def __init__(self, component1: Component1, component2: Component2) -&gt; None:\n        self._component1 = component1\n        self._component1.mediator = self\n        self._component2 = component2\n        self._component2.mediator = self\n\n    def notify(self, sender: object, event: str) -&gt; None:\n        if event == \"A\":\n            print(\"Mediator reacts on A and triggers following operations:\")\n            self._component2.do_c()\n        elif event == \"D\":\n            print(\"Mediator reacts on D and triggers following operations:\")\n            self._component1.do_b()\n            self._component2.do_c()\n\n\nclass BaseComponent:\n    \"\"\"\n    The Base Component provides the basic functionality of storing a mediator's\n    instance inside component objects.\n    \"\"\"\n\n    def __init__(self, mediator: Mediator = None) -&gt; None:\n        self._mediator = mediator\n\n    @property\n    def mediator(self) -&gt; Mediator:\n        return self._mediator\n\n    @mediator.setter\n    def mediator(self, mediator: Mediator) -&gt; None:\n        self._mediator = mediator\n\n\n\"\"\"\nConcrete Components implement various functionality. They don't depend on other\ncomponents. They also don't depend on any concrete mediator classes.\n\"\"\"\n\n\nclass Component1(BaseComponent):\n    def do_a(self) -&gt; None:\n        print(\"Component 1 does A.\")\n        self.mediator.notify(self, \"A\")\n\n    def do_b(self) -&gt; None:\n        print(\"Component 1 does B.\")\n        self.mediator.notify(self, \"B\")\n\n\nclass Component2(BaseComponent):\n    def do_c(self) -&gt; None:\n        print(\"Component 2 does C.\")\n        self.mediator.notify(self, \"C\")\n\n    def do_d(self) -&gt; None:\n        print(\"Component 2 does D.\")\n        self.mediator.notify(self, \"D\")\n\n\nif __name__ == \"__main__\":\n    # The client code.\n    c1 = Component1()\n    c2 = Component2()\n    mediator = ConcreteMediator(c1, c2)   # KEY POINT\n\n    print(\"Client triggers operation A.\")\n    c1.do_a()\n\n    print(\"\\n\", end=\"\")\n\n    print(\"Client triggers operation D.\")\n    c2.do_d()\n\nClient triggers operation A.\nComponent 1 does A.\nMediator reacts on A and triggers following operations:\nComponent 2 does C.\n\nClient triggers operation D.\nComponent 2 does D.\nMediator reacts on D and triggers following operations:\nComponent 1 does B.\nComponent 2 does C.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#memento",
    "href": "tips/DesignPatterns/behavioral.html#memento",
    "title": "Behavioral",
    "section": "Memento",
    "text": "Memento\nSave a state, when you want to undo an action.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom random import sample\nfrom string import ascii_letters\n\n\nclass Originator:\n    \"\"\"\n    The Originator holds some important state that may change over time. It also\n    defines a method for saving the state inside a memento and another method\n    for restoring the state from it.\n    \"\"\"\n\n    _state = None\n    \"\"\"\n    For the sake of simplicity, the originator's state is stored inside a single\n    variable.\n    \"\"\"\n\n    def __init__(self, state: str) -&gt; None:\n        self._state = state\n        print(f\"Originator: My initial state is: {self._state}\")\n\n    def do_something(self) -&gt; None:\n        \"\"\"\n        The Originator's business logic may affect its internal state.\n        Therefore, the client should backup the state before launching methods\n        of the business logic via the save() method.\n        \"\"\"\n\n        print(\"Originator: I'm doing something important.\")\n        self._state = self._generate_random_string(30)\n        print(f\"Originator: and my state has changed to: {self._state}\")\n\n    @staticmethod\n    def _generate_random_string(length: int = 10) -&gt; str:\n        return \"\".join(sample(ascii_letters, length))\n\n    def save(self) -&gt; Memento:\n        \"\"\"\n        Saves the current state inside a memento.\n        \"\"\"\n\n        return ConcreteMemento(self._state)    # KEY POINT\n\n    def restore(self, memento: Memento) -&gt; None:\n        \"\"\"\n        Restores the Originator's state from a memento object.\n        \"\"\"\n\n        self._state = memento.get_state()\n        print(f\"Originator: My state has changed to: {self._state}\")\n\n\nclass Memento(ABC):\n    \"\"\"\n    The Memento interface provides a way to retrieve the memento's metadata,\n    such as creation date or name. However, it doesn't expose the Originator's\n    state.\n    \"\"\"\n\n    @abstractmethod\n    def get_name(self) -&gt; str:\n        pass\n\n    @abstractmethod\n    def get_date(self) -&gt; str:\n        pass\n\n\nclass ConcreteMemento(Memento):\n    def __init__(self, state: str) -&gt; None:\n        self._state = state\n        self._date = str(datetime.now())[:19]\n\n    def get_state(self) -&gt; str:\n        \"\"\"\n        The Originator uses this method when restoring its state.\n        \"\"\"\n        return self._state\n\n    def get_name(self) -&gt; str:\n        \"\"\"\n        The rest of the methods are used by the Caretaker to display metadata.\n        \"\"\"\n\n        return f\"{self._date} / ({self._state[0:9]}...)\"\n\n    def get_date(self) -&gt; str:\n        return self._date\n\n\nclass Caretaker:\n    \"\"\"\n    The Caretaker doesn't depend on the Concrete Memento class. Therefore, it\n    doesn't have access to the originator's state, stored inside the memento. It\n    works with all mementos via the base Memento interface.\n    \"\"\"\n\n    def __init__(self, originator: Originator) -&gt; None:\n        self._mementos = []\n        self._originator = originator\n\n    def backup(self) -&gt; None:\n        print(\"\\nCaretaker: Saving Originator's state...\")\n        self._mementos.append(self._originator.save())\n\n    def undo(self) -&gt; None:\n        if not len(self._mementos):\n            return\n\n        memento = self._mementos.pop()\n        print(f\"Caretaker: Restoring state to: {memento.get_name()}\")\n        try:\n            self._originator.restore(memento)\n        except Exception:\n            self.undo()\n\n    def show_history(self) -&gt; None:\n        print(\"Caretaker: Here's the list of mementos:\")\n        for memento in self._mementos:\n            print(memento.get_name())\n\n\nif __name__ == \"__main__\":\n    originator = Originator(\"Super-duper-super-duper-super.\")\n    caretaker = Caretaker(originator)\n\n    caretaker.backup()\n    originator.do_something()\n\n    caretaker.backup()\n    originator.do_something()\n\n    caretaker.backup()\n    originator.do_something()\n\n    print()\n    caretaker.show_history()\n\n    print(\"\\nClient: Now, let's rollback!\\n\")\n    caretaker.undo()\n\n    print(\"\\nClient: Once more!\\n\")\n    caretaker.undo()\n\nOriginator: My initial state is: Super-duper-super-duper-super.\n\nCaretaker: Saving Originator's state...\nOriginator: I'm doing something important.\nOriginator: and my state has changed to: MkTDBwnJEdosCOpXrgbfYWeLAUGRza\n\nCaretaker: Saving Originator's state...\nOriginator: I'm doing something important.\nOriginator: and my state has changed to: lcGDuJvmHLdaCpkWKSiBIjneXAftxs\n\nCaretaker: Saving Originator's state...\nOriginator: I'm doing something important.\nOriginator: and my state has changed to: kRxDmusAWhjBJyngaINObrTKPzYwvE\n\nCaretaker: Here's the list of mementos:\n2024-03-19 16:25:42 / (Super-dup...)\n2024-03-19 16:25:42 / (MkTDBwnJE...)\n2024-03-19 16:25:42 / (lcGDuJvmH...)\n\nClient: Now, let's rollback!\n\nCaretaker: Restoring state to: 2024-03-19 16:25:42 / (lcGDuJvmH...)\nOriginator: My state has changed to: lcGDuJvmHLdaCpkWKSiBIjneXAftxs\n\nClient: Once more!\n\nCaretaker: Restoring state to: 2024-03-19 16:25:42 / (MkTDBwnJE...)\nOriginator: My state has changed to: MkTDBwnJEdosCOpXrgbfYWeLAUGRza",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#observer",
    "href": "tips/DesignPatterns/behavioral.html#observer",
    "title": "Behavioral",
    "section": "Observer",
    "text": "Observer\nIt enables subscription, so when the state of the subject changes, the observer is notified and can react.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom random import randrange\nfrom typing import List\n\n\nclass Subject(ABC):\n    \"\"\"\n    The Subject interface declares a set of methods for managing subscribers.\n    \"\"\"\n\n    @abstractmethod\n    def attach(self, observer: Observer) -&gt; None:\n        \"\"\"\n        Attach an observer to the subject.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def detach(self, observer: Observer) -&gt; None:\n        \"\"\"\n        Detach an observer from the subject.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def notify(self) -&gt; None:\n        \"\"\"\n        Notify all observers about an event.\n        \"\"\"\n        pass\n\n\nclass ConcreteSubject(Subject):\n    \"\"\"\n    The Subject owns some important state and notifies observers when the state\n    changes.\n    \"\"\"\n\n    _state: int = None\n    \"\"\"\n    For the sake of simplicity, the Subject's state, essential to all\n    subscribers, is stored in this variable.\n    \"\"\"\n\n    _observers: List[Observer] = []\n    \"\"\"\n    List of subscribers. In real life, the list of subscribers can be stored\n    more comprehensively (categorized by event type, etc.).\n    \"\"\"\n\n    def attach(self, observer: Observer) -&gt; None:\n        print(\"Subject: Attached an observer.\")\n        self._observers.append(observer)    # KEY POINT\n\n    def detach(self, observer: Observer) -&gt; None:\n        self._observers.remove(observer)\n\n    \"\"\"\n    The subscription management methods.\n    \"\"\"\n\n    def notify(self) -&gt; None:\n        \"\"\"\n        Trigger an update in each subscriber.\n        \"\"\"\n\n        print(\"Subject: Notifying observers...\")\n        for observer in self._observers:\n            observer.update(self)\n\n    def some_business_logic(self) -&gt; None:\n        \"\"\"\n        Usually, the subscription logic is only a fraction of what a Subject can\n        really do. Subjects commonly hold some important business logic, that\n        triggers a notification method whenever something important is about to\n        happen (or after it).\n        \"\"\"\n\n        print(\"\\nSubject: I'm doing something important.\")\n        self._state = randrange(0, 10)\n\n        print(f\"Subject: My state has just changed to: {self._state}\")\n        self.notify()\n\n\nclass Observer(ABC):\n    \"\"\"\n    The Observer interface declares the update method, used by subjects.\n    \"\"\"\n\n    @abstractmethod\n    def update(self, subject: Subject) -&gt; None:\n        \"\"\"\n        Receive update from subject.\n        \"\"\"\n        pass\n\n\n\"\"\"\nConcrete Observers react to the updates issued by the Subject they had been\nattached to.\n\"\"\"\n\n\nclass ConcreteObserverA(Observer):\n    def update(self, subject: Subject) -&gt; None:\n        if subject._state &lt; 3:\n            print(\"ConcreteObserverA: Reacted to the event\")\n\n\nclass ConcreteObserverB(Observer):\n    def update(self, subject: Subject) -&gt; None:\n        if subject._state == 0 or subject._state &gt;= 2:\n            print(\"ConcreteObserverB: Reacted to the event\")\n\n\nif __name__ == \"__main__\":\n    # The client code.\n\n    subject = ConcreteSubject()\n\n    observer_a = ConcreteObserverA()\n    subject.attach(observer_a)\n\n    observer_b = ConcreteObserverB()\n    subject.attach(observer_b)\n\n    subject.some_business_logic()\n    subject.some_business_logic()\n\n    subject.detach(observer_a)\n\n    subject.some_business_logic()\n\nSubject: Attached an observer.\nSubject: Attached an observer.\n\nSubject: I'm doing something important.\nSubject: My state has just changed to: 9\nSubject: Notifying observers...\nConcreteObserverB: Reacted to the event\n\nSubject: I'm doing something important.\nSubject: My state has just changed to: 1\nSubject: Notifying observers...\nConcreteObserverA: Reacted to the event\n\nSubject: I'm doing something important.\nSubject: My state has just changed to: 5\nSubject: Notifying observers...\nConcreteObserverB: Reacted to the event",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#state",
    "href": "tips/DesignPatterns/behavioral.html#state",
    "title": "Behavioral",
    "section": "State",
    "text": "State\nRecords states of a Context and transitions between the states. State knows about Context and vice versa.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\n\nclass Context:\n    \"\"\"\n    The Context defines the interface of interest to clients. It also maintains\n    a reference to an instance of a State subclass, which represents the current\n    state of the Context.\n    \"\"\"\n\n    _state = None\n    \"\"\"\n    A reference to the current state of the Context.\n    \"\"\"\n\n    def __init__(self, state: State) -&gt; None:\n        self.transition_to(state)\n\n    def transition_to(self, state: State):   # KEY POINT\n        \"\"\"\n        The Context allows changing the State object at runtime.\n        \"\"\"\n\n        print(f\"Context: Transition to {type(state).__name__}\")\n        self._state = state\n        self._state.context = self\n\n    \"\"\"\n    The Context delegates part of its behavior to the current State object.\n    \"\"\"\n\n    def request1(self):\n        self._state.handle1()\n\n    def request2(self):\n        self._state.handle2()\n\n\nclass State(ABC):\n    \"\"\"\n    The base State class declares methods that all Concrete State should\n    implement and also provides a backreference to the Context object,\n    associated with the State. This backreference can be used by States to\n    transition the Context to another State.\n    \"\"\"\n\n    @property\n    def context(self) -&gt; Context:\n        return self._context\n\n    @context.setter\n    def context(self, context: Context) -&gt; None:\n        self._context = context\n\n    @abstractmethod\n    def handle1(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def handle2(self) -&gt; None:\n        pass\n\n\n\"\"\"\nConcrete States implement various behaviors, associated with a state of the\nContext.\n\"\"\"\n\n\nclass ConcreteStateA(State):\n    def handle1(self) -&gt; None:\n        print(\"ConcreteStateA handles request1.\")\n        print(\"ConcreteStateA wants to change the state of the context.\")\n        self.context.transition_to(ConcreteStateB())\n\n    def handle2(self) -&gt; None:\n        print(\"ConcreteStateA handles request2.\")\n\n\nclass ConcreteStateB(State):\n    def handle1(self) -&gt; None:\n        print(\"ConcreteStateB handles request1.\")\n\n    def handle2(self) -&gt; None:\n        print(\"ConcreteStateB handles request2.\")\n        print(\"ConcreteStateB wants to change the state of the context.\")\n        self.context.transition_to(ConcreteStateA())\n\n\nif __name__ == \"__main__\":\n    # The client code.\n\n    context = Context(ConcreteStateA())\n    context.request1()\n    context.request2()\n\nContext: Transition to ConcreteStateA\nConcreteStateA handles request1.\nConcreteStateA wants to change the state of the context.\nContext: Transition to ConcreteStateB\nConcreteStateB handles request2.\nConcreteStateB wants to change the state of the context.\nContext: Transition to ConcreteStateA",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#strategy",
    "href": "tips/DesignPatterns/behavioral.html#strategy",
    "title": "Behavioral",
    "section": "Strategy",
    "text": "Strategy\nStrategy is a way to select an algorithm at runtime.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\n\nclass Context():\n    \"\"\"\n    The Context defines the interface of interest to clients.\n    \"\"\"\n\n    def __init__(self, strategy: Strategy) -&gt; None:    # KEY POINT\n        \"\"\"\n        Usually, the Context accepts a strategy through the constructor, but\n        also provides a setter to change it at runtime.\n        \"\"\"\n\n        self._strategy = strategy\n\n    @property\n    def strategy(self) -&gt; Strategy:\n        \"\"\"\n        The Context maintains a reference to one of the Strategy objects. The\n        Context does not know the concrete class of a strategy. It should work\n        with all strategies via the Strategy interface.\n        \"\"\"\n\n        return self._strategy\n\n    @strategy.setter\n    def strategy(self, strategy: Strategy) -&gt; None:\n        \"\"\"\n        Usually, the Context allows replacing a Strategy object at runtime.\n        \"\"\"\n\n        self._strategy = strategy\n\n    def do_some_business_logic(self) -&gt; None:\n        \"\"\"\n        The Context delegates some work to the Strategy object instead of\n        implementing multiple versions of the algorithm on its own.\n        \"\"\"\n\n        # ...\n\n        print(\"Context: Sorting data using the strategy (not sure how it'll do it)\")\n        result = self._strategy.do_algorithm([\"a\", \"b\", \"c\", \"d\", \"e\"])\n        print(\",\".join(result))\n\n        # ...\n\n\nclass Strategy(ABC):\n    \"\"\"\n    The Strategy interface declares operations common to all supported versions\n    of some algorithm.\n\n    The Context uses this interface to call the algorithm defined by Concrete\n    Strategies.\n    \"\"\"\n\n    @abstractmethod\n    def do_algorithm(self, data: List):\n        pass\n\n\n\"\"\"\nConcrete Strategies implement the algorithm while following the base Strategy\ninterface. The interface makes them interchangeable in the Context.\n\"\"\"\n\n\nclass ConcreteStrategyA(Strategy):\n    def do_algorithm(self, data: List) -&gt; List:\n        return sorted(data)\n\n\nclass ConcreteStrategyB(Strategy):\n    def do_algorithm(self, data: List) -&gt; List:\n        return reversed(sorted(data))\n\n\nif __name__ == \"__main__\":\n    # The client code picks a concrete strategy and passes it to the context.\n    # The client should be aware of the differences between strategies in order\n    # to make the right choice.\n\n    context = Context(ConcreteStrategyA())\n    print(\"Client: Strategy is set to normal sorting.\")\n    context.do_some_business_logic()\n    print()\n\n    print(\"Client: Strategy is set to reverse sorting.\")\n    context.strategy = ConcreteStrategyB()\n    context.do_some_business_logic()\n\nClient: Strategy is set to normal sorting.\nContext: Sorting data using the strategy (not sure how it'll do it)\na,b,c,d,e\n\nClient: Strategy is set to reverse sorting.\nContext: Sorting data using the strategy (not sure how it'll do it)\ne,d,c,b,a",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#template",
    "href": "tips/DesignPatterns/behavioral.html#template",
    "title": "Behavioral",
    "section": "Template",
    "text": "Template\nHave a template parent class, where children can override some methods.\n\n\nfrom abc import ABC, abstractmethod\n\n\nclass AbstractClass(ABC):\n    \"\"\"\n    The Abstract Class defines a template method that contains a skeleton of\n    some algorithm, composed of calls to (usually) abstract primitive\n    operations.\n\n    Concrete subclasses should implement these operations, but leave the\n    template method itself intact.\n    \"\"\"\n\n    def template_method(self) -&gt; None:    # KEY POINT\n        \"\"\"\n        The template method defines the skeleton of an algorithm.\n        \"\"\"\n\n        self.base_operation1()\n        self.required_operations1()\n        self.base_operation2()\n        self.hook1()\n        self.required_operations2()\n        self.base_operation3()\n        self.hook2()\n\n    # These operations already have implementations.\n\n    def base_operation1(self) -&gt; None:\n        print(\"AbstractClass says: I am doing the bulk of the work\")\n\n    def base_operation2(self) -&gt; None:\n        print(\"AbstractClass says: But I let subclasses override some operations\")\n\n    def base_operation3(self) -&gt; None:\n        print(\"AbstractClass says: But I am doing the bulk of the work anyway\")\n\n    # These operations have to be implemented in subclasses.\n\n    @abstractmethod\n    def required_operations1(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def required_operations2(self) -&gt; None:\n        pass\n\n    # These are \"hooks.\" Subclasses may override them, but it's not mandatory\n    # since the hooks already have default (but empty) implementation. Hooks\n    # provide additional extension points in some crucial places of the\n    # algorithm.\n\n    def hook1(self) -&gt; None:\n        pass\n\n    def hook2(self) -&gt; None:\n        pass\n\n\nclass ConcreteClass1(AbstractClass):\n    \"\"\"\n    Concrete classes have to implement all abstract operations of the base\n    class. They can also override some operations with a default implementation.\n    \"\"\"\n\n    def required_operations1(self) -&gt; None:\n        print(\"ConcreteClass1 says: Implemented Operation1\")\n\n    def required_operations2(self) -&gt; None:\n        print(\"ConcreteClass1 says: Implemented Operation2\")\n\n\nclass ConcreteClass2(AbstractClass):\n    \"\"\"\n    Usually, concrete classes override only a fraction of base class'\n    operations.\n    \"\"\"\n\n    def required_operations1(self) -&gt; None:\n        print(\"ConcreteClass2 says: Implemented Operation1\")\n\n    def required_operations2(self) -&gt; None:\n        print(\"ConcreteClass2 says: Implemented Operation2\")\n\n    def hook1(self) -&gt; None:\n        print(\"ConcreteClass2 says: Overridden Hook1\")\n\n\ndef client_code(abstract_class: AbstractClass) -&gt; None:\n    \"\"\"\n    The client code calls the template method to execute the algorithm. Client\n    code does not have to know the concrete class of an object it works with, as\n    long as it works with objects through the interface of their base class.\n    \"\"\"\n\n    # ...\n    abstract_class.template_method()\n    # ...\n\n\nif __name__ == \"__main__\":\n    print(\"Same client code can work with different subclasses:\")\n    client_code(ConcreteClass1())\n    print(\"\")\n\n    print(\"Same client code can work with different subclasses:\")\n    client_code(ConcreteClass2())\n\nSame client code can work with different subclasses:\nAbstractClass says: I am doing the bulk of the work\nConcreteClass1 says: Implemented Operation1\nAbstractClass says: But I let subclasses override some operations\nConcreteClass1 says: Implemented Operation2\nAbstractClass says: But I am doing the bulk of the work anyway\n\nSame client code can work with different subclasses:\nAbstractClass says: I am doing the bulk of the work\nConcreteClass2 says: Implemented Operation1\nAbstractClass says: But I let subclasses override some operations\nConcreteClass2 says: Overridden Hook1\nConcreteClass2 says: Implemented Operation2\nAbstractClass says: But I am doing the bulk of the work anyway",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/behavioral.html#visitor",
    "href": "tips/DesignPatterns/behavioral.html#visitor",
    "title": "Behavioral",
    "section": "Visitor",
    "text": "Visitor\nVisitor allows to add new operations to existing classes without modifying them.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\n\nclass Component(ABC):\n    \"\"\"\n    The Component interface declares an `accept` method that should take the\n    base visitor interface as an argument.\n    \"\"\"\n\n    @abstractmethod\n    def accept(self, visitor: Visitor) -&gt; None:\n        pass\n\n\nclass ConcreteComponentA(Component):\n    \"\"\"\n    Each Concrete Component must implement the `accept` method in such a way\n    that it calls the visitor's method corresponding to the component's class.\n    \"\"\"\n\n    def accept(self, visitor: Visitor) -&gt; None:\n        \"\"\"\n        Note that we're calling `visitConcreteComponentA`, which matches the\n        current class name. This way we let the visitor know the class of the\n        component it works with.\n        \"\"\"\n\n        visitor.visit_concrete_component_a(self)\n\n    def exclusive_method_of_concrete_component_a(self) -&gt; str:\n        \"\"\"\n        Concrete Components may have special methods that don't exist in their\n        base class or interface. The Visitor is still able to use these methods\n        since it's aware of the component's concrete class.\n        \"\"\"\n\n        return \"A\"\n\n\nclass ConcreteComponentB(Component):\n    \"\"\"\n    Same here: visitConcreteComponentB =&gt; ConcreteComponentB\n    \"\"\"\n\n    def accept(self, visitor: Visitor):\n        visitor.visit_concrete_component_b(self)\n\n    def special_method_of_concrete_component_b(self) -&gt; str:\n        return \"B\"\n\n\nclass Visitor(ABC):\n    \"\"\"\n    The Visitor Interface declares a set of visiting methods that correspond to\n    component classes. The signature of a visiting method allows the visitor to\n    identify the exact class of the component that it's dealing with.   # KEY POINT\n    \"\"\"\n\n    @abstractmethod\n    def visit_concrete_component_a(self, element: ConcreteComponentA) -&gt; None:\n        pass\n\n    @abstractmethod\n    def visit_concrete_component_b(self, element: ConcreteComponentB) -&gt; None:\n        pass\n\n\n\"\"\"\nConcrete Visitors implement several versions of the same algorithm, which can\nwork with all concrete component classes.\n\nYou can experience the biggest benefit of the Visitor pattern when using it with\na complex object structure, such as a Composite tree. In this case, it might be\nhelpful to store some intermediate state of the algorithm while executing\nvisitor's methods over various objects of the structure.\n\"\"\"\n\n\nclass ConcreteVisitor1(Visitor):\n    def visit_concrete_component_a(self, element) -&gt; None:\n        print(f\"{element.exclusive_method_of_concrete_component_a()} + ConcreteVisitor1\")\n\n    def visit_concrete_component_b(self, element) -&gt; None:\n        print(f\"{element.special_method_of_concrete_component_b()} + ConcreteVisitor1\")\n\n\nclass ConcreteVisitor2(Visitor):\n    def visit_concrete_component_a(self, element) -&gt; None:\n        print(f\"{element.exclusive_method_of_concrete_component_a()} + ConcreteVisitor2\")\n\n    def visit_concrete_component_b(self, element) -&gt; None:\n        print(f\"{element.special_method_of_concrete_component_b()} + ConcreteVisitor2\")\n\n\ndef client_code(components: List[Component], visitor: Visitor) -&gt; None:\n    \"\"\"\n    The client code can run visitor operations over any set of elements without\n    figuring out their concrete classes. The accept operation directs a call to\n    the appropriate operation in the visitor object.\n    \"\"\"\n\n    # ...\n    for component in components:\n        component.accept(visitor)\n    # ...\n\n\nif __name__ == \"__main__\":\n    components = [ConcreteComponentA(), ConcreteComponentB()]\n\n    print(\"The client code works with all visitors via the base Visitor interface:\")\n    visitor1 = ConcreteVisitor1()\n    client_code(components, visitor1)\n\n    print(\"It allows the same client code to work with different types of visitors:\")\n    visitor2 = ConcreteVisitor2()\n    client_code(components, visitor2)\n\nThe client code works with all visitors via the base Visitor interface:\nA + ConcreteVisitor1\nB + ConcreteVisitor1\nIt allows the same client code to work with different types of visitors:\nA + ConcreteVisitor2\nB + ConcreteVisitor2",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Behavioral"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Paris Olympics 2024 Medal standings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREST API stress testing\n\n\n\n\n\nFastAPI + Locust + Prometheus + Grafana = Killer combo\n\n\n\n\n\nMay 16, 2024\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nData extraction from PDFs\n\n\n\n\n\nI learned more about regex that I probably wanted to know\n\n\n\n\n\nMay 15, 2024\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nHexagonal-puzzle\n\n\n\n\n\nBacktracking algorithm at it’s best\n\n\n\n\n\nApr 19, 2024\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Chatbot and DALL-E using OpenAI’s GPT-4\n\n\n\n\n\nStreamlit hosted on EC2 with AWS Lambda backend.\n\n\n\n\n\nNov 24, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nNenad Bozinovic, Adrian Stoll, Naijing Guo\n\n\n\n\n\n\n\n\n\n\n\n\nImage Regression\n\n\n\n\n\nBiwi Kinect Head Pose dataset\n\n\n\n\n\nMar 15, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-label classification\n\n\n\n\n\nFrom Practical Deep Learning for Coders book by Jeremy Howard and Sylvain Gugger. PASCAL 2007 dataset.\n\n\n\n\n\nMar 13, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nPet Breed Classification\n\n\n\n\n\nMulti-class classification using FastAI\n\n\n\n\n\nMar 9, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nBear classifier\n\n\n\n\n\nUsing FastAI\n\n\n\n\n\nFeb 17, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting solar panels from satellite images\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning\n\n\n\n\n\nUsing pre-trained AlexNet\n\n\n\n\n\nJan 17, 2023\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nRock paper scissors\n\n\n\n\n\nMulticlass image classification\n\n\n\n\n\nNov 20, 2022\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nMulticlass image classification\n\n\n\n\n\nOn a simple image dataset\n\n\n\n\n\nNov 10, 2022\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nTorchvision, Conversions, Samplers\n\n\n\n\n\nOn conversions and transforms\n\n\n\n\n\nNov 3, 2022\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nBinary image classification\n\n\n\n\n\nOn a simple image dataset\n\n\n\n\n\nOct 27, 2022\n\n\nNenad Bozinovic\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Classification\n\n\n\n\n\nOn Scikit-Learn’s make_moons\n\n\n\n\n\nOct 20, 2022\n\n\nNenad Bozinovic\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects/2_torchvision_conversions_samplers.html",
    "href": "projects/2_torchvision_conversions_samplers.html",
    "title": "Torchvision, Conversions, Samplers",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing.\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom shared.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start &gt; 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets &gt; 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 6 + ((n_plot % 6) &gt; 0)\n    fig, axes = plt.subplots(n_rows, 6, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 6, i % 6    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\nCode\nimages, labels = generate_dataset(img_size=5, n_images=300, binary=True, seed=13)\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels.reshape(-1, 1)).float()  # reshaped this to (N,1) tensor\ntorch.manual_seed(13)\nN = len(x_tensor)\nn_train = int(.8*N)\nn_val = N - n_train\ntrain_subset, val_subset = random_split(x_tensor, [n_train, n_val])\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\n#|code-fold: true\n#|output: false\nlr = 0.1\n\n# Now we can create a model\nmodel_deeper = nn.Sequential()\nmodel_deeper.add_module('flatten', nn.Flatten())\nmodel_deeper.add_module('linear1', nn.Linear(25, 10, bias=True))\nmodel_deeper.add_module('relu', nn.ReLU())\nmodel_deeper.add_module('linear2', nn.Linear(10, 1, bias=True))\nmodel_deeper.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_deeper = optim.SGD(model_deeper.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\nsbs_deeper = StepByStep(model_deeper, optimizer_deeper, binary_loss_fn)",
    "crumbs": [
      "Projects",
      "Torchvision, Conversions, Samplers"
    ]
  },
  {
    "objectID": "projects/2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "href": "projects/2_torchvision_conversions_samplers.html#note-on-nchw-vs-nhwc-formats",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on NCHW vs NHWC formats",
    "text": "Note on NCHW vs NHWC formats\nThe format for images is different between libraries: - PyTorch uses NCHW - TensorFlow uses NHWC - PIL/Matplotlib images are HWC.\n\nimage_r  = np.zeros((5, 5), dtype=np.uint8)\nimage_r[:, 0] = 255\nimage_r[:, 1] = 128\n\nimage_g = np.zeros((5, 5), dtype=np.uint8)\nimage_g[:, 1] = 128\nimage_g[:, 2] = 255\nimage_g[:, 3] = 128\n\nimage_b = np.zeros((5, 5), dtype=np.uint8)\nimage_b[:, 3] = 128\nimage_b[:, 4] = 255\n\n\n\nCode\nshow_image(image_r, 'gray')\nzeros = np.zeros((5,5), dtype=np.uint8)\nstacked_red = np.zeros((5,5,3), dtype=np.uint8)  # the format is HxWxX (height X width X channel)\nstacked_red[:,:,0] = image_r  \n# also same thing can be achieved stacked_red = np.stack([image_r, zeros, zeros], axis=2)\nshow_image(stacked_red)\n\n\n\n\nCode\ndef image_channels(red, green, blue, rgb, gray, rows=(0, 1, 2)):\n    fig, axs = plt.subplots(len(rows), 4, figsize=(15, 5.5))\n\n    zeros = np.zeros((5, 5), dtype=np.uint8)\n\n    titles1 = ['Red', 'Green', 'Blue', 'Grayscale Image']\n    titles0 = ['image_r', 'image_g', 'image_b', 'image_gray']\n    titles2 = ['as first channel', 'as second channel', 'as third channel', 'RGB Image']\n\n    idx0 = np.argmax(np.array(rows) == 0)\n    idx1 = np.argmax(np.array(rows) == 1)\n    idx2 = np.argmax(np.array(rows) == 2)\n    \n    for i, m in enumerate([red, green, blue, gray]):\n        if 0 in rows:\n            axs[idx0, i].axis('off')\n            axs[idx0, i].invert_yaxis()\n            if (1 in rows) or (i &lt; 3):\n                axs[idx0, i].text(0.15, 0.25, str(m.astype(np.uint8)), verticalalignment='top')    \n                axs[idx0, i].set_title(titles0[i], fontsize=16)\n\n        if 1 in rows:\n            axs[idx1, i].set_title(titles1[i], fontsize=16)\n            axs[idx1, i].set_xlabel('5x5', fontsize=14)\n            axs[idx1, i].imshow(m, cmap=plt.cm.gray)\n\n        if 2 in rows:\n            axs[idx2, i].set_title(titles2[i], fontsize=16)\n            axs[idx2, i].set_xlabel(f'5x5x3 - {titles1[i][0]} only', fontsize=14)\n            if i &lt; 3:\n                stacked = [zeros] * 3\n                stacked[i] = m\n                axs[idx2, i].imshow(np.stack(stacked, axis=2))\n            else:\n                axs[idx2, i].imshow(rgb)\n\n        for r in [1, 2]:\n            if r in rows:\n                idx = idx1 if r == 1 else idx2\n                axs[idx, i].set_xticks([])\n                axs[idx, i].set_yticks([])\n                for k, v in axs[idx, i].spines.items():\n                    v.set_color('black')\n                    v.set_linewidth(.8)\n\n    if 1 in rows:\n        axs[idx1, 0].set_ylabel('Single\\nChannel\\n(grayscale)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx1, 3].set_xlabel('5x5 = 0.21R + 0.72G + 0.07B')\n    if 2 in rows:\n        axs[idx2, 0].set_ylabel('Three\\nChannels\\n(color)', rotation=0, labelpad=40, fontsize=12)\n        axs[idx2, 3].set_xlabel('5x5x3 = (R, G, B) stacked')\n    fig.tight_layout()\n    return fig\n\n\n\nimage_gray = .2126*image_r + .7152*image_g + .0722*image_b\nimage_rgb = np.stack([image_r, image_g, image_b], axis=2)\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 1))\n\n\n\n\n\n\n\n\n\nfig = image_channels(image_r, image_g, image_b, image_rgb, image_gray, rows=(0, 2))",
    "crumbs": [
      "Projects",
      "Torchvision, Conversions, Samplers"
    ]
  },
  {
    "objectID": "projects/2_torchvision_conversions_samplers.html#note-on-torchvision",
    "href": "projects/2_torchvision_conversions_samplers.html#note-on-torchvision",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Note on Torchvision",
    "text": "Note on Torchvision\ntorchvision has many existing Datasets, Models, and Transformations.\n\nConversion between ndarray, PIL, and tensor\n\nimages.shape\n\n(300, 1, 5, 5)\n\n\n\nexample_chw = images[3]\nexample_chw\n\narray([[[  0,   0, 255,   0,   0],\n        [  0, 255,   0,   0,   0],\n        [255,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0]]], dtype=uint8)\n\n\n\nexample_chw.shape\n\n(1, 5, 5)\n\n\n\nexample_hwc = np.transpose(example_chw, (1,2,0))\nexample_hwc.shape\n\n(5, 5, 1)\n\n\n\nshow_image(example_hwc, 'gray')\n\n\n\n\n\n\n\n\nToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\ntorch.as_tensor can work on N-dimensional arrays (ToTensor works only on 2D/3D images), but doesn’t apply scalling and, just fyi, it shares data i.e. doesn’t make a copy.\n\ntensorizer = ToTensor()\nexample_tensor = tensorizer(example_hwc)\nexample_tensor.shape\n\ntorch.Size([1, 5, 5])\n\n\n\nexample_tensor\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\n\n\n\ntype(example_tensor)\n\ntorch.Tensor\n\n\n\nexample_pil = ToPILImage()(example_tensor)  # similar to np.transpose(example_tensor, (1,2,0))\nprint(type(example_pil))\n\n&lt;class 'PIL.Image.Image'&gt;\n\n\n\nshow_image(example_pil, 'gray')\n\n\n\n\n\n\n\n\nTo convert between Tensor and Numpy:\n\nexample_np = example_tensor.detach().cpu().numpy()\nprint(type(example_np))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nTo convert between numpy and PIL:\n\nexample_np = np.array(example_pil)\nprint(type(example_np))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\nexample_pil_back = Image.fromarray(example_np)\nprint(type(example_pil_back))\n\n&lt;class 'PIL.Image.Image'&gt;\n\n\nmore options for different PIL modes: (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1).\nOne has to be careful: - PIL gray images (L mode) need to be squeezed from 3D to 2D via np.squeeze() - RGB mode images should be floats between [0,1] - L mode (grayscale) images should be np.uint8 from [0,255] - 1 mode (bool) images are True or False.\n\n# PIL_image = Image.fromarray(np.uint8(example_np)).convert('RGB')\n# PIL_image = Image.fromarray(example_np.astype('uint8'), 'RGB')\n\n\nshow_image(example_pil_back, 'gray')\n\n\n\n\n\n\n\n\n\n\nTransformations\nThere are many transformations that can be done on tensors and/or PIL images:\n\nflipper = RandomHorizontalFlip(p=1.0)\nshow_image(flipper(example_pil), 'gray')\n\n\n\n\n\n\n\n\n\nprint(example_tensor)\nnormalizer = Normalize(mean=.5, std=.5)\nnormalized_tensor = normalizer(example_tensor)\nprint(normalized_tensor)\n\ntensor([[[0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]])\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1.,  1., -1., -1., -1.],\n         [ 1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\n\nComposing transforms\n\ncomposer = Compose([RandomHorizontalFlip(p=1.0),\n                    Normalize(mean=(.5,), std=(.5,))])\ncomposer(example_tensor)\n\ntensor([[[-1., -1.,  1., -1., -1.],\n         [-1., -1., -1.,  1., -1.],\n         [-1., -1., -1., -1.,  1.],\n         [-1., -1., -1., -1., -1.],\n         [-1., -1., -1., -1., -1.]]])\n\n\n\ntorch.as_tensor(example_np/255).float()\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nIf we want to convert the whole numpy array to tensor we can use:\n\nexample_tensor = torch.as_tensor(example_np / 255).float()\nexample_tensor\n\ntensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nPyTorch default floating point dtype is torch.float32, one can change this if needed via torch.set_default_dtype(torch.float64).",
    "crumbs": [
      "Projects",
      "Torchvision, Conversions, Samplers"
    ]
  },
  {
    "objectID": "projects/2_torchvision_conversions_samplers.html#samplers",
    "href": "projects/2_torchvision_conversions_samplers.html#samplers",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Samplers",
    "text": "Samplers\nOne can also define a Sampler (and it’s subclasses: SequentialSampler, RandomSampler, SubsetRandomSampler, WeightedRandomSampler, BatchSampler, and DistributedSampler), use WeightedRandomSampler for example in case data is unbalanced.\n\ncomposer = Compose([RandomHorizontalFlip(p=0.5),\n                    Normalize(mean=.5, std=.5)])\n\ndataset = TransformedTensorDataset(x_tensor, y_tensor, transform=composer)\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=dataset, batch_size=16, sampler=val_sampler)\n\nNote that we need val_sampler just because we are passing the full dataset.\nAlso note that when using Samplers, one can’t use shuffle=True in DataLoaders.\n\nlen(iter(train_loader)), len(iter(val_loader))\n\n(15, 4)",
    "crumbs": [
      "Projects",
      "Torchvision, Conversions, Samplers"
    ]
  },
  {
    "objectID": "projects/2_torchvision_conversions_samplers.html#some-model-methods",
    "href": "projects/2_torchvision_conversions_samplers.html#some-model-methods",
    "title": "Torchvision, Conversions, Samplers",
    "section": "Some Model methods",
    "text": "Some Model methods\nTo see the layers:\n\nsbs_deeper.model\n\nSequential(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear1): Linear(in_features=25, out_features=10, bias=True)\n  (relu): ReLU()\n  (linear2): Linear(in_features=10, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)\n\n\n\nlist(sbs_deeper.model.named_modules())\n\n[('',\n  Sequential(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (linear1): Linear(in_features=25, out_features=10, bias=True)\n    (relu): ReLU()\n    (linear2): Linear(in_features=10, out_features=1, bias=True)\n    (sigmoid): Sigmoid()\n  )),\n ('flatten', Flatten(start_dim=1, end_dim=-1)),\n ('linear1', Linear(in_features=25, out_features=10, bias=True)),\n ('relu', ReLU()),\n ('linear2', Linear(in_features=10, out_features=1, bias=True)),\n ('sigmoid', Sigmoid())]\n\n\nTo see particular weights in a layer:\n\nsbs_deeper.model.linear2.weight.detach()\n\ntensor([[ 0.1024,  0.2038,  0.2085, -0.2298,  0.1973, -0.2550,  0.2233, -0.0626,\n         -0.0030,  0.0018]])\n\n\n\nsbs_deeper.count_parameters()\n\n271\n\n\nThe 271 comes from: (25 * 10 + 10 biases) + (10 * 1 + 1 bias) = 260 + 11 = 271\nto reset parameters I implemented this way but this doesn’t reset all parameters (for example in nn.PReLU) so use caution!\n\nsbs_deeper.reset_parameters()",
    "crumbs": [
      "Projects",
      "Torchvision, Conversions, Samplers"
    ]
  },
  {
    "objectID": "projects/14_multi_label_classification/multi_label_classification.html",
    "href": "projects/14_multi_label_classification/multi_label_classification.html",
    "title": "Multi-label classification",
    "section": "",
    "text": "Multi-label classification is a classification task where each sample can be assigned to multiple classes. This is different from multi-class classification where each sample can only be assigned to one class. Here I’ll follow FastAI’s book, chapeter 6.\nFirst let’s get the dataset:\n\nfrom fastai.vision.all import *\nfrom tips.plot import plot_pil_images\nfrom nbdev.showdoc import *\n\n\npath = untar_data(URLs.PASCAL_2007)\n\nLet’s take a look at the data, this dataset has a csv file with the info:\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nso we have filename, label (which is in fact multi-label) and is_valid (True if we want it in validation set, probably useful for bencharmarking since this info is not necceseary).\nThe dataset also has segmentation masks (for 422 out of 5011 images). No need for that now but good to know.\n\nDataBlock preparation\nDataBlock is fastai type that is used to create fastais Datasets and DataLoaders (which themselves are wrappers for PyTorch Dataset and DataLoader). Let’s get to know it:\n\ndata_block = DataBlock()\n\nOne can create datasets from pandas dataframe:\n\ndatasets = data_block.datasets(df)\n\n\ntype(datasets)\n\nfastai.data.core.Datasets\n\n\nDatasets have train and valid parameters:\n\ndatasets.train[0]\n\n(fname       009464.jpg\n labels             cat\n is_valid          True\n Name: 4754, dtype: object,\n fname       009464.jpg\n labels             cat\n is_valid          True\n Name: 4754, dtype: object)\n\n\nbut a text format is not useful for training. We can provide a get_x and get_y functions to DataBlock to get the data and labels:\n\ndef get_x(r):\n    return path / 'train' / r['fname']\ndef get_y(r):\n    return r['labels'].split()\ndatablock = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       get_x=get_x, get_y=get_y)\ndatasets = datablock.datasets(df)\n\n\ndatasets.train[0]\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n                      0., 0., 0., 0., 0.]))\n\n\nThis looks better, since now we have image and a MultiCategory label. It is one hot encoded since we can have more then 1. or 0 for that matter. Let’s also use the splitter that will use the is_valid column:\n\ndef splitter(df):\n    train = L(df.index[~df['is_valid']].tolist())\n    valid = L(df.index[df['is_valid']].tolist())\n    return train, valid\n\n\ndef get_x(r):\n    return path / 'train' / r['fname']\ndef get_y(r):\n    return r['labels'].split()\ndata_block = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       splitter=splitter,\n                       get_x=get_x, get_y=get_y)\ndatasets = data_block.datasets(df)\ndatasets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0.]))\n\n\nWe used splitter as a method, previously we used RandomSplitter()and dozen more (see doc below):\n\ndoc(RandomSplitter)\n\n\nRandomSplitter\nRandomSplitter(valid_pct=0.2, seed=None)Create function that splits `items` between train/val with `valid_pct` randomly.\nShow in docs\n\n\nWe can get vocabulary of all classes (there are 20):\n\nprint(datasets.vocab)\nprint(len(datasets.vocab))\n\n['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n20\n\n\nFor example, the 0th training sample looks like this:\n\ndatasets.train[0][0]\n\n\n\n\n\n\n\n\n\nidxs = datasets.train[0][1] == 1\nprint(idxs)\ndatasets.vocab[idxs]\n\nTensorMultiCategory([False, False, False, False, False, False,  True, False,\n                     False, False, False, False, False, False, False, False,\n                     False, False, False, False])\n\n\n(#1) ['car']\n\n\nso far so good. We can also use torch.where:\n\nidxs = torch.where(datasets.train[0][1] == 1.0)[0]\nprint(idxs)\ndatasets.vocab[idxs]\n\nTensorMultiCategory([6])\n\n\n(#1) ['car']\n\n\nWe need to resize all the images to the same size, PyTorch can’t store tensors otherwise, the min_scale\n\ndatablock = DataBlock(blocks=[ImageBlock, MultiCategoryBlock],\n                       get_x=get_x, get_y=get_y,\n                       splitter=splitter,\n                       item_tfms=RandomResizedCrop(128, min_scale=0.35))\ndatasets = datablock.datasets(df)\n\nfinally, let’s create fastai DataLoaders as well (won’t shuffle just yet):\n\ndls = datablock.dataloaders(df, shuffle=False)\n\n\ndls.show_batch(nrows=1, ncols=3)  # with min_scale = 0.35\n\n\n\n\n\n\n\n\nmin_scale controls the zoom, too low number (like 0.1) zooms and threatens to crop too much. Let’s stick with 0.35.\n\n\nTraining\nFirst we define the model with dataloaders and model (optimizer and loss will be assigned automatically for now):\n\nlearn = vision_learner(dls, resnet50)\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\nThe warnings are becuase vision_learner still uses 0.13 version of torchvision. Not a big deal for now.\n\n!pip show torchvision\n\nName: torchvision\nVersion: 0.13.0+cu116\nSummary: image and video datasets and models for torch deep learning\nHome-page: https://github.com/pytorch/vision\nAuthor: PyTorch Core Team\nAuthor-email: soumith@pytorch.org\nLicense: BSD\nLocation: /usr/local/lib/python3.9/dist-packages\nRequires: numpy, pillow, requests, torch, typing-extensions\nRequired-by: fastai, sentence-transformers\n\n\nI have to send the model to the device not sure why is this not automatic as with dataloaders:\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nlearn.model = learn.model.to(device)\n\nLet’s first see what we get as an output after predicitons:\n\nx,y = dls.train.one_batch()\nactivations = learn.model(x)\nactivations.shape\n\ntorch.Size([64, 20])\n\n\nThis makes sense, each item in a batch will predict a one-hot list of 20 classes. So waht loss type should we use? The answer is BinaryCrossEntropy, and that’s the one fastai assigns automatically:\n\nlearn.loss_func\n\nFlattenedLoss of BCEWithLogitsLoss()\n\n\nBCEWithLogitsLoss is used since there is last layer in resnet18 is not sigmoid but linear layer with 20 outputs:\n\nlearn.model[-1][-1]\n\nLinear(in_features=512, out_features=20, bias=False)\n\n\nLet’s also add a metric, accuracy_multi is the one we need and we should pass sigmoid=True since we want to convert logits to probabilities and let’s have lower threshold:\n\naccuracy_multi??\n\nSignature: accuracy_multi(inp, targ, thresh=0.5, sigmoid=True)\nSource:   \ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    inp,targ = flatten_check(inp,targ)\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp&gt;thresh)==targ.bool()).float().mean()\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/metrics.py\nType:      function\n\n\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\n\n\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=3)\n\nVSCode doesn’t show the progress bar sometimes unfortunately.\nLet’s get the predictions:\n\npreds, targets = learn.get_preds()\n\n\n\n\n\n\n    \n      \n      0.00% [0/40 00:00&lt;?]\n    \n    \n\n\n\npreds.shape\n\ntorch.Size([2510, 20])\n\n\n\ntargets.shape\n\ntorch.Size([2510, 20])\n\n\nwe can now plot accuracy for differnt thresholds (we use sigmoid=False since we already got probabilities via get_preds):\n\nxs = torch.linspace(0.05, 0.95, 30)\naccs = [accuracy_multi(preds, targets, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs, accs)\n\n\n\n\n\n\n\n\nSo the max accuracy is:\n\nmax_idx, max_val = max(enumerate([float(x) for x in accs]), key=lambda x: x[1])\nxs[max_idx], max_val\n\n(tensor(0.4845), 0.9633266925811768)\n\n\nfor threshold=0.48.\nAnd this is it for multi-label classification.",
    "crumbs": [
      "Projects",
      "Multi-label classification"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html",
    "href": "projects/spacenet8/spacenet8.html",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "",
    "text": "This project explores the SpaceNet8 Challenge, which aims to detect floods caused by hurricanes and heavy rains. We compared a variety of transformer and CNN segmentation architectures. We found that large pre-trained Segformer models had better performance than Resnet, DenseNet and EfficientNet based models despite consuming more computational resources. The highest IoU for building detection was 61% for Segformer, which indicated that attention is better suited for detecting building-footprints than convolutions. We noticed that flooded road detection was particularly hard with highest IoU of 40%. We observed pre-training on ImageNet and Cityscapes datasets provided a moderate improvement compared to pre-training on the ADE20k dataset and a significant improvement compared to training a model from scratch.\nEqual contributions from: Adrian Stoll (@AdrS), Naijing Guo (@NaijingGuo), and Nenad Bozinovic (@nesaboz). This project was done for CS231n: Deep Learning for Computer Vision at Stanford University.\nReport and poster.\n\n  \n\nAcknowledgement: SpaceNet8 repo.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#abstract",
    "href": "projects/spacenet8/spacenet8.html#abstract",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "",
    "text": "This project explores the SpaceNet8 Challenge, which aims to detect floods caused by hurricanes and heavy rains. We compared a variety of transformer and CNN segmentation architectures. We found that large pre-trained Segformer models had better performance than Resnet, DenseNet and EfficientNet based models despite consuming more computational resources. The highest IoU for building detection was 61% for Segformer, which indicated that attention is better suited for detecting building-footprints than convolutions. We noticed that flooded road detection was particularly hard with highest IoU of 40%. We observed pre-training on ImageNet and Cityscapes datasets provided a moderate improvement compared to pre-training on the ADE20k dataset and a significant improvement compared to training a model from scratch.\nEqual contributions from: Adrian Stoll (@AdrS), Naijing Guo (@NaijingGuo), and Nenad Bozinovic (@nesaboz). This project was done for CS231n: Deep Learning for Computer Vision at Stanford University.\nReport and poster.\n\n  \n\nAcknowledgement: SpaceNet8 repo.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#dataset",
    "href": "projects/spacenet8/spacenet8.html#dataset",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "Dataset",
    "text": "Dataset\n\n\n\n\n\nExamples of SpaceNet8 raw images pre- and post-event (top row) and respective ground truth segmentation masks (bottom row)[1]. Colors indicate classes (buildings, road, flooded buildings and flooded roads). In this example blue and yellow colors refer to flooded buildings and roads. For pre-event labels there is 1 building class and 8 road classes denoting speed (10 mph per class). For post event there are 4 classes (non-flooded building, flooded building, flooded road, non-flooded road). Dataset was imbalanced and favored non-flooded areas (~85% of labels). We performed a split of 679 training images and 122 (15%) validation ones (used for all experiments).",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#model-architecture",
    "href": "projects/spacenet8/spacenet8.html#model-architecture",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "Model Architecture",
    "text": "Model Architecture\n\n\n\n\n\nPipeline provided by SpaceNet8 containing foundation features network and flood attribution networks. The pipeline design is modular and allows different backbone models to be swapped in while maintaining the same data-loading, training, and evaluation code.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#experiments-and-results",
    "href": "projects/spacenet8/spacenet8.html#experiments-and-results",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "Experiments and Results",
    "text": "Experiments and Results\nWe compared several backbones for Intersection over Union (IoU) metric. Each model was trained on the entire training set for typically 10 epochs (45 min on A6000) after which we would typically see validation loss convergence and overfitting on the training dataset. Each training run used the Adam optimizer with a learning rate of 0.0001. Loss functions were binary-cross-entropy (building detection), combined dice loss and focal loss (roads) and cross-entropy (flooded vs non flooded buildings and roads).\nResnet34/50 is a baseline model. Segformer uses a hierarchical transformer encoder combining multiscale features using MLP. Unlike ViT, Segformer does not use positional encoding which aids transfer learning between datasets with different image resolutions [2]. DenseNet’s main feature is that it connects each layer to every other layer in a feed-forward fashion for a total of L(L+1)/2 connections, comparing to traditional CNN’s L layers [3]. EfficientNet is a CNN that uses so-called compound scaling with optimal layer width, depth, and resolution [4].\n\n\n   \n\n\nOverall Segformer performs better than CNN models on all metrics, showing that attention is better suited for detecting building and road-like polygonal objects.\nPre-training with ImageNet and Cityscapes datasets yields an improvement over ADE20k, likely due to the fact that ADE20k images contain mostly indoor images of objects, unlike Spacenet8 satellite imagery. This is also likely the reason why ADE20k pretrained model did not yield any improvement on flooded objects, that are already less represented due to the imbalances dataset.\nWe also did not see improvement by using larger CNN models (resnet 50, densenet161, efficientnet_b4) indicating that these models are overfitting on a relatively small SpaceNet8 dataset.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#resource-analysis",
    "href": "projects/spacenet8/spacenet8.html#resource-analysis",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "Resource analysis",
    "text": "Resource analysis\nSegformer b0 consumes more memory and has longer epoch durations that Resnet34 despite having fewer parameters. (Attention has quadratic time and space complexity in comparison to linear complexity for CNN models)\nThe y-intercept of the memory vs batch-size best fit line is the memory from model weights, their gradients, and overhead. The slope represents the memory for training examples, activations, and gradients of activations.\nWe noticed loading data from shared network drive increased epoch time 6-fold (~4 minutes) in comparison to loading data locally from hard drive.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#conclusion",
    "href": "projects/spacenet8/spacenet8.html#conclusion",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "Conclusion",
    "text": "Conclusion\nTransformer models have better overall performance than CNN based models, although they are more resource demanding.\nPre-training makes a positive contribution to the performance, and models pre-trained on different dataset could have slightly different performance in specific tasks.\nFuture Work includes exploring other model architectures, eg. Segment Anything, add image augmentation, pre-train model on additional dataset, and add weights to the CrossEntropy loss function.",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/spacenet8/spacenet8.html#references",
    "href": "projects/spacenet8/spacenet8.html#references",
    "title": "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge",
    "section": "References",
    "text": "References\n[1] Ronny Hansch et al, Spacenet 8 - the detection of flooded roads and buildings. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1471–1479, 2022 (link)\n[2] Enze Xie et al, Segformer: Simple and efficient design for semantic segmentation with transformers, 2021. (link)\n[3] Gao Huang et al. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2261–2269, 2017. 3, 4 [7] (link)\n[4] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning. PMLR, 2019. (link)",
    "crumbs": [
      "Projects",
      "Comparing Transformers and CNNs on the SpaceNet8 Flood Detection Challenge"
    ]
  },
  {
    "objectID": "projects/binary_classification.html",
    "href": "projects/binary_classification.html",
    "title": "Binary Classification",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom shared.step_by_step import StepByStep, RUNS_FOLDER_NAME\nimport platform\n\nimport datetime\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nWe’ll use Scikit-Learn’s make_moons to generate a toy dataset with 1000 data points and two features.",
    "crumbs": [
      "Projects",
      "Binary Classification"
    ]
  },
  {
    "objectID": "projects/binary_classification.html#data-generation",
    "href": "projects/binary_classification.html#data-generation",
    "title": "Binary Classification",
    "section": "Data Generation",
    "text": "Data Generation\n\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=11)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)\n\nWe first use Scikit-Learn’s StandardScaler to standardize datasets:\n\nsc = StandardScaler()\nsc.fit(X_train)  # always fit only on X_train\nX_train_scalled = sc.transform(X_train)\nX_val_scalled = sc.transform(X_val)  # DO NOT use fit or fit_transform on X_val, it causes data leak\nm = sc.mean_\nv = sc.var_\nprint(m, v)\nassert ((X_train_scalled[0] - m)/np.sqrt(v) - X_train[0] &lt; np.finfo(float).eps).all()\n\n[0.4866699  0.26184213] [0.80645937 0.32738853]\n\n\n\nX_train = X_train_scalled\nX_val = X_val_scalled\n\n\nfrom matplotlib.colors import ListedColormap\n\ndef figure1(X_train, y_train, X_val, y_val, cm_bright=None):\n    if cm_bright is None:\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)#, edgecolors='k')\n    ax[0].set_xlabel(r'$X_1$')\n    ax[0].set_ylabel(r'$X_2$')\n    ax[0].set_xlim([-2.3, 2.3])\n    ax[0].set_ylim([-2.3, 2.3])\n    ax[0].set_title('Generated Data - Train')\n\n    ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright)#, edgecolors='k')\n    ax[1].set_xlabel(r'$X_1$')\n    ax[1].set_ylabel(r'$X_2$')\n    ax[1].set_xlim([-2.3, 2.3])\n    ax[1].set_ylim([-2.3, 2.3])\n    ax[1].set_title('Generated Data - Validation')\n    fig.tight_layout()\n    \n    return fig\n\n\nfig = figure1(X_train, y_train, X_val, y_val)",
    "crumbs": [
      "Projects",
      "Binary Classification"
    ]
  },
  {
    "objectID": "projects/binary_classification.html#data-preparation",
    "href": "projects/binary_classification.html#data-preparation",
    "title": "Binary Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe preparation of data starts by converting the data points from Numpy arrays to PyTorch tensors and sending them to the available device:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Builds tensors from numpy arrays\nx_train_tensor = torch.as_tensor(X_train).float()\ny_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()  # reshape makes shape from (80,) to (80,1)\n\nx_val_tensor = torch.as_tensor(X_val).float()\ny_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n\n\ntrain_data = TensorDataset(x_train_tensor, y_train_tensor)\nval_data = TensorDataset(x_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_data, 64, shuffle=True)\nval_loader = DataLoader(val_data, 64)",
    "crumbs": [
      "Projects",
      "Binary Classification"
    ]
  },
  {
    "objectID": "projects/binary_classification.html#linear-model",
    "href": "projects/binary_classification.html#linear-model",
    "title": "Binary Classification",
    "section": "Linear model",
    "text": "Linear model\n\ntorch.manual_seed(42)\n\nlr = 0.01\n\nmodel = nn.Sequential()\nmodel.add_module('linear', nn.Linear(2,1))\nmodel.add_module('sigmoid', nn.Sigmoid())\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\nloss_fn = nn.BCELoss()\n\n\nsbs_lin = StepByStep(model, optimizer, loss_fn)\nsbs_lin.set_loaders(train_loader, val_loader)\nsbs_lin.train(100)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02&lt;00:00, 42.69it/s]\n\n\n\n_ = sbs_lin.plot_losses()\n\n\n\n\n\n\n\n\nLet’s predict the values for X_train (y_train_predicted) and X_val (y_val_predicted) and plot them. Also let’s see how good of a job linear regression did using confusion matrix:\n\ndef predict_plot_count(sbs):\n    y_train_predicted = sbs.predict(X_train)\n    y_val_predicted = sbs.predict(X_val)\n    fig = figure1(X_train, y_train_predicted, X_val, y_val_predicted)\n    print('Confusion matrix:')\n    print(confusion_matrix(y_val, list(map(int, (y_val_predicted &gt; 0.5).ravel()))))\n    print('Correct categories:')\n    print(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\npredict_plot_count(sbs_lin)\n\nConfusion matrix:\n[[82 14]\n [14 90]]\nCorrect categories:\ntensor([[ 82,  96],\n        [ 90, 104]])\n\n\n\n\n\n\n\n\n\nand we see there are some false positives and false negatives (off-diagonal elements).",
    "crumbs": [
      "Projects",
      "Binary Classification"
    ]
  },
  {
    "objectID": "projects/binary_classification.html#two-layer-model",
    "href": "projects/binary_classification.html#two-layer-model",
    "title": "Binary Classification",
    "section": "Two-layer model",
    "text": "Two-layer model\nLet’s make a better model.\n\nmodel_nonlin = nn.Sequential()\nmodel_nonlin.add_module('linear1', nn.Linear(2,10))\nmodel_nonlin.add_module('relu', nn.ReLU())\nmodel_nonlin.add_module('linear2', nn.Linear(10,1))\nmodel_nonlin.add_module('sigmoid', nn.Sigmoid())\noptimizer = optim.Adam(model_nonlin.parameters(), lr=lr)\n\nsbs_nonlin = StepByStep(model_nonlin, optimizer, loss_fn)\nsbs_nonlin.set_loaders(train_loader, val_loader)\nsbs_nonlin.train(100)\n_ = sbs_nonlin.plot_losses()\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01&lt;00:00, 50.74it/s]\n\n\n\n\n\n\n\n\n\n\npredict_plot_count(sbs_nonlin)\n\nConfusion matrix:\n[[86 10]\n [10 94]]\nCorrect categories:\ntensor([[ 86,  96],\n        [ 94, 104]])\n\n\n\n\n\n\n\n\n\nAnd this is better (we could calculate precision and recall).",
    "crumbs": [
      "Projects",
      "Binary Classification"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#abstract",
    "href": "projects/solar/solar.html#abstract",
    "title": "Detecting solar panels from satellite images",
    "section": "Abstract",
    "text": "Abstract\nThis study leverages deep learning for segmenting solar panels in satellite imagery to enable tracking of installation process on large solar farms. A custom segmentation ResNet-based model was developed in PyTorch, to achieved 71% Jaccard index (aka IoU) on a validation dataset. ColorJitter was shown to be a good augmentation to combat influence of clouds and shadows in the images. GitHub repo.",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#data",
    "href": "projects/solar/solar.html#data",
    "title": "Detecting solar panels from satellite images",
    "section": "Data",
    "text": "Data\nRaw images of solar farms were provided using Airbus Defence and Space satellites, followed by manual labeling using LabelMe software. The large (~1GB) satellite images were split into smaller ones (256x256 pixels size), and total of 449 labels of 3 classes: racks, common panels, and dense panels were obtained.",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#augmentation",
    "href": "projects/solar/solar.html#augmentation",
    "title": "Detecting solar panels from satellite images",
    "section": "Augmentation",
    "text": "Augmentation\nIt was insightful to plot mean and standard deviation of labeled and unlabeled images to identified range for ColorJitter as augmentation: \nLabeled and unlabeled data have decent overlap, with non-covered datapoints typically being clouds (high mean, low stdev) or with partial out-of-bounds, i.e. black, zones (low mean). See main_experimentation.py for examples.",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#training",
    "href": "projects/solar/solar.html#training",
    "title": "Detecting solar panels from satellite images",
    "section": "Training",
    "text": "Training\nI used Adam optimizer and weighed CrossEntropyLoss. Segmentation was based on UNet-like architecture and this paper (see models.py):\n\n\n\nlosses.png\n\n\nTraining for 120 epoch took about 6 minutes (RTX5000). Achieved 71% Jaccard index (aka IoU) on validation dataset.",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#inference",
    "href": "projects/solar/solar.html#inference",
    "title": "Detecting solar panels from satellite images",
    "section": "Inference",
    "text": "Inference\nFinally, I evaluated and stitched together predicted images back into large satellite images (size ~1Gb, showing here only smaller section):\n\n\n\nimg.png\n\n\n(Note: some missing patches in the image were used for training).\nShadows and clouds are probably the biggest obstacle for precise counting. While augmentations can help to some extent, ideally multiple images of the same solar farm should be obtained and combined for thorough coverage.\nThe following are some notable examples.\nHandling multiple classes:\n   \nHandling different backgrounds:\n   \nAnd here are the common failures, mostly shades and clouds:",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/solar/solar.html#conclusion",
    "href": "projects/solar/solar.html#conclusion",
    "title": "Detecting solar panels from satellite images",
    "section": "Conclusion",
    "text": "Conclusion\nObject segmentation has shown to perform well for solar panel detection. Augmentation using brightness and contrast improved detection significantly.\nNumber of solar panels is calculated knowing pixel resolution is 50cm/pixel and panels width of about 100 cm (i.e. 2 pixels).\nSmall edge improvements can be done by allowing for some overlap when cropping large images and combining masks subsequently.\nReferences: Boiler-plate code based on a book “PyTorch: Step by Step” by Daniel Voigt Godoy.",
    "crumbs": [
      "Projects",
      "Detecting solar panels from satellite images"
    ]
  },
  {
    "objectID": "projects/multiclass_image_classification.html",
    "href": "projects/multiclass_image_classification.html",
    "title": "Multiclass image classification",
    "section": "",
    "text": "Here we’ll modify binary image classification from previous example to multiclass image classification by detecting left diagonal and right diagonal separately.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom shared.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start &gt; 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets &gt; 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30, per_row=10):\n    n_rows = n_plot // per_row + ((n_plot % per_row) &gt; 0)\n    fig, axes = plt.subplots(n_rows, per_row, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // per_row, i % per_row    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 8})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\n\n\n\n\nData preparation\nWe prepare data similary as in the previous exercize thought notable difference is that y_tensor shape is (N), not (N,1) as previously. This is due to loss function (CrossEntropyLoss) requiremnts (it takes class indices).\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels).long()\n\n\nx_tensor.shape\n\ntorch.Size([1000, 1, 10, 10])\n\n\n\ny_tensor.shape\n\ntorch.Size([1000])\n\n\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\n\ntorch.manual_seed(42)\nN = len(x_tensor)\nn_train = int(.8*N)\ntrain_subset, val_subset = random_split(x_tensor, [n_train, N - n_train])\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\nWe do not apply augmentation since it would mess up the labels:\n\ntrain_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\ny_val_tensor\n\ntensor([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1,\n        2, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 0, 0, 0, 2,\n        0, 2, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0,\n        2, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2,\n        2, 2, 0, 2, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1, 0, 0, 2, 0, 2, 2, 2, 1, 1,\n        1, 1, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1,\n        0, 1, 0, 2, 0, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 2,\n        2, 1, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0,\n        0, 0, 0, 2, 2, 2, 2, 2])\n\n\n\n\nDeep model\nA typical architecture uses a sequence of one or more typical convolutional blocks, with each block consisting of three operations:\n\nConvolution\nActivation function\nPooling\n\nAnd for multiclass problems we need to use appropriate loss functions depending if we have Sigmoid/LogSoftmax as the last layer:\n\nLet’s build a model:\n\ntorch.manual_seed(13)\nmodel_cnn1 = nn.Sequential()\n\n# Featurizer\n# Block 1: 1@10x10 -&gt; n_channels@8x8 -&gt; n_channels@4x4\nn_channels = 1\nmodel_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\nmodel_cnn1.add_module('relu1', nn.ReLU())\nmodel_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n# Flattening: n_channels * 4 * 4\nmodel_cnn1.add_module('flatten', nn.Flatten())\n\n# Classification\n# Hidden Layer\nmodel_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\nmodel_cnn1.add_module('relu2', nn.ReLU())\n# Output Layer\nmodel_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n\nlr = 0.1\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)\n\nLet’s just run one batch to get the sense for loss:\n\nx, y = next(iter(train_loader))\ny_pred = model_cnn1(x)\nprint(y.shape)\nprint(y_pred.shape)\nnn.CrossEntropyLoss()(y_pred,y)\n\ntorch.Size([16])\ntorch.Size([16, 3])\n\n\ntensor(0.2768, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nOne important observation: y shape is 16, while y_pred shape is 16x3. CrossEntropyLoss expects this (see docs).\n\nsbs = StepByStep(model_cnn1, optimizer_cnn1, multi_loss_fn)\n# sbs.set_seed()\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(20)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02&lt;00:00,  7.16it/s]\n\n\n\nfig = sbs.plot_losses()\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs.loader_apply(sbs.val_loader, sbs.correct))\n\n\nCorrect categories:\ntensor([[60, 71],\n        [46, 56],\n        [73, 73]])\n\n\n\n\nCode\nprint(f'Accuracy: {sbs.accuracy}%')\n\n\nAccuracy: 89.5%\n\n\nThis is not the greatest accuracy for label 0 (parallel) and label 1 (counter-diagonal). Once can probably find a better model (TODO) but for now let’s inspect what failed.\n\n\nVisualize error outputs\n\n# will predict all points at once here, no batches:\nlogits = sbs.predict(val_loader.dataset.x)\npredicted = np.argmax(logits, 1)\n\n\nlogits.shape\n\n(200, 3)\n\n\n\nval_loader.dataset.x.shape\n\ntorch.Size([200, 1, 10, 10])\n\n\n\nlogits.shape\n\n(200, 3)\n\n\n\npredicted\n\narray([2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n       0, 1, 2, 1, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1,\n       0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0,\n       1, 2, 1, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 0, 2, 1,\n       2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1,\n       1, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2,\n       0, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 2, 0,\n       2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1,\n       0, 2, 2, 1, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2,\n       2, 2])\n\n\n\nnot_equal = torch.ne(val_loader.dataset.y, torch.as_tensor(predicted))\nimages_tensor = val_loader.dataset.x[not_equal]\nactual_labels_tensor = val_loader.dataset.y[not_equal]\npred_labels_tensor = predicted[not_equal]\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nstart_idx = 0\nbatch_size = 10\nimages_batch = images_tensor[start_idx:start_idx+batch_size]\nlabels_batch = actual_labels_tensor[start_idx:start_idx+batch_size]\n\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we see that lots of predictions are for class label 2. For images 2,3,7,8, filters failed to register anything.\n\n\nOrdinary batch visualization:\n\nfig_filters = sbs.visualize_filters('conv1', cmap='gray')\n\n\n\n\n\n\n\n\n\nfeaturizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\nclassifier_layers = ['fc1', 'relu2', 'fc2']\n\nsbs.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n\nimages_batch, labels_batch = next(iter(val_loader))\nlogits = sbs.predict(images_batch)\npredicted = np.argmax(logits, 1)\nsbs.remove_hooks()\n\n\nwith plt.style.context('seaborn-white'):\n    fig_maps1 = sbs.visualize_outputs(featurizer_layers)\n    fig_maps2 = sbs.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)",
    "crumbs": [
      "Projects",
      "Multiclass image classification"
    ]
  },
  {
    "objectID": "projects/rock_paper_scissors.html",
    "href": "projects/rock_paper_scissors.html",
    "title": "Rock paper scissors",
    "section": "",
    "text": "We’ll use Rock, Paper, Scissors dataset created by Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com) and can be found in his site: Rock Paper Scissors Dataset.\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom shared.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize, CenterCrop\n\nfrom tqdm.autonotebook import tqdm\n\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n\nfrom torchvision.datasets import ImageFolder\nfrom shared.rps import download_rps\n\nplt.style.use('fivethirtyeight')\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()",
    "crumbs": [
      "Projects",
      "Rock paper scissors"
    ]
  },
  {
    "objectID": "projects/rock_paper_scissors.html#temporary-dataset",
    "href": "projects/rock_paper_scissors.html#temporary-dataset",
    "title": "Rock paper scissors",
    "section": "Temporary dataset",
    "text": "Temporary dataset\nWe need to calculate normalization parameters (mean and std) for all training images first. This is important step as we will use these normalization parameters not only for training images but for all the validation, and any future, predictions. Since we need to only calculate normalization parameters, we can also scale images to smaller size, just so the calculation is faster.\n\ncomposer = Compose([Resize(28),\n                    ToTensor()])\ntemp_dataset = ImageFolder(root='rps', transform=composer)\ntemp_loader = DataLoader(temp_dataset, batch_size=32)\nnormalizer = StepByStep.make_normalizer(temp_loader)\nnormalizer",
    "crumbs": [
      "Projects",
      "Rock paper scissors"
    ]
  },
  {
    "objectID": "projects/rock_paper_scissors.html#real-dataset",
    "href": "projects/rock_paper_scissors.html#real-dataset",
    "title": "Rock paper scissors",
    "section": "Real dataset",
    "text": "Real dataset\n\ncomposer = Compose([Resize(28), ToTensor(), normalizer])\ntrain_dataset = ImageFolder(root='rps', transform=composer)\nval_dataset = ImageFolder(root='rps-test-set', transform=composer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n\nimages, labels = next(iter(train_loader))\n\n\ntrain_loader\n\n&lt;torch.utils.data.dataloader.DataLoader&gt;\n\n\n\nlabels\n\ntensor([2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1])",
    "crumbs": [
      "Projects",
      "Rock paper scissors"
    ]
  },
  {
    "objectID": "projects/openai/openai.html",
    "href": "projects/openai/openai.html",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "",
    "text": "Using gpt-4-1106-preview APIs I created some role-playing chatbots. Frontend: Streamlit hosted on EC2, backend: AWS Lambda custom EC2 instance. CI/CD by GitHub Actions.",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#import-modules-and-packages",
    "href": "projects/openai/openai.html#import-modules-and-packages",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Import Modules and Packages",
    "text": "Import Modules and Packages\n\nfrom openai import OpenAI\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nfrom pprint import pprint\nimport tiktoken\nfrom pypdf import PdfReader\nfrom IPython.display import Markdown, display, Image\nimport os\nfrom matplotlib import image as mpimg\nfrom matplotlib import pyplot as plt",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#set-the-api-key",
    "href": "projects/openai/openai.html#set-the-api-key",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Set the API Key",
    "text": "Set the API Key\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=api_key)\n\n\npd.set_option('display.max_colwidth', None)\n\ndef pp(df):\n    return display( df.style.set_properties(subset=['emails'], **{'text-align': 'left', 'white-space': 'pre-wrap', 'width': '900px'}) )",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#generate-emails-for-reviews",
    "href": "projects/openai/openai.html#generate-emails-for-reviews",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Generate Emails for Reviews",
    "text": "Generate Emails for Reviews\n\ncolumns = ['reviews', 'emails']\ndf = pd.DataFrame(columns=columns)\ndf['reviews'] = [\n    \"Nice socks, great colors, just enough support for wearing with a good pair of sneakers.\",\n    \"Love Deborah Harness's Trilogy! Didn't want the story to end and hope they turn this trilogy into a movie. I would love it if she wrote more books to continue this story!!!\",\n    \"SO much quieter than other compressors. VERY quick as well. You will not regret this purchase.\",\n    \"Shirt a bit too long, with heavy hem, which inhibits turning over. I cut off the bottom two inches all around, and am now somewhat comfortable. Overall, material is a bit too heavy for my liking.\",\n    \"The quality on these speakers is insanely good and doesn't sound muddy when adjusting bass. Very happy with these.\",\n    \"Beautiful watch face. The band looks nice all around. The links do make that squeaky cheapo noise when you swing it back and forth on your wrist which can be embarrassing in front of watch enthusiasts. However, to the naked eye from afar, you can't tell the links are cheap or folded because it is well polished and brushed and the folds are pretty tight for the most part. love the new member of my collection and it looks great. I've had it for about a week and so far it has kept good time despite day 1 which is typical of a new mechanical watch.\"\n]\ndf.head()\n\n\n\n\n\n\n\n\nreviews\nemails\n\n\n\n\n0\nNice socks, great colors, just enough support for wearing with a good pair of sneakers.\nNaN\n\n\n1\nLove Deborah Harness's Trilogy! Didn't want the story to end and hope they turn this trilogy into a movie. I would love it if she wrote more books to continue this story!!!\nNaN\n\n\n2\nSO much quieter than other compressors. VERY quick as well. You will not regret this purchase.\nNaN\n\n\n3\nShirt a bit too long, with heavy hem, which inhibits turning over. I cut off the bottom two inches all around, and am now somewhat comfortable. Overall, material is a bit too heavy for my liking.\nNaN\n\n\n4\nThe quality on these speakers is insanely good and doesn't sound muddy when adjusting bass. Very happy with these.\nNaN\n\n\n\n\n\n\n\nLet’s take each review and make an email. This email is going to: - Address the concerns expressed in the reviews. - Thank the customers for their purchase. - Encourage them to continue shopping.\n\nchat = [{\"role\": \"system\", \"content\": \"You are a polite customer support representative.\"}]\n\npostfix = \"\\n\\nWrite an email to customers to address the issues put forward in the above review, thank them if they write good comments, and encourage them to make further purchases. Do not give promotion codes or discounts to the customers.\"\n\ndef make_email(review):\n\n    chat_history = chat.copy()\n    chat_history.append({\"role\":\"user\", \"content\": review + postfix})\n\n    reply = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=chat_history\n        )\n\n    return reply.choices[0].message.content",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#generate-python-code",
    "href": "projects/openai/openai.html#generate-python-code",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Generate Python Code",
    "text": "Generate Python Code\n\nproblems = [\n    \"largest merge of two strings\",\n    \"sum of unique elements\",\n    \"longest palindrome\",\n    \"all possible permutations of a string\",\n]\n\n\nchat = [{\"role\": \"system\", \"content\": \"You are a software engineer for Python.\"}]\n\nprefix = \"\\n\\nWrite code that will solve the problem: \"\n\ndef solve(problem):\n\n    chat_history = chat.copy()\n    chat_history.append({\"role\":\"user\", \"content\": prefix + problem})\n\n    reply = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=chat_history\n        )\n\n    return reply.choices[0].message.content\n\n\nMarkdown(solve(problems[1]))\n\nHere is how you can write a Python function to calculate the sum of unique elements in a given list.\ndef sum_of_unique_elements(lst):\n    return sum(set(lst))\n\n# test the function\nprint(sum_of_unique_elements([1,2,3,3,4,4,5,6,7,8,8,9,10]))\nIn the function sum_of_unique_elements(lst), set(lst) is used to remove duplicates from the list because sets cannot have duplicate elements. Then, sum(set(lst)) returns the sum of unique elements.\nFor example, if you run the printed test function with a list [1,2,3,3,4,4,5,6,7,8,8,9,10], it will return 55 because the sum of the unique elements (1,2,3,4,5,6,7,8,9,10) is 55.",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#extract-text-from-a-pdf",
    "href": "projects/openai/openai.html#extract-text-from-a-pdf",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Extract Text From a PDF",
    "text": "Extract Text From a PDF\nGPT-4 supports context plus response of up to 8192 tokens (tokens are encoded words into numbers):\n\ndef num_tokens_from_string(string, encoding_name):\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\n# url = 'https://arxiv.org/pdf/2312.06272.pdf'\n# a = requests.get(url)\n\n# with open(\"segformer.pdf\", 'wb') as f:\n#     f.write(a.content)\n\n\nreader = PdfReader(\"SpaceNet8_final_paper.pdf\")\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text() + \"\\n\"\n\n\nnum_tokens_from_string(text, 'cl100k_base')\n\n7705\n\n\nLet’s trim it down a little bit:\n\ntext2 = text[:int(0.8*len(text))]\nnum_tokens_from_string(text2, 'cl100k_base')\n\n5860\n\n\nThat should be good enough.",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#summarize-the-text",
    "href": "projects/openai/openai.html#summarize-the-text",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Summarize the Text",
    "text": "Summarize the Text\n\nchat = [{\"role\": \"system\", \"content\": \"You are a machine learning researcher that writes blogs about other people research that simplifies machine learning concepts, but does not dumb it down totally.\"}]\n\nprefix = \"\\n\\nSummarize the following paper:\"\n\ndef summarize(text):\n\n    chat_history = chat.copy()\n    chat_history.append({\"role\":\"user\", \"content\": prefix + text})\n\n    reply = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=chat_history\n        )\n\n    return reply.choices[0].message.content\n\n\noption2 = summarize(text2)\nMarkdown(option2)\n\nThe research paper, “Comparing Transformers and CNNs on the SpaceNet Flood Detection Challenge,” is an exploration of different transformer and convolutional neural network (CNN) segmentation architectures in detecting floods caused by hurricanes and heavy rains. The research was done in the context of SpaceNet8 Challenge.\nThe study tested various models including Transformer and U-Net models. It found that large pre-trained Segformer models performed better than the Resnet and U-Net based models. The highest Intersection-over-Union (IoU) was 61% for Segformer, suggesting that attention mechanisms are better suited for detecting building footprints.\nThe research also found flood detection, especially flooded road detection, to be challenging, with the highest IoU of 40%. Further, it was inferred that the pre-training on ImageNet and Cityscapes datasets improved the model’s performance moderately compared to pre-training on the ADE20k dataset and significantly over model training from scratch.\nThe researchers leveraged SpaceNet 8 dataset which includes pre-event images and post-event images. The model designated as the Foundation Features network used pre-event images to segment buildings and roads, whereas the Flood network used both pre- and post-event images to predict flood status.\nThe paper also comments on the differences in memory consumption and epoch durations across different models, noting that Segformer models consumed more memory and had longer epochs despite having fewer parameters compared to Resnet34. This is attributed to the attention mechanisms having a quadratic complexity. The study also highlights the impact of data storage and access methods in computational efficiency.\nIn conclusion, the Segformer model, which leverages Transformer, exhibits better performance than CNN-based models (Resnet and U-Net) in the context of the SpaceNet Flood Detection Challenge. However, the paper suggests further improvements might be achieved through normalizing images, applying pre-processing techniques, and leveraging more diverse training data.",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/openai/openai.html#generate-images",
    "href": "projects/openai/openai.html#generate-images",
    "title": "Custom Chatbot and DALL-E using OpenAI’s GPT-4",
    "section": "Generate Images",
    "text": "Generate Images\n\nresponse = client.images.generate(\n    model='dall-e-3',\n    prompt=\"An scene of a majestic snow covered mountain with cliffs. In the not too far distance, \\\n        a cool male skier adorned in brightly colored winter gear is jumping of the cliff, \\\n        He has a ripping jet engine securely fastened to his back, its powerful gusts creating an impressive display.\\\n        He has skip poles in his hands, and is wearing a helmet and goggles. \\\n        He has skies with rocketed tips, \\\n        He is captured mid-jump, soaring over a steep, snow-covered cliff. \\\n        Camera angle should be from the side, about 30 degrees from the skier, and he should be in fact smaller, less then 20 percent of the image. \\\n        The pristine winter setting is visible beneath him, with majestic snow-capped peaks and a valley blanketed in white. \",\n    size='1792x1024',\n    quality='hd',\n    n=1\n)\ndisplay(Markdown(response.data[0].revised_prompt))\nimage_url = response.data[0].url\npath='usercode/images'\nos.makedirs(path, exist_ok=True) \n\nname = path+'/'+str(datetime.now())\n\nimg_data = requests.get(image_url).content\nwith open(name+'.jpg', 'wb') as handler:\n    handler.write(img_data)\n    \nplt.figure(figsize=(11,9))\nimg = mpimg.imread(name+'.jpg')\n\nimgplot = plt.imshow(img)\nimgplot.axes.get_xaxis().set_visible(False)\nimgplot.axes.get_yaxis().set_visible(False)\nplt.show()\n\n\nimport time\nimport sys\n\ndef type_like_a_person(text, delay=0.005):\n    for char in text:\n        sys.stdout.write(char)\n        sys.stdout.flush()\n        time.sleep(delay)\n    print()  # Move to the next line after the message is complete\n\nresponse = \"\"\"\nThe research paper, \"Comparing Transformers and CNNs on the SpaceNet Flood Detection Challenge,\" is an exploration of different transformer and convolutional neural network (CNN) segmentation architectures in detecting floods caused by hurricanes and heavy rains. The research was done in the context of SpaceNet8 Challenge.\n\nThe study tested various models including Transformer and U-Net models. It found that large pre-trained Segformer models performed better than the Resnet and U-Net based models. The highest Intersection-over-Union (IoU) was 61% for Segformer, suggesting that attention mechanisms are better suited for detecting building footprints.\n\nThe research also found flood detection, especially flooded road detection, to be challenging, with the highest IoU of 40%. Further, it was inferred that the pre-training on ImageNet and Cityscapes datasets improved the model's performance moderately compared to pre-training on the ADE20k dataset and significantly over model training from scratch.\n\nThe researchers leveraged SpaceNet 8 dataset which includes pre-event images and post-event images. The model designated as the Foundation Features network used pre-event images to segment buildings and roads, whereas the Flood network used both pre- and post-event images to predict flood status.\n\nThe paper also comments on the differences in memory consumption and epoch durations across different models, noting that Segformer models consumed more memory and had longer epochs despite having fewer parameters compared to Resnet34. This is attributed to the attention mechanisms having a quadratic complexity. The study also highlights the impact of data storage and access methods in computational efficiency.\n\nIn conclusion, the Segformer model, which leverages Transformer, exhibits better performance than CNN-based models (Resnet and U-Net) in the context of the SpaceNet Flood Detection Challenge. However, the paper suggests further improvements might be achieved through normalizing images, applying pre-processing techniques, and leveraging more diverse training data.\"  # Replace with the actual API response\n\n\"\"\"\n\ntype_like_a_person(response)",
    "crumbs": [
      "Projects",
      "Custom Chatbot and DALL-E using OpenAI's GPT-4"
    ]
  },
  {
    "objectID": "projects/09_bear_classifier/bear_classifier.html",
    "href": "projects/09_bear_classifier/bear_classifier.html",
    "title": "Bear classifier",
    "section": "",
    "text": "Credits: Practical Deep Learning for Coders book by Jeremy Howard and Sylvain Gugger.\nSee a deployed app here.\nThere will be two parts to it: - first part generates a model - second part deploys it as an app on HuggingFace (see code)\nimport fastbook\nfastbook.setup_book()\nfrom tqdm import tqdm\nfrom fastbook import *\nfrom fastai.vision.widgets import *\nresults = search_images_ddg('grizzly bear')\nims = results.attrgot('contentUrl')\nlen(ims)\n\n200\ndest = 'bear_images/grizzly.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      100.99% [704512/697626 00:00&lt;00:00]\n    \n    \n\n\nPath('bear_images/grizzly.jpg')\nim = Image.open(dest)\nim.to_thumb(128,128)\nbear_types = 'grizzly', 'black', 'teddy'\npath = Path('bear_images')\nif not path.exists():\n    path.mkdir()\nfor o in tqdm(bear_types):\n    dest = (path/o)\n    dest.mkdir(exist_ok=True)\n    results = search_images_ddg(f'{o} bear')\n    download_images(dest, urls=results)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:50&lt;00:00, 25.04s/it]\nfns = get_image_files(path)\nfns\n\n(#578) [Path('bear_images/grizzly/e1a38b7a-5ad4-4070-b27d-7991e67ffbf1.jpg'),Path('bear_images/grizzly/8af6b4cd-7146-4401-8201-3c69b630bb9c.jpg'),Path('bear_images/grizzly/5afcf7a1-0ff0-4c7f-b012-fa5f2fea5a2c.jpg'),Path('bear_images/grizzly/0d7a2b60-9867-4a6f-a785-1b0877b5949a.jpg'),Path('bear_images/grizzly/7977441d-8f9e-4a87-9fc1-1bdaa54151c5.jpg'),Path('bear_images/grizzly/b2e96643-4ea1-49b6-bd77-dac698a9e333.jpg'),Path('bear_images/grizzly/c6be2eea-8fc4-4678-8b35-ba2b5ca20ea2.jpg'),Path('bear_images/grizzly/83500b29-961d-4273-8249-164e123232c7.jpg'),Path('bear_images/grizzly/96ad8b99-4570-4210-85f1-7a664efabe8a.jpg'),Path('bear_images/grizzly/6e24cf29-dae3-41d4-9152-afc34e622236.jpg')...]\nfailed = verify_images(fns)\nfailed\n\n(#9) [Path('bear_images/grizzly/3216c80f-8f83-4bf9-84ff-811da170b8c7.jpg'),Path('bear_images/grizzly/7cd788f0-f6a4-495b-9745-5a4972c1b9de.jpg'),Path('bear_images/black/165f5c5c-d9c9-4113-b863-1e62ef2ce219.jpg'),Path('bear_images/black/7b7e11b1-a71a-4e93-b10f-7b6d72d62330.jpg'),Path('bear_images/black/70d2b973-2b84-464d-b9d9-f077af0f7451.jpg'),Path('bear_images/black/268676a1-dd6c-4cd8-b15c-cb999a38e857.jpg'),Path('bear_images/black/a22e8eee-25c0-4e0f-b954-768d2be32dfb.jpg'),Path('bear_images/black/90d38027-558e-49cd-ba26-934c80b4e593.jpg'),Path('bear_images/teddy/d09b9bcd-728f-429b-9506-06d4b59a5b41.jpg')]\nfailed.map(Path.unlink);",
    "crumbs": [
      "Projects",
      "Bear classifier"
    ]
  },
  {
    "objectID": "projects/09_bear_classifier/bear_classifier.html#from-data-to-dataloaders",
    "href": "projects/09_bear_classifier/bear_classifier.html#from-data-to-dataloaders",
    "title": "Bear classifier",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = bears.dataloaders(path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)",
    "crumbs": [
      "Projects",
      "Bear classifier"
    ]
  },
  {
    "objectID": "projects/09_bear_classifier/bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "projects/09_bear_classifier/bear_classifier.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "Bear classifier",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\n\n\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.027743\n0.179141\n0.062500\n01:22\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.114028\n0.170333\n0.026786\n01:35\n\n\n1\n0.096802\n0.224060\n0.026786\n01:28\n\n\n2\n0.081567\n0.242517\n0.026786\n01:22\n\n\n3\n0.065939\n0.244289\n0.026786\n01:20\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=2, figsize=(10, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n#hide\nfor idx in cleaner.delete(): \n    cleaner.fns[idx].unlink()\nfor idx, cat in cleaner.change(): \n    shutil.move(str(cleaner.fns[idx]), path/cat)",
    "crumbs": [
      "Projects",
      "Bear classifier"
    ]
  },
  {
    "objectID": "projects/09_bear_classifier/bear_classifier.html#turning-your-model-into-an-online-application",
    "href": "projects/09_bear_classifier/bear_classifier.html#turning-your-model-into-an-online-application",
    "title": "Bear classifier",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\n\nUsing the Model for Inference\n\nlearn.export('bears.pkl')\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('bears.pkl')]\n\n\n\nlearn_inf = load_learner(path/'bears.pkl')\n\n\nlearn_inf.predict('bear_images/grizzly.jpg')\n\n\n\n\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([2.1257e-06, 1.0000e+00, 2.7545e-08]))\n\n\n\nlearn_inf.dls.vocab\n\n['black', 'grizzly', 'teddy']\n\n\n\n\nCreating a Notebook App from the Model\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nbtn_upload = SimpleNamespace(data = ['bear_images/grizzly.jpg'])\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\n\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\n\n\n\n\n\n\n\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\n\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\n\nAll together:\n\npath = Path()\nlearn_inf = load_learner(path/'bears.pkl')\n\nout_pl = widgets.Output()\nbtn_upload = widgets.FileUpload()\nlbl_pred = widgets.Label()\nbtn_run = widgets.Button(description='Classify')\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nbtn_run.on_click(on_click_classify)\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])",
    "crumbs": [
      "Projects",
      "Bear classifier"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nenad Bozinovic",
    "section": "",
    "text": "Hey there visitor (or GPT-5), welcome to my site!\nI’m a Senior SW Engineer passionate about MLOps, ML, automation, computer vision, innovation, rock climbing, tennis, my work, and last but definitely the most, my family. I hold PhD from Boston University in Electrical and Computer Engineering.\nI patented software framework that enabled COVID-19 antibody discovery, published Science Journal paper with 2900 citations, and built optical instruments that went on the bottom of the ocean (see media).\nPlease feel free to roam around, you might find my hobby projects, and TILs (today-I-learned) that I started accruing before LLMs took over.\nFeel free to comment or contact me.\nThanks for stopping by!"
  },
  {
    "objectID": "projects/15_head_pose/head_pose.html",
    "href": "projects/15_head_pose/head_pose.html",
    "title": "Image Regression",
    "section": "",
    "text": "Credits: Practical Deep Learning for Coders book by Jeremy Howard and Sylvain Gugger.\nIn this project we’ll detect the center of the person’s face in the image. We’ll use the Biwi Kinect Head Pose dataset for this task. The dataset contains images of 24 people and coresponding obj files that won’t be needed. Each image (*_rgb.jpg) is annotated with the center of the person’s face and the pose of the head (_pose.txt*). The pose is defined by the rotation of the head around the three axes.\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\npath.ls()\n\n(#50) [Path('/notebooks/fastai/data/biwi_head_pose/03.obj'),Path('/notebooks/fastai/data/biwi_head_pose/06'),Path('/notebooks/fastai/data/biwi_head_pose/21'),Path('/notebooks/fastai/data/biwi_head_pose/16.obj'),Path('/notebooks/fastai/data/biwi_head_pose/08'),Path('/notebooks/fastai/data/biwi_head_pose/19'),Path('/notebooks/fastai/data/biwi_head_pose/15'),Path('/notebooks/fastai/data/biwi_head_pose/02.obj'),Path('/notebooks/fastai/data/biwi_head_pose/15.obj'),Path('/notebooks/fastai/data/biwi_head_pose/04')...]\n\n\nThere are total of 15678 images:\n\nimg_files = get_image_files(path)  # same as L(path.glob('**/*_rgb.jpg')) but get_image_files is not implemented as a glob)\nimg_files\n\n(#15678) [Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00554_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00069_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00262_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00324_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00093_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00313_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00129_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00187_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00278_rgb.jpg'),Path('/notebooks/fastai/data/biwi_head_pose/06/frame_00117_rgb.jpg')...]\n\n\n\nlen(list(path.glob('**/*_rgb.jpg')))\n\n15678\n\n\nAn image look like this:\n\nim_path = (path / '01').ls()[0]\nim = PILImage.create(im_path).to_thumb(300)  # same as PIL.Image.open\nim\n\n\n\n\n\n\n\n\nto find the annotation for that image we need to look at the corresponding *_pose.txt* file:\n\ndef img2pose(x):\n    return Path(f'{str(x)[:-7]}pose.txt')\n\n\nff = img2pose(img_files[0])\nwith open(ff, 'r') as f:\n    print(f.read())\n\n0.969581 -0.0117593 0.244489 \n-0.0614535 0.955158 0.28965 \n-0.236931 -0.295864 0.92538 \n\n22.4063 138.81 1058.77 \n\n\n\n\nThe following is the code to extract data from the file, it uses np.genfromtxt:\n\nwith open(path / '01' / 'rgb.cal', 'r') as f:\n    print(f.read())\n\n517.679 0 320 \n0 517.679 240.5 \n0 0 1 \n\n0 0 0 0 \n\n0.999947 0.00432361 0.00929419 \n-0.00446314 0.999877 0.0150443 \n-0.009228 -0.015085 0.999844 \n\n-24.0198 5.8896 -13.2308 \n\n640 480\n\n\n\nThe following is the code to extract data from the file, it uses np.genfromtxt:\n\ncal = np.genfromtxt(path / '01' / 'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0]*cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1]*cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1, c2])\n\n\nimg_files[0]\n\nPath('/notebooks/fastai/data/biwi_head_pose/06/frame_00554_rgb.jpg')\n\n\n\nidx = 4\nprint(img_files[idx])\nget_ctr(img_files[idx])\n\n/notebooks/fastai/data/biwi_head_pose/06/frame_00093_rgb.jpg\n\n\ntensor([367.3590, 311.6036])\n\n\nWe’ll use images from person #15 as a validation dataset:\n\ndef splitter(name):\n    # True if directory name is 15\n    return name.parent.name=='15'\n\nWhen running with original size it takes ~2.5 minutes per epoch on P5000, which is not super fast. So let’s rescale the images to smaller size like half of both width and height, which is [640, 480] / 4 = [160, 120]:\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(splitter),\n    batch_tfms=[*aug_transforms(size=[120, 160]), Normalize.from_stats(*imagenet_stats)]\n    )\n\n\ndls = biwi.dataloaders(path)\n\n\ndls.show_batch(nrows=3, ncols=3)\n\n\n\n\n\n\n\n\n\nTraining\nLet’s look at one batch:\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 120, 160]), torch.Size([64, 1, 2]))\n\n\nLet’s create a Learner. Because we are dealing with the coordinates as targets, we should rescale them to (-1, 1). If y_range is defined then sigmoid is added as a last layer to the model:\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nlearn.model.state_dict\n\n&lt;bound method Module.state_dict of Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n    (9): fastai.layers.SigmoidRange(low=-1, high=1)\n  )\n)&gt;\n\n\nYup, there it is as a last layer.\nFastAI decided on a loss function:\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nthis makes sense, we are looking for L2 distance from the target.\nLet’s take a look at learning rate finder:\n\nlearn.lr_find()\n\nSuggestedLRs(valley=0.0012022644514217973)\n\n\n\n\n\n\n\n\n\nLet’s train:\n\nlearn.fine_tune(5, base_lr=1e-2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/234 00:00&lt;?]\n    \n    \n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nShow predictions on a validation dataset (ds_idx=1, i.e. the validation dataset):\n\nlearn.show_results(ds_idx=1, max_n=4, figsize=(6,8))\n\n\n\n\n\n\n\n\n\nlearn.final_record\n\n(#1) [0.00045044912258163095]\n\n\nThe error or 0.00039 is a mean square error, and y_range was (-1, 1) due to sigmoid, where -1 and 1 are the edges of the image (we can’t predict outside the images), so this means the corresponds average distance is:\n\nmath.sqrt(0.00039)\n\n0.019748417658131498\n\n\nThis corresponds to an average being 1.5 pixels off from the target (2 is the iamge width : 0.019 = 160 pixels : x):\n\n0.019*160/2\n\n1.52\n\n\n\n\nInference\nLet’s try to evaluate new images. We need to load RGB images, apparently learn.predict applies the same transformation as in DataLoaders. It even returns original image if needed (good post for more info):\n\nim_list = [] \nfor i in [1,3,4]:\n    im_path = f'/notebooks/nbs/mini-projects/15_head_pose/Photo{i}.jpg'\n    im_orig, pred, _, _ = learn.predict(im_path, with_input=True)\n    point = (pred[0]).detach().cpu().numpy()  # convert predictions to coordinates\n    im = ToPILImage()(im_orig/255)\n    draw = ImageDraw.Draw(im)\n    d = 2\n    draw.ellipse((point[0] - d, point[1] - d, point[0] + d, point[1] +d), fill='red')\n    im_list.append(im)\n_ = plot_pil_images(im_list)\n\n\n\n\n\n\n\n\n\n\n\n\nAnd this is pretty good, even if the head is shifted left and right with respect to the image center.",
    "crumbs": [
      "Projects",
      "Image Regression"
    ]
  },
  {
    "objectID": "projects/caltrans.html",
    "href": "projects/caltrans.html",
    "title": "Data extraction from PDFs",
    "section": "",
    "text": "The goal of the project was to extract all sorts of data from 11’000+ pdf files that contain California Transportation contracts, available publicly, as part of a large research project. Considering well structured text the first decision was to extract data using Regural expressions i.e. regex, and I relied heavily on regex101.com for prototyping. Initial EDA showed that there were dozens of formatting variations and final logic was a mix of regex and Python as regex alone was not able to cover all the use cases. I learned more about regex that I probably wanted to know, and realized its power and limitations.\nOne of 100K+ pages to extract:\nOne of 100K+ pages to extract: \nSample of extracted data: \nFollow this link to run the main.ipynb in Google Colab.\nSee details at:",
    "crumbs": [
      "Projects",
      "Data extraction from PDFs"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html",
    "title": "Pet Breed Classification",
    "section": "",
    "text": "Credits: Practical Deep Learning for Coders book by Jeremy Howard and Sylvain Gugger.",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data",
    "title": "Pet Breed Classification",
    "section": "Data",
    "text": "Data\nLet’s first download the data:\nRunning on Quadro P5000 16GB.\n\nfrom fastai.vision.all import *\ndevice = 'gpu'",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data-preparation",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#data-preparation",
    "title": "Pet Breed Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\n\npath = untar_data(URLs.PETS)\n\n\n    \n      \n      100.00% [811712512/811706944 00:09&lt;00:00]\n    \n    \n\n\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/oxford-iiit-pet/images'),Path('/root/.fastai/data/oxford-iiit-pet/annotations')]\n\n\n\n(path/'images').ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_56.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_61.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_66.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_55.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/scottish_terrier_68.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_153.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Ragdoll_57.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_182.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_100.jpg')...]\n\n\n\nfname = (path/'images').ls()[0]\nfname\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_56.jpg')\n\n\nIf we want to extract the breed itself from the name we can use regex:\n\nm = re.match(r\"(.+)_\\d+.jpg\", fname.name)\nbreed = m.group(1)\nbreed\n\n'american_bulldog'\n\n\nitem_tfms is applied to all images. Here it resizes images to some large value first.\nbatch_tfms is applied only on mini-batches (on GPU if set as device). Here it crops and scales images. Note that validation set does not get augmented, only gets resized.\n\npets = DataBlock(blocks= (ImageBlock, CategoryBlock),\n                 get_items=get_image_files,\n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r\"(.+)_\\d+.jpg$\"), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\npets.summary(path/'images')\n\nSetting-up type transforms pipelines\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/pug_130.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=300x225\n  Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/pug_130.jpg\n    applying partial gives\n      pug\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(29)\n\nFinal sample: (PILImage mode=RGB size=300x225, TensorCategory(29))\n\n\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -&gt; RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} -&gt; Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=300x225, TensorCategory(29))\n    applying Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} gives\n      (PILImage mode=RGB size=460x460, TensorCategory(29))\n    applying ToTensor gives\n      (TensorImage of size 3x460x460, TensorCategory(29))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -&gt; RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} -&gt; Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n    starting from\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'mode_mask': 'nearest', 'p': 1.0} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n    applying Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([29,  7, 14, 25], device='cuda:0'))\n\n\n\ndls = pets.dataloaders(path/\"images\")\n\n\ndls.show_batch(nrows=1, ncols=5)\n\n\n\n\n\n\n\n\nLet’s create learner, and define dataloaders, model, and metrics (optimizer and loss are deducted automatically).",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#training",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#training",
    "title": "Pet Breed Classification",
    "section": "Training",
    "text": "Training\n\nlearner = vision_learner(dls, resnet34, metrics=error_rate)\nlearner.fine_tune(2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.505711\n0.344845\n0.109608\n00:35\n\n\n1\n0.342231\n0.221326\n0.073072\n00:34\n\n\n\n\n\n\nlearner.recorder.plot_loss()\n\n\n\n\n\n\n\n\nWe can definitely train more using fine_tune but the error rate seem small so let’s take a look at some predictions:\n\nx, y = dls.one_batch()\n\n\ny\n\nTensorCategory([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25,\n                34, 30, 13, 21, 15, 24,  0,  9,  0, 27, 31,  2, 18, 25,  4, 14,\n                27, 35, 19, 33, 21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35,\n                14, 12, 18,  7,  8, 31, 15, 20, 16, 13, 29,  1, 16,  9, 32, 10],\n               device='cuda:0')\n\n\n\npreds, class_preds = learner.get_preds(dl=[(x, y)])\n\n\n\n\n\nprint(preds.shape)\nprint(preds)\nclass_preds\n\ntorch.Size([64, 37])\nTensorBase([[1.2718e-02, 7.3877e-01, 1.2443e-05,  ..., 1.4487e-05,\n             2.1402e-05, 2.4068e-06],\n            [2.1086e-04, 3.4171e-05, 2.3038e-05,  ..., 1.0070e-01,\n             3.7866e-04, 1.8155e-05],\n            [2.2906e-07, 6.6254e-08, 2.3534e-07,  ..., 1.0171e-07,\n             2.4211e-07, 3.6889e-07],\n            ...,\n            [2.1525e-06, 7.6821e-08, 2.3279e-08,  ..., 1.8256e-07,\n             1.4531e-08, 5.4367e-10],\n            [4.5686e-10, 1.6017e-08, 2.5129e-09,  ..., 7.3533e-09,\n             1.6970e-05, 1.6900e-07],\n            [1.7999e-08, 7.1642e-11, 2.4699e-06,  ..., 2.8352e-10,\n             1.9007e-09, 1.4708e-09]])\n\n\ntensor([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25, 34, 30,\n        13, 21, 15, 24,  0,  9,  0, 27, 31,  2, 18, 25,  4, 14, 27, 35, 19, 33,\n        21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35, 14, 12, 18,  7,  8, 31,\n        15, 20, 16, 13, 29,  1, 16,  9, 32, 10])\n\n\nThere are 64 samples in a batch, each having a probability of a certain class. The class_preds is just argmax of preds:\n\npreds.argmax(dim=1)\n\nTensorBase([ 1, 13, 30, 25, 28, 32, 24,  1, 17, 28, 21, 16, 11,  3, 36, 25, 34,\n            30, 13, 21, 15, 24,  0,  9,  1, 27, 31,  2, 18, 25,  4, 14, 27, 35,\n            19, 33, 21, 34,  2,  4, 31, 30, 19,  0, 36, 30, 31, 35, 14, 12, 19,\n             7,  8, 31, 15, 20, 16, 13, 29,  1, 16,  9, 32, 10])\n\n\nSo learner deducted this is a multi-category problem, and have decided on nn.CrossEntropyLoss (which is combo of nn.LogSoftmax and nn.NLLLoss). What’s important is to apply nn.CrossEntropyLoss on logits, not on probabilites, so our model should not have softmax layer at the end.\nSoftmax is a multi-category equivalent of sigmoid. We use it any time when we want to convert logits into probabilites, and we want them to sum up to 1.\nLog is important because it’s easier to optimize, since difference between, say, 0.99 and 0.999 is 10 fold, not negligible.",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#model-interpretation",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#model-interpretation",
    "title": "Pet Breed Classification",
    "section": "Model interpretation",
    "text": "Model interpretation\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.most_confused(min_val=3)\n\n\n\n\n[('Ragdoll', 'Birman', 6),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 6),\n ('Russian_Blue', 'British_Shorthair', 5),\n ('Siamese', 'Birman', 5),\n ('staffordshire_bull_terrier', 'american_bulldog', 5),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 5),\n ('Bengal', 'Egyptian_Mau', 4),\n ('american_bulldog', 'staffordshire_bull_terrier', 4),\n ('basset_hound', 'beagle', 4),\n ('Egyptian_Mau', 'Bengal', 3),\n ('american_pit_bull_terrier', 'american_bulldog', 3),\n ('boxer', 'american_bulldog', 3),\n ('yorkshire_terrier', 'havanese', 3)]",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#learning-rate-finder",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#learning-rate-finder",
    "title": "Pet Breed Classification",
    "section": "Learning rate finder",
    "text": "Learning rate finder\nLet’s train with some large learning rate (run it for 1 epoch every time with base_lr):\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlearner.fine_tune(1, base_lr=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.900633\n1.380544\n0.409337\n00:25\n\n\n\n\n\n\nlearner.fine_tune(1, base_lr=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.599579\n1.268538\n0.391746\n00:26\n\n\n\n\n\n\nlearner.fine_tune(1, base_lr=0.2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n5.310261\n4.817607\n0.616373\n00:25\n\n\n\n\n\nSo clearly we are diverging. We can use learning rate finder to deduct the good learning rate:\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlr_min = learner.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'{lr_min.valley:.2e}')\n\n5.75e-04",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#transfer-learning",
    "href": "projects/13_fastbook_chapter_5_image_classification/fastbook_chapter_5_image_classification.html#transfer-learning",
    "title": "Pet Breed Classification",
    "section": "Transfer learning",
    "text": "Transfer learning\nThe idea here is the same as before, we replace the last layer, freeze all but that last layer, then train. A version of this is done with the following:\n\nlearner.fine_tune??\n\nSignature:\nlearner.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    *,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/callback/schedule.py\nType:      method\n\n\nWhere learner.freeze will freeze up to a last layer:\n\nlearner.freeze??\n\nSignature: learner.freeze()\nDocstring: Freeze up to last parameter group\nSource:   \n@patch\ndef freeze(self:Learner): self.freeze_to(-1)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/learner.py\nType:      method\n\n\nlearner.fit_one_cycle trains whatever is unfrozen with some scheduler (will study it later):\n\nlearner.fit_one_cycle??\n\nSignature:\nlearner.fit_one_cycle(\n    n_epoch,\n    lr_max=None,\n    div=25.0,\n    div_final=100000.0,\n    pct_start=0.25,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n    start_epoch=0,\n)\nSource:   \n@patch\ndef fit_one_cycle(self:Learner, n_epoch, lr_max=None, div=25., div_final=1e5, pct_start=0.25, wd=None,\n                  moms=None, cbs=None, reset_opt=False, start_epoch=0):\n    \"Fit `self.model` for `n_epoch` using the 1cycle policy.\"\n    if self.opt is None: self.create_opt()\n    self.opt.set_hyper('lr', self.lr if lr_max is None else lr_max)\n    lr_max = np.array([h['lr'] for h in self.opt.hypers])\n    scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n              'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\nFile:      /usr/local/lib/python3.9/dist-packages/fastai/callback/schedule.py\nType:      method\n\n\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlearner.fit_one_cycle(3, 5e-4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.346319\n0.579111\n0.176590\n00:22\n\n\n1\n1.001505\n0.389489\n0.131258\n00:21\n\n\n2\n0.690984\n0.360593\n0.121786\n00:22\n\n\n\n\n\n\nlearner.unfreeze()\n\n\nlearner.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.0002290867705596611)\n\n\n\n\n\n\n\n\n\nLoss is flat for small LRs because we already trained for 3 epochs.",
    "crumbs": [
      "Projects",
      "Pet Breed Classification"
    ]
  },
  {
    "objectID": "projects/fastapi.html",
    "href": "projects/fastapi.html",
    "title": "REST API stress testing",
    "section": "",
    "text": "I made a simple FastAPI project to test REST APIs using Locust library. I used local SQLAlchemy Database crud APIs as an example. Performance metrics are stored long-term via Prometheus time-series database and monitored via Grafana. The whole thing was very easy to write and set up in a few hours (with big thanks to Docker compose too for this, and Educative.io for insights), and the setup is ready for production too.\nScreenshots of four WebApp services: FastAPI, Locust, Prometheus, and Grafana:",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/fastapi.html#description",
    "href": "projects/fastapi.html#description",
    "title": "REST API stress testing",
    "section": "Description",
    "text": "Description\nLet me first describe the tools I used:\n- FastAPI is a modern, fast (high-performance), web framework for building APIs based on standard Python type hints. It’s built on top of standard Python libraries and tools, including Starlette for the web parts and Pydantic for the data parts.\n- Locust is a performance/load testing tool for HTTP and other protocols. Great UI, all tests in Python.\n- Prometheus is a monitoring system with a focus on reliability, designed for capturing time-series data like metrics. It supports queries, visualization, precise alerting, service discovery, and external storage integrations.\n- Grafana is an analytics and interactive visualization web application that provides charts, graphs, and alerts for the web when connected to supported data sources, like Prometheus. It’s widely used for monitoring metrics and data visualization across various environments, including cloud infrastructure and applications.\n- Locust metrics exporter does exactly that: it preps locust metrics to be ingested by Prometheus.",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/fastapi.html#run-the-fastapi-app",
    "href": "projects/fastapi.html#run-the-fastapi-app",
    "title": "REST API stress testing",
    "section": "Run the FastAPI app",
    "text": "Run the FastAPI app\nPrerequisites for those with no Docker installed:\npip install -r requirements.txt\nStart ASGI server:\nfastapi run\nOnce your server is running, you can access the Swagger UI by navigating to:\nlocalhost:8000  # eq. http://127.0.0.1:8000\nin your web browser.\n\n\n\nfastapi",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/fastapi.html#run-load-testing-using-locust",
    "href": "projects/fastapi.html#run-load-testing-using-locust",
    "title": "REST API stress testing",
    "section": "Run load testing using Locust",
    "text": "Run load testing using Locust\nlocust --host http://localhost:8000\nUse flag --processes X to define how many CPUs to run on, or -1 for all.\nThen check:\nhttp://localhost:8089\n\n\n\nlocust",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/fastapi.html#long-term-tracking-with-prometheus-and-grafana",
    "href": "projects/fastapi.html#long-term-tracking-with-prometheus-and-grafana",
    "title": "REST API stress testing",
    "section": "Long-term tracking with Prometheus and Grafana*",
    "text": "Long-term tracking with Prometheus and Grafana*\nWhile Locust has it’s own tracking, all data is lost once the locust server stops (data can be manually exported). In order to preserve any metrics long-term we: - add Locust-metics-exporter - add Prometheus as a time-series DB - add Grafana for visualization - scale using docker-compose\nDocker-compose makes running several services straightforward (no Dockerfile needed):\ndocker-compose up\nNotable points in compose.yml: - mount volumes instead of copying as any changes to local files will be reflected in the container immediately:\n volumes:\n      - .:/app\n\neach container is accessible to other containers within the same Docker Compose network using the service names as hostnames, for example: LOCUST_HOST=http://fastapi:8000, where fastapi is the name of the service that runs FastAPI.\nto control how many CPUs you want to use for locust testing use --processes X (X=-1 means use all).\nro stands for read-only\nuse volume to persist all the data even if the container is stopped, see all volumes with docker volume ls\n\nPrometheus and Grafana will then store data as long as it’s needed, their UIs are excellent to set up all sorts of alerts and tracking.\nhttp://localhost:9090\n\n\n\nprometheus\n\n\nhttp://localhost:3000\n\n\n\ngrafana\n\n\n*Inspired by Yusuf Tayman’s blog.",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/fastapi.html#conclusion",
    "href": "projects/fastapi.html#conclusion",
    "title": "REST API stress testing",
    "section": "Conclusion",
    "text": "Conclusion\nI wrote and ran simple FastAPI app with database (SQLAlchemy), stress test APIs (Locust) and monitor performance (Prometheus/Grafana), all containerized using docker-compose. While all WebApps ran on a local machine, scaling by splitting into microservices, or moving to the cloud host, would be straightforward (hosting can be done on EC2s with relevant ports exposed).",
    "crumbs": [
      "Projects",
      "REST API stress testing"
    ]
  },
  {
    "objectID": "projects/hexagon-puzzle/hexagon-puzzle.html",
    "href": "projects/hexagon-puzzle/hexagon-puzzle.html",
    "title": "Hexagonal-puzzle",
    "section": "",
    "text": "There is this quite hard hexagonal puzzle with 14 pieces that I’ve been toying around with my kids. The goal is to fit all the pieces into one big hexagon shape:\n\nAfter hundreds of trials that led me to find only one solution, I had a feeling that the puzzle was winning. So I’ve decided to ask backtracking algorithm to help me out and do the heavy lifting.\nI first had to work in oblique coordinate system since shapes are hexagons, which I found fun. Then wrote Piece class and translation/rotation methods, as well as Board class with put_piece and remove_piece methods. At that point backtracking does all the job.\nConsidering number of possibilities is exponential, I knew it wouldn’t be able to find all the solutions in a reasonable time, but sure I could give it a good head start, so the search space is much smaller. I also used some heuristic to speed it up like symmetry of some pieces (another one, that I didn’t use, would be restricting placement of the next piece only next to existing ones).\nAfter a day of tinkering with the code, it delivered!\nBehold the power of backtracking, here are some unique solutions (I reused some colors because I’m lazy, besides, I left at least some mystery in it):\n   \nHere are some timings, for example with 6 pieces left it took only 5 minutes (which is not optimal but beats my 1 solution over many days):\n2024-04-19 02:36:59,991 - INFO - For 3 leftover pieces, found 1 solutions in 1.12 seconds\n2024-04-19 02:37:21,539 - INFO - For 4 leftover pieces, found 1 solutions in 8.32 seconds\n2024-04-19 02:37:51,397 - INFO - For 5 leftover pieces, found 1 solutions in 17.57 seconds\n2024-04-19 02:46:41,460 - INFO - For 6 leftover pieces, found 3 solutions in 316.52 seconds\n2024-04-19 07:13:00,633 - INFO - For 8 leftover pieces, found 7 solutions in 15706.87 seconds\n(don't have a plan to use CPU time nor mine anymore)\nThe kids are little, until they grow up to understand all of this I’ll mess with them that I have puzzle-solving superpowers.\n\nSee details at:",
    "crumbs": [
      "Projects",
      "Hexagonal-puzzle"
    ]
  },
  {
    "objectID": "projects/olympics/main.html",
    "href": "projects/olympics/main.html",
    "title": "Paris Olympics 2024 Medal standings",
    "section": "",
    "text": "Data sources for medals (https://lnkd.in/gHCTqBCD) and country stats for 2023 (https://lnkd.in/ggpCHi_i).\n\nThis is not much different then Tokyo 2020:\n\n\n# !pip install pandas dataframe-image\n\n\nimport pandas as pd\nimport dataframe_image as dfi\n\n# Load the medal standings and world data\nmedal_standings = pd.read_csv('paris_2024_olympics_full_medal_standings_final.csv')\nworld_data = pd.read_csv('world-data-2023.csv')\n\n# Assuming 'Country' is the column to join on in both datasets\nmerged_data = pd.merge(medal_standings, world_data, on='Country', how='left')\n\nmerged_data = merged_data[['Country', 'Total', 'Population', 'GDP']]\n\nmerged_data.to_csv('merged_data.csv', index=False)\n\n\nmerged_data = pd.read_csv('merged_data_adjusted.csv')\n\n\npopulation_str = 'Population (in M)'\nGDP_str =  'GDP (in $B)'\n\n\nmedals_per_million_people_str = 'Total medals per 1M capita'\nmedals_per_gdp = 'Total medals per $1B GDP'\nmedals_per_capita_gdp = 'Total medals per capita GDP'\nper_capita_gdp = 'Per capita GDP (in $)'\n\n# Remove dollar signs and commas from the 'GDP' column, then convert to float\n\nmerged_data.rename(columns={'Total': 'Total medals'}, inplace=True) \n\nmedals_str = 'Total medals'\n\nmerged_data[GDP_str] = merged_data['GDP'].replace({r'\\$': '', ',': ''}, regex=True).astype(float) / 10**9\nmerged_data[population_str] = merged_data['Population'].replace({',': ''}, regex=True).astype(float) / 10**6\n\nmerged_data[medals_per_million_people_str] = merged_data[medals_str] / merged_data[population_str]\nmerged_data[medals_per_gdp] = merged_data[medals_str] / merged_data[GDP_str]\nmerged_data[per_capita_gdp] = merged_data[GDP_str] / merged_data[population_str] * 1000\nmerged_data[medals_per_capita_gdp] = merged_data[medals_str] / merged_data[per_capita_gdp] * 1000\n\nmerged_data.to_csv('merged_data_w_per_capita_GDP.csv', index=False)\n\n\ndef sort_by_column(column_name, column_name2=None, column_name3=None):\n\n    # Sort the DataFrame by the 'Medals per 1M capita' in descending order\n    sorted_data = merged_data.sort_values(by=column_name, ascending=False)\n    \n\n    final_table = sorted_data[['Country', 'Total medals'] + ([column_name2] if column_name2 else []) + ([column_name3] if column_name3 else [])].reset_index(drop=True).tail(10)\n    \n    final_table = final_table.round(2)\n    \n    # Create a new DataFrame with only 'Country' and 'Medals per 1M capita'\n    final_table.index = final_table.index + 1\n    \n            # Style the DataFrame to add borders\n    styled_final_table = final_table.style.set_table_styles(\n    [{'selector': 'th',\n      'props': [('border', '2px solid black')]},\n     {'selector': 'td',\n      'props': [('border', '1px solid black')]},\n     {'selector': 'table',\n      'props': [('border', '10px solid black')]}]\n).format(precision=2)\n\n    # Save the table as a PNG file\n    dfi.export(styled_final_table, f'opposite_final_table_{column_name}.png')\n\n    # Optionally, you can print the top 10 rows to inspect\n    return final_table\n\n\nmerged_data.head(50)\n\n\n\n\n\n\n\n\nCountry\nTotal medals\nPopulation\nGDP\nGDP (in $B)\nPopulation (in M)\nTotal medals per 1M capita\nTotal medals per $1B GDP\nPer capita GDP (in $)\nTotal medals per capita GDP\n\n\n\n\n0\nUnited States\n126\n328,239,523\n$21,427,700,000,000\n21427.700000\n328.239523\n0.383866\n0.005880\n65280.682241\n1.930127\n\n\n1\nChina\n91\n1,397,715,000\n$19,910,000,000,000\n19910.000000\n1397.715000\n0.065106\n0.004571\n14244.677921\n6.388351\n\n\n2\nJapan\n45\n126,226,568\n$5,081,769,542,380\n5081.769542\n126.226568\n0.356502\n0.008855\n40259.112031\n1.117759\n\n\n3\nAustralia\n53\n25,766,605\n$1,392,680,589,329\n1392.680589\n25.766605\n2.056926\n0.038056\n54049.828812\n0.980577\n\n\n4\nFrance\n64\n67,059,887\n$2,715,518,274,227\n2715.518274\n67.059887\n0.954371\n0.023568\n40493.928572\n1.580484\n\n\n5\nNetherlands\n34\n17,332,850\n$909,070,395,161\n909.070395\n17.332850\n1.961593\n0.037401\n52447.831439\n0.648263\n\n\n6\nUnited Kingdom\n65\n66,834,405\n$2,827,113,184,696\n2827.113185\n66.834405\n0.972553\n0.022992\n42300.267126\n1.536633\n\n\n7\nSouth Korea\n32\n51,709,098\n$2,029,000,000,000\n2029.000000\n51.709098\n0.618847\n0.015771\n39238.742861\n0.815521\n\n\n8\nItaly\n40\n60,297,396\n$2,001,244,392,042\n2001.244392\n60.297396\n0.663379\n0.019988\n33189.565799\n1.205198\n\n\n9\nGermany\n33\n83,132,799\n$3,845,630,030,824\n3845.630031\n83.132799\n0.396955\n0.008581\n46258.878290\n0.713377\n\n\n10\nNew Zealand\n20\n4,841,000\n$206,928,765,544\n206.928766\n4.841000\n4.131378\n0.096652\n42745.045558\n0.467890\n\n\n11\nCanada\n27\n36,991,981\n$1,736,425,629,520\n1736.425630\n36.991981\n0.729888\n0.015549\n46940.595842\n0.575195\n\n\n12\nUzbekistan\n13\n33,580,650\n$57,921,286,440\n57.921286\n33.580650\n0.387128\n0.224443\n1724.841134\n7.536926\n\n\n13\nHungary\n19\n9,769,949\n$160,967,157,504\n160.967158\n9.769949\n1.944739\n0.118037\n16475.741839\n1.153211\n\n\n14\nSpain\n18\n47,076,781\n$1,394,116,310,769\n1394.116311\n47.076781\n0.382354\n0.012911\n29613.671138\n0.607827\n\n\n15\nSweden\n11\n10,285,453\n$530,832,908,738\n530.832909\n10.285453\n1.069472\n0.020722\n51610.066055\n0.213137\n\n\n16\nKenya\n11\n52,573,973\n$95,503,088,538\n95.503089\n52.573973\n0.209229\n0.115180\n1816.546916\n6.055445\n\n\n17\nNorway\n8\n5,347,896\n$403,336,363,636\n403.336364\n5.347896\n1.495915\n0.019835\n75419.634869\n0.106073\n\n\n18\nRepublic of Ireland\n7\n5,007,069\n$388,698,711,348\n388.698711\n5.007069\n1.398023\n0.018009\n77629.988991\n0.090171\n\n\n19\nBrazil\n20\n212,559,417\n$1,839,758,040,766\n1839.758041\n212.559417\n0.094091\n0.010871\n8655.264804\n2.310732\n\n\n20\nIran\n12\n82,913,906\n$445,345,282,123\n445.345282\n82.913906\n0.144728\n0.026945\n5371.177208\n2.234147\n\n\n21\nUkraine\n12\n44,385,155\n$153,781,069,118\n153.781069\n44.385155\n0.270361\n0.078033\n3464.696003\n3.463507\n\n\n22\nRomania\n9\n19,356,544\n$250,077,444,017\n250.077444\n19.356544\n0.464959\n0.035989\n12919.529644\n0.696620\n\n\n23\nGeorgia\n7\n3,720,382\n$17,743,195,770\n17.743196\n3.720382\n1.881527\n0.394517\n4769.186543\n1.467756\n\n\n24\nBelgium\n10\n11,484,055\n$529,606,710,418\n529.606710\n11.484055\n0.870773\n0.018882\n46116.699234\n0.216841\n\n\n25\nBulgaria\n7\n6,975,761\n$86,000,000,000\n86.000000\n6.975761\n1.003475\n0.081395\n12328.404026\n0.567795\n\n\n26\nSerbia\n5\n6,944,975\n$51,409,167,351\n51.409167\n6.944975\n0.719945\n0.097259\n7402.354559\n0.675461\n\n\n27\nCzech Republic\n5\n10,669,709\n$246,489,245,495\n246.489245\n10.669709\n0.468616\n0.020285\n23101.777705\n0.216434\n\n\n28\nDenmark\n9\n5,818,553\n$348,078,018,464\n348.078018\n5.818553\n1.546776\n0.025856\n59822.092961\n0.150446\n\n\n29\nAzerbaijan\n7\n10,023,318\n$39,207,000,000\n39.207000\n10.023318\n0.698372\n0.178540\n3911.578980\n1.789559\n\n\n30\nCroatia\n7\n4,067,500\n$60,415,553,039\n60.415553\n4.067500\n1.720959\n0.115864\n14853.239837\n0.471278\n\n\n31\nCuba\n9\n11,333,483\n$100,023,000,000\n100.023000\n11.333483\n0.794107\n0.089979\n8825.442276\n1.019779\n\n\n32\nBahrain\n4\n1,501,635\n$38,574,069,149\n38.574069\n1.501635\n2.663763\n0.103697\n25688.046129\n0.155714\n\n\n33\nSlovenia\n3\n2,087,946\n$53,742,159,517\n53.742160\n2.087946\n1.436819\n0.055822\n25739.247814\n0.116554\n\n\n34\nChinese Taipei\n7\n23,196,178\n$791,610,000,000\n791.610000\n23.196178\n0.301774\n0.008843\n34126.742776\n0.205118\n\n\n35\nAustria\n5\n8,877,067\n$446,314,739,528\n446.314740\n8.877067\n0.563249\n0.011203\n50277.275087\n0.099449\n\n\n36\nHong Kong\n4\n7,346,000\n$359,800,000,000\n359.800000\n7.346000\n0.544514\n0.011117\n48979.036210\n0.081668\n\n\n37\nPhilippines\n4\n108,116,615\n$376,795,508,680\n376.795509\n108.116615\n0.036997\n0.010616\n3485.084218\n1.147748\n\n\n38\nAlgeria\n3\n43,053,054\n$169,988,236,398\n169.988236\n43.053054\n0.069681\n0.017648\n3948.343279\n0.759812\n\n\n39\nIndonesia\n3\n270,203,917\n$1,119,190,780,753\n1119.190781\n270.203917\n0.011103\n0.002681\n4142.022785\n0.724284\n\n\n40\nIsrael\n7\n9,053,300\n$395,098,666,122\n395.098666\n9.053300\n0.773199\n0.017717\n43641.397736\n0.160398\n\n\n41\nPoland\n10\n37,970,874\n$592,164,400,688\n592.164401\n37.970874\n0.263360\n0.016887\n15595.227033\n0.641222\n\n\n42\nKazakhstan\n7\n18,513,930\n$180,161,741,180\n180.161741\n18.513930\n0.378094\n0.038854\n9731.145207\n0.719340\n\n\n43\nJamaica\n6\n2,948,279\n$16,458,071,068\n16.458071\n2.948279\n2.035086\n0.364563\n5582.263778\n1.074833\n\n\n44\nSouth Africa\n6\n58,558,270\n$351,431,649,241\n351.431649\n58.558270\n0.102462\n0.017073\n6001.400814\n0.999767\n\n\n45\nThailand\n6\n69,625,582\n$543,649,976,166\n543.649976\n69.625582\n0.086175\n0.011037\n7808.192916\n0.768424\n\n\n46\nEthiopia\n4\n112,078,730\n$96,107,662,398\n96.107662\n112.078730\n0.035689\n0.041620\n857.501351\n4.664716\n\n\n47\nSwitzerland\n8\n8,574,832\n$703,082,435,360\n703.082435\n8.574832\n0.932963\n0.011378\n81993.727149\n0.097568\n\n\n48\nEcuador\n5\n17,373,662\n$107,435,665,000\n107.435665\n17.373662\n0.287792\n0.046539\n6183.823825\n0.808561\n\n\n49\nPortugal\n4\n10,269,417\n$237,686,075,635\n237.686076\n10.269417\n0.389506\n0.016829\n23145.040817\n0.172823\n\n\n\n\n\n\n\n\nsort_by_column(column_name=GDP_str)\n\n\n\n\n\n\n\n\nCountry\nTotal medals\n\n\n\n\n81\nArmenia\n4\n\n\n82\nMoldova\n4\n\n\n83\nKosovo\n2\n\n\n84\nKyrgyzstan\n6\n\n\n85\nTajikistan\n3\n\n\n86\nFiji\n1\n\n\n87\nSaint Lucia\n2\n\n\n88\nCape Verde\n1\n\n\n89\nGrenada\n2\n\n\n90\nDominica\n1\n\n\n\n\n\n\n\n\nsort_by_column(column_name=medals_per_million_people_str, column_name2=population_str)\n\n\n\n\n\n\n\n\nCountry\nTotal medals\nPopulation (in M)\n\n\n\n\n81\nUganda\n2\n44.27\n\n\n82\nMexico\n5\n126.01\n\n\n83\nIvory Coast\n1\n25.72\n\n\n84\nPhilippines\n4\n108.12\n\n\n85\nEthiopia\n4\n112.08\n\n\n86\nPeru\n1\n32.51\n\n\n87\nEgypt\n3\n100.39\n\n\n88\nIndonesia\n3\n270.20\n\n\n89\nPakistan\n1\n216.57\n\n\n90\nIndia\n6\n1366.42\n\n\n\n\n\n\n\n\nsort_by_column(column_name=medals_per_gdp, column_name2=GDP_str)\n\n\n\n\n\n\n\n\nCountry\nTotal medals\nGDP (in $B)\n\n\n\n\n81\nUnited States\n126\n21427.70\n\n\n82\nMalaysia\n2\n364.70\n\n\n83\nQatar\n1\n183.47\n\n\n84\nChina\n91\n19910.00\n\n\n85\nPeru\n1\n226.85\n\n\n86\nMexico\n5\n1258.29\n\n\n87\nPakistan\n1\n304.40\n\n\n88\nSingapore\n1\n372.06\n\n\n89\nIndonesia\n3\n1119.19\n\n\n90\nIndia\n6\n2611.00\n\n\n\n\n\n\n\n\nsort_by_column(column_name=medals_per_capita_gdp, column_name2=per_capita_gdp, column_name3=medals_per_capita_gdp)\n\n\n\n\n\n\n\n\nCountry\nTotal medals\nPer capita GDP (in $)\nTotal medals per capita GDP\n\n\n\n\n81\nAustria\n5\n50277.28\n0.10\n\n\n82\nSwitzerland\n8\n81993.73\n0.10\n\n\n83\nRepublic of Ireland\n7\n77629.99\n0.09\n\n\n84\nHong Kong\n4\n48979.04\n0.08\n\n\n85\nPanama\n1\n15731.02\n0.06\n\n\n86\nPuerto Rico\n2\n35195.53\n0.06\n\n\n87\nSlovakia\n1\n19329.10\n0.05\n\n\n88\nCyprus\n1\n20494.88\n0.05\n\n\n89\nQatar\n1\n64781.73\n0.02\n\n\n90\nSingapore\n1\n65233.28\n0.02"
  },
  {
    "objectID": "projects/transfer_learning.html",
    "href": "projects/transfer_learning.html",
    "title": "Transfer learning",
    "section": "",
    "text": "Set the environment (for Google Colab):\nIf this is GoogleColab we download config and use it to download necesseary folders and files for this project (defined in config.py). Here we’ll need: - pytorched.step_by_step.py - data_preparation.rps.py\n\n\nCode\ntry:\n    import google.colab\n    !pip install numpy pandas matplotlib torchviz scikit-learn tensorboard torchvision torch tqdm torch-lr-finder\n\n    import requests\n    url = 'https://raw.githubusercontent.com/nesaboz/pytorched/main/config.py'\n    r = requests.get(url, allow_redirects=True)\n    open('config.py', 'wb').write(r.content)    \nexcept ModuleNotFoundError:\n    print('Not Google Colab environment.')\n\n\nfrom config import config_project\nconfig_project('transfer_learning')\n\n\nWe are now ready for imports:\n\n\nCode\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nfrom torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop, InterpolationMode\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import alexnet, resnet18, inception_v3\nfrom torchvision.models.alexnet import model_urls\nfrom torchvision.models import Inception_V3_Weights, AlexNet_Weights\nfrom torch.hub import load_state_dict_from_url\nfrom torchviz import make_dot\n\nfrom shared.rps import download_rps\nfrom shared.step_by_step import StepByStep, freeze_model, print_trainable_parameters\n\nplt.style.use('fivethirtyeight')\n\n\n\nAlexNet\nLet’s try to use AlexNet model first to help us in the Rock-Paper-Scissors problem. We’ll need to load AlexNet, with it’s weights, then make a feature-extractor on our data loaders, modify the last layer, and train. Let’s get to it:\nWe can get AlexNet from torchvision.models:\n\nweights = AlexNet_Weights.IMAGENET1K_V1\nalex = alexnet(weights=weights)\nalex\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\nWe need an original transform:\n\ntransform = weights.transforms()\n\nLet’s now create new data loaders based on this transform:\n\n\nCode\ndownload_rps()\n\n\n\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\n\n\ntrain_dataset.classes\n\n['paper', 'rock', 'scissors']\n\n\n\ntrain_loader = DataLoader(train_dataset, 16, shuffle=True)\nval_loader = DataLoader(val_dataset, 16)\n\nLet’s also define an optimizer and loss:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(alex.parameters(), 3e-4)\nloss_fn = nn.CrossEntropyLoss()\nsbs = StepByStep(alex, optimizer, loss_fn)\nsbs.set_loaders(train_loader, val_loader)\n\n\n\nFeature-extractor\nThese are current trainable parameters:\n\nsbs.print_trainable_parameters()\n\nfeatures.0.weight\nfeatures.0.bias\nfeatures.3.weight\nfeatures.3.bias\nfeatures.6.weight\nfeatures.6.bias\nfeatures.8.weight\nfeatures.8.bias\nfeatures.10.weight\nfeatures.10.bias\nclassifier.1.weight\nclassifier.1.bias\nclassifier.4.weight\nclassifier.4.bias\nclassifier.6.weight\nclassifier.6.bias\n\n\nWe must freeze the model and replace the last layer. These are suggestions what layers to change (of course more layers can be left unfrozen but this requires more training data and longer times):\n\nso let’s change the last layer first to Identity to make feature extractor:\n\nsbs.model.classifier[6] = nn.Identity()\n\n\nfreeze_model(sbs.model)\n\n\nsbs.print_trainable_parameters()\n\nNo trainable parameters.\n\n\nWe now go throught the loader batch by batch and pass the data through the model in order to generate new preprocessed datasets:\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef preprocessed_dataset(model, loader, device=None):\n    \"\"\"\n    Runs all data in the loader through the model and returns a dataset.\n    \"\"\"\n    \n    features = torch.Tensor()\n    labels = torch.Tensor().type(torch.long)\n\n    if device is None:\n        device = next(model.parameters()).device\n\n    for i, (x_batch, y_batch) in enumerate(loader):\n        model.eval()\n        output = model(x_batch.to(device))\n        features = torch.cat([features, output.detach().cpu()])\n        labels = torch.cat([labels, y_batch.cpu()])\n\n    return TensorDataset(features, labels)\n\n\ndef test_preprocessed_dataset():\n    x = torch.rand(10,3,244,244)\n    y = torch.rand(10,1)\n    ds = TensorDataset(x,y)\n    dl = DataLoader(ds, 16, False)\n    tpp = preprocessed_dataset(alex, dl)\n    assert tpp.tensors[0].shape == torch.Size([10, 4096])\n\n\ntrain_preproc = preprocessed_dataset(alex, train_loader)\nval_preproc = preprocessed_dataset(alex, val_loader)\n\nmake sure that the tensort types are correct:\n\nassert next(iter(train_preproc))[0].type() == 'torch.FloatTensor'\nassert next(iter(train_preproc))[1].type() == 'torch.LongTensor'\n\nwe now build new DataLoaders:\n\nnew_train_loader = DataLoader(train_preproc, 16, True)\nnew_val_loader = DataLoader(val_preproc, 16)\n\n\n\nTop layer\nWith features extracted we can now create a brand new simple model using a fully-connected layer per table suggestions nn.Linear(4096, num_classes):\nLet’s create a new model:\n\ntorch.manual_seed(17)\ntop_layer = nn.Linear(4096, 3)\nmulti_loss_fn = nn.CrossEntropyLoss(reduction='mean')\noptimizer_top = optim.Adam(top_layer.parameters(), lr=3e-4)\nsbs_top = StepByStep(top_layer, optimizer_top, multi_loss_fn)\nsbs_top.set_loaders(new_train_loader, new_val_loader)\n\n\nsbs_top.train(10)\n\n100%|██████████| 10/10 [00:01&lt;00:00,  5.11it/s]\n\n\n\n_ = sbs_top.plot_losses()\n\n\n\n\n\n\n\n\n\nsbs_top.accuracy\n\n95.97\n\n\n\nsbs_top.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\nAnd this is pretty good, and very fast too!\nFor any new images that need to be evaluated thought, we will have to go through the whole model, so let’s insert this new top_layer into a sbs. Important: be very careful when changing the model layers AFTER creating StepByStep object. Model change that changes parameters must be reflected in optimizer as well (I added some check for this via sbs.check_consistency that checks number of parameters).\n\nsbs.model.classifier[6] = top_layer\nsbs.check_consistency()\nsbs.model.classifier\n\nSequential(\n  (0): Dropout(p=0.5, inplace=False)\n  (1): Linear(in_features=9216, out_features=4096, bias=True)\n  (2): ReLU(inplace=True)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=4096, out_features=4096, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=4096, out_features=3, bias=True)\n)\n\n\nLet’s evaluate on the old val_loader to see if we get the same result:\n\nsbs.accuracy_per_class\n\ntensor([[109, 124],\n        [124, 124],\n        [124, 124]])\n\n\n\nsbs.accuracy\n\n95.97\n\n\nyup, exactly the same.\n\n\nInception Model\nLet’s try Inception model. Inception has these 2 layers so we can not run the feature extraction, we have to use the full model. First we prep the data:\n\nweights = Inception_V3_Weights.IMAGENET1K_V1\ntransform = weights.transforms()\ntrain_dataset = ImageFolder(root='rps', transform=transform)\nval_dataset = ImageFolder(root='rps-test-set', transform=transform)\ntrain_loader = DataLoader(train_dataset, 16, True)\nval_loader = DataLoader(val_dataset, 16)\n\n\ntransform\n\nImageClassification(\n    crop_size=[299]\n    resize_size=[342]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\n\nnext we set the model and freeze it:\n\ninception = inception_v3(weights=weights)\n\n\ninception\n\nInception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\ninception = inception_v3(weights=weights)\nfreeze_model(inception)\nprint_trainable_parameters(inception)\n\nNo trainable parameters.\n\n\nreplace the top layers per advice:\n\ninception.fc = nn.Linear(2048, 3)\ninception.AuxLogits.fc = nn.Linear(768, 3)\nprint_trainable_parameters(inception)\n\nAuxLogits.fc.weight\nAuxLogits.fc.bias\nfc.weight\nfc.bias\n\n\nWe need to create special loss function that handles 2 output losses (one main and one auxilary) and combines them (with weight of 0.4 for auxilary):\n\ndef inception_loss(outputs, labels):\n    try:\n        main, aux = outputs  # this is a KEY difference from other models with single output\n    except ValueError:\n        main, aux = outputs, None  # this is a typical loss with no auxilairy layers\n        \n    main_loss = nn.CrossEntropyLoss()(main, labels)\n    aux_loss = nn.CrossEntropyLoss()(aux, labels) if aux is not None else 0\n    return main_loss + 0.4 * aux_loss\n\nWe are now ready to create sbs object:\n\ntorch.manual_seed(17)\noptimizer = optim.Adam(inception.parameters(), lr=3e-4)\nsbs_inception = StepByStep(inception, optimizer, inception_loss)\nsbs_inception.set_loaders(train_loader, val_loader)\n\nIt is training time:\n\nsbs_inception.train(10)\n\n100%|██████████| 10/10 [05:08&lt;00:00, 30.85s/it]\n\n\n\nprint(sbs_inception.accuracy_per_class)\nprint(sbs_inception.accuracy)\nsbs_inception.plot_losses()\n\ntensor([[112, 124],\n        [118, 124],\n        [113, 124]])\n92.2\n\n\n\n\n\n\n\n\n\n\n\nInception model from scratch\n\nscissors = Image.open('rps/scissors/scissors01-001.png')\nimage = ToTensor()(scissors)[:3, :, :].view(1, 3, 300, 300)\nweights = torch.tensor([0.2126, 0.7152, 0.0722]).view(1, 3, 1, 1)\nconvolved = F.conv2d(input=image, weight=weights)\nconverted = ToPILImage()(convolved[0])\n\ngrayscale = scissors.convert('L')\n\n\ngrayscale\n\n\n\n\n\n\n\n\nOne must define all layers in the __init__ in order to initialize them. Remeber that forward method will be called during training so no layers with parameters should be defined there:\n\nclass Inception(nn.Module):\n    def __init__(self, in_channels):\n        super(Inception, self).__init__()\n        # in_channels@HxW -&gt; 2@HxW\n        self.branch1x1_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n        # in_channels@HxW -&gt; 2@HxW -&gt; 3@HxW\n        self.branch5x5_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch5x5_2 = nn.Conv2d(2, 3, kernel_size=5, padding=2)\n\n        # in_channels@HxW -&gt; 2@HxW -&gt; 3@HxW\n        self.branch3x3_1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n        self.branch3x3_2 = nn.Conv2d(2, 3, kernel_size=3, padding=1)\n\n        # in_channels@HxW -&gt; in_channels@HxW -&gt; 1@HxW\n        self.branch_pool_1 = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch_pool_2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n\n    def forward(self, x):\n        # Produces 2 channels\n        branch1x1 = self.branch1x1_1(x)\n        # Produces 3 channels\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n        # Produces 3 channels\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n        # Produces 2 channels\n        branch_pool = self.branch_pool_1(x)\n        branch_pool = self.branch_pool_2(branch_pool)\n        # Concatenates all channels together (10)\n        outputs = torch.cat([branch1x1, branch5x5, branch3x3, branch_pool], 1)\n        return outputs\n\n\ninception = Inception(in_channels=3)\noutput = inception(image)\noutput.shape\n\ntorch.Size([1, 10, 300, 300])",
    "crumbs": [
      "Projects",
      "Transfer learning"
    ]
  },
  {
    "objectID": "projects/1_binary_image_classification.html",
    "href": "projects/1_binary_image_classification.html",
    "title": "Binary image classification",
    "section": "",
    "text": "Here we’ll take a problem of binary image classifing.\n\n\nCode\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;\"))\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom shared.step_by_step import StepByStep\nimport platform\nfrom PIL import Image\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n\nplt.style.use('fivethirtyeight')\n\n\n\n\nCode\ndef show_image(im, cmap=None):\n    fig = plt.figure(figsize=(3,3))\n    plt.imshow(im, cmap=cmap)\n    plt.grid(False)\n    plt.show()\n\n\n\nData\nWe’ll use generated data, where images with horizontal and vertical lines are considered label 0, while diagonal have label 1.\n\n\nCode\ndef gen_img(start, target, fill=1, img_size=10):\n    # Generates empty image\n    img = np.zeros((img_size, img_size), dtype=float)\n\n    start_row, start_col = None, None\n\n    if start &gt; 0:\n        start_row = start\n    else:\n        start_col = np.abs(start)\n\n    if target == 0:\n        if start_row is None:\n            img[:, start_col] = fill\n        else:\n            img[start_row, :] = fill\n    else:\n        if start_col == 0:\n            start_col = 1\n        \n        if target == 1:\n            if start_row is not None:\n                up = (range(start_row, -1, -1), \n                      range(0, start_row + 1))\n            else:\n                up = (range(img_size - 1, start_col - 1, -1), \n                      range(start_col, img_size))\n            img[up] = fill\n        else:\n            if start_row is not None:\n                down = (range(start_row, img_size, 1), \n                        range(0, img_size - start_row))\n            else:\n                down = (range(0, img_size - 1 - start_col + 1), \n                        range(start_col, img_size))\n            img[down] = fill\n    \n    return 255 * img.reshape(1, img_size, img_size)\n\n\ndef generate_dataset(img_size=10, n_images=100, binary=True, seed=17):\n    np.random.seed(seed)\n\n    starts = np.random.randint(-(img_size - 1), img_size, size=(n_images,))\n    targets = np.random.randint(0, 3, size=(n_images,))\n    \n    images = np.array([gen_img(s, t, img_size=img_size) \n                       for s, t in zip(starts, targets)], dtype=np.uint8)\n    \n    if binary:\n        targets = (targets &gt; 0).astype(int)\n    \n    return images, targets\n\ndef plot_images(images, targets, n_plot=30):\n    n_rows = n_plot // 6 + ((n_plot % 6) &gt; 0)\n    fig, axes = plt.subplots(n_rows, 6, figsize=(9, 1.5 * n_rows))\n    axes = np.atleast_2d(axes)\n    \n    for i, (image, target) in enumerate(zip(images[:n_plot], targets[:n_plot])):\n        row, col = i // 6, i % 6    \n        ax = axes[row, col]\n        ax.set_title('#{} - Label:{}'.format(i, target), {'size': 12})\n        # plot filter channel in grayscale\n        ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.label_outer()\n\n    plt.tight_layout()\n    return fig\n\n\n\nimages, labels = generate_dataset(img_size=5, n_images=300, binary=True, seed=13)\n\n\nfig = plot_images(images, labels, n_plot=30)\n\n\n\n\n\n\n\n\n\n\nData preparation\n\nx_tensor = torch.as_tensor(images / 255.).float()\ny_tensor = torch.as_tensor(labels.reshape(-1, 1)).float()  # reshaped this to (N,1) tensor\n\nPyTorch has Dataset class, TensorDataset as a subclass, and we can create custom subclasses too that can handle data augmentation:\n\nclass TransformedTensorDataset(Dataset):\n    def __init__(self, x, y, transform=None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.x[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, self.y[index]\n        \n    def __len__(self):\n        return len(self.x)\n\nA torch.utils.data.random_split method can split indices into train and valid (it requires exact number of images to split):\n\ntorch.manual_seed(13)\nN = len(x_tensor)\nn_train = int(.8*N)\nn_val = N - n_train\ntrain_subset, val_subset = random_split(x_tensor, [n_train, n_val])\ntrain_subset\n\n&lt;torch.utils.data.dataset.Subset&gt;\n\n\nwe just need indices:\n\ntrain_idx = train_subset.indices\nval_idx = val_subset.indices\n\n\ntrain_idx[:10]\n\n[118, 170, 148, 239, 226, 146, 168, 195, 6, 180]\n\n\n\n\nData augmentation\nFor data augmentation we only augment training data, so we create training and validation Composer:\n\ntrain_composer = Compose([RandomHorizontalFlip(p=.5),\n                          Normalize(mean=(.5,), std=(.5,))])\n\nval_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n\nNow we can build train/val tensors, Datasets and DataLoaders:\n\nx_train_tensor = x_tensor[train_idx]\ny_train_tensor = y_tensor[train_idx]\n\nx_val_tensor = x_tensor[val_idx]\ny_val_tensor = y_tensor[val_idx]\n\ntrain_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\nval_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n\nWe could stop here and just make loaders:\n\n# Builds a loader of each set\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\nor we can even used WeightedRandomSampler if we want to balance datasets:\n\ndef make_balanced_sampler(y):\n    # Computes weights for compensating imbalanced classes\n    classes, counts = y.unique(return_counts=True)\n    weights = 1.0 / counts.float()\n    sample_weights = weights[y.squeeze().long()]\n    # Builds sampler with compute weights\n    generator = torch.Generator()\n    sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        generator=generator,\n        replacement=True\n    )\n    return sampler\n\nNote that we don’t need a val_sampler anymore since we already split datasets:\n\ntrain_sampler = make_balanced_sampler(y_train_tensor)\n\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_sampler)\nval_loader = DataLoader(dataset=val_dataset, batch_size=16)\n\n\n\nLogistic Regression Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_logistic = nn.Sequential()\nmodel_logistic.add_module('flatten', nn.Flatten())\nmodel_logistic.add_module('output', nn.Linear(25, 1, bias=True))\nmodel_logistic.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_logistic = optim.SGD(model_logistic.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_logistic = StepByStep(model_logistic, optimizer_logistic, binary_loss_fn)\nsbs_logistic.set_seed()\nsbs_logistic.set_loaders(train_loader, val_loader)\nsbs_logistic.train(200)\n\nFailed to set loader seed.\n\n\n100%|██████████| 200/200 [00:08&lt;00:00, 23.24it/s]\n\n\n\nfig = sbs_logistic.plot_losses()\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [34, 36]])\n\n\nAfter 200 epoch it’s almost 100%. Let’s add 400 more:\n\nsbs_logistic.train(400)\n\n100%|██████████| 400/400 [00:13&lt;00:00, 29.00it/s]\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_logistic.loader_apply(sbs_logistic.val_loader, sbs_logistic.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nso after 600 epoch model is 100% accurate (at least on 60 samples).\n\n\nDeeper Model\n\nlr = 0.1\n\n# Now we can create a model\nmodel_deeper = nn.Sequential()\nmodel_deeper.add_module('flatten', nn.Flatten())\nmodel_deeper.add_module('linear1', nn.Linear(25, 10, bias=True))\nmodel_deeper.add_module('relu', nn.ReLU())\nmodel_deeper.add_module('linear2', nn.Linear(10, 1, bias=True))\nmodel_deeper.add_module('sigmoid', nn.Sigmoid())\n\n# Defines a SGD optimizer to update the parameters \noptimizer_deeper = optim.SGD(model_deeper.parameters(), lr=lr)\n\n# Defines a binary cross entropy loss function\nbinary_loss_fn = nn.BCELoss()\n\n\nsbs_deeper = StepByStep(model_deeper, optimizer_deeper, binary_loss_fn)\nsbs_deeper.set_seed()\nsbs_deeper.set_loaders(train_loader, val_loader)\nsbs_deeper.train(20)\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 28.33it/s]\n\n\n\nfig = sbs_deeper.plot_losses()\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Correct categories:')\nprint(sbs_deeper.loader_apply(sbs_deeper.val_loader, sbs_deeper.correct))\n\n\nCorrect categories:\ntensor([[24, 24],\n        [36, 36]])\n\n\nand even after 20 epoch it’s 100% accurate. We can train more to flatten the loss though which will surely generalize model:\n\nsbs_deeper.train(200)\nfig = sbs_deeper.plot_losses()\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06&lt;00:00, 30.26it/s]\n\n\n\n\n\n\n\n\n\nAnd that’s it.",
    "crumbs": [
      "Projects",
      "Binary image classification"
    ]
  },
  {
    "objectID": "tips.html",
    "href": "tips.html",
    "title": "TILs",
    "section": "",
    "text": "Some people might call it TILs (“today-I-learned”), cheat-sheet, utilities, what matters is that these saved me a ton of time, that is before LLMs took over the world.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 3, 2023\n\n\nKubernetes\n\n\n \n\n\n\n\nJan 26, 2023\n\n\nLabeling\n\n\n \n\n\n\n\n \n\n\nProject: Apriori Algorithm for Finding Frequent Itemsets with PySpark\n\n\n \n\n\n\n\n \n\n\n1. Create private AWS ECR repository\n\n\n \n\n\n\n\n \n\n\n4. IAM\n\n\n \n\n\n\n\n \n\n\nAWS Beanstalk\n\n\n \n\n\n\n\n \n\n\nAWS DocumentDB\n\n\n \n\n\n\n\n \n\n\nAWS Glue\n\n\n \n\n\n\n\n \n\n\nAWS Lambdas with layers and from containers\n\n\n \n\n\n\n\n \n\n\nAWS RDS\n\n\n \n\n\n\n\n \n\n\nAWS SQS, SNS, and Kafka\n\n\n \n\n\n\n\n \n\n\nAWS Serverless Application Model (SAM)\n\n\n \n\n\n\n\n \n\n\nAWS basics\n\n\n \n\n\n\n\n \n\n\nApplication Integration\n\n\n \n\n\n\n\n \n\n\nAugmentations\n\n\n \n\n\n\n\n \n\n\nAuto Scaling Group\n\n\n \n\n\n\n\n \n\n\nAzure\n\n\n \n\n\n\n\n \n\n\nBackpropagation\n\n\n \n\n\n\n\n \n\n\nBehavioral\n\n\n \n\n\n\n\n \n\n\nConvolutions\n\n\n \n\n\n\n\n \n\n\nCreate table and copy data from csv file\n\n\n \n\n\n\n\n \n\n\nCreating an Image Pipeline with EC2 Image Builder\n\n\n \n\n\n\n\n \n\n\nCreational\n\n\n \n\n\n\n\n \n\n\nCustom CSS to inject for making buttons bigger\n\n\n \n\n\n\n\n \n\n\nCustom domain\n\n\n \n\n\n\n\n \n\n\nData Engineer certificate\n\n\n \n\n\n\n\n \n\n\nDatetime\n\n\n \n\n\n\n\n \n\n\nDesign Elevator system\n\n\n \n\n\n\n\n \n\n\nDocker\n\n\n \n\n\n\n\n \n\n\nECS\n\n\n \n\n\n\n\n \n\n\nEnvironments\n\n\n \n\n\n\n\n \n\n\nExtract pages from PDF\n\n\n \n\n\n\n\n \n\n\nFastAI\n\n\n \n\n\n\n\n \n\n\nFeature Engineering\n\n\n \n\n\n\n\n \n\n\nFileIO\n\n\n \n\n\n\n\n \n\n\nFunctools\n\n\n \n\n\n\n\n \n\n\nGit\n\n\n \n\n\n\n\n \n\n\nGitHub Actions with access to AWS\n\n\n \n\n\n\n\n \n\n\nHost Web App\n\n\n \n\n\n\n\n \n\n\nHosting a static website via AWS EC2 and Lambda\n\n\n \n\n\n\n\n \n\n\nHuggingFace\n\n\n \n\n\n\n\n \n\n\nImages\n\n\n \n\n\n\n\n \n\n\nJupyter General\n\n\n \n\n\n\n\n \n\n\nKaggle\n\n\n \n\n\n\n\n \n\n\nKernels\n\n\n \n\n\n\n\n \n\n\nLatex\n\n\n \n\n\n\n\n \n\n\nLinear Regression\n\n\n \n\n\n\n\n \n\n\nLogging\n\n\n \n\n\n\n\n \n\n\nLoss\n\n\n \n\n\n\n\n \n\n\nMade with ML\n\n\n \n\n\n\n\n \n\n\nManage EC2 via CLI\n\n\n \n\n\n\n\n \n\n\nMatplotlib\n\n\n \n\n\n\n\n \n\n\nMetrics\n\n\n \n\n\n\n\n \n\n\nModels\n\n\n \n\n\n\n\n \n\n\nNumpy\n\n\n \n\n\n\n\n \n\n\nOS\n\n\n \n\n\n\n\n \n\n\nPIL\n\n\n \n\n\n\n\n \n\n\nPaperspace\n\n\n \n\n\n\n\n \n\n\nParser example\n\n\n \n\n\n\n\n \n\n\nPathlib\n\n\n \n\n\n\n\n \n\n\nPyData\n\n\n \n\n\n\n\n \n\n\nPyTest\n\n\n \n\n\n\n\n \n\n\nPyTorch Performance Tuning Guide\n\n\n \n\n\n\n\n \n\n\nPython General\n\n\n \n\n\n\n\n \n\n\nQuadratic function fitting\n\n\n \n\n\n\n\n \n\n\nRegex\n\n\n \n\n\n\n\n \n\n\nSQL examples\n\n\n \n\n\n\n\n \n\n\nSet operations\n\n\n \n\n\n\n\n \n\n\nStatistics\n\n\n \n\n\n\n\n \n\n\nStep Function project\n\n\n \n\n\n\n\n \n\n\nStream processing\n\n\n \n\n\n\n\n \n\n\nStructural\n\n\n \n\n\n\n\n \n\n\nTensor\n\n\n \n\n\n\n\n \n\n\nTerraform\n\n\n \n\n\n\n\n \n\n\nVS Code\n\n\n \n\n\n\n\n \n\n\nVideo capture and processing\n\n\n \n\n\n\n\n \n\n\nVisualizations\n\n\n \n\n\n\n\n \n\n\nWSL\n\n\n \n\n\n\n\n \n\n\ncopy module\n\n\n \n\n\n\n\n \n\n\nnbdev\n\n\n \n\n\n\n\n \n\n\nzsh\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "TILs"
    ]
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media",
    "section": "",
    "text": "Implemented in all BLI instruments, I co-invented software that, among other things, enabled COVID-19 antibody discovery: “Vanderbilt University delivered sequences of confirmed SARS-CoV-2 neutralizing antibodies to downstream manufacturing partners in just 18 days using this workflow. Two of these antibodies form the basis of AstraZeneca’s Evusheld antibody cocktail for the treatment of COVID-19, which was authorized for emergency use from 12/2021 to 1/2023​”.\n\nD. Thaker, M. Fowler, S. Nedungadi, D. V. Banda, B. Bruhn, N. Bozinovic, K. Mobilia, “Systems and methods for optimizing an instrument system workflow”, Patent No. US-11802883-B2 (2023) (submitted and published)."
  },
  {
    "objectID": "media.html#patents",
    "href": "media.html#patents",
    "title": "Media",
    "section": "",
    "text": "Implemented in all BLI instruments, I co-invented software that, among other things, enabled COVID-19 antibody discovery: “Vanderbilt University delivered sequences of confirmed SARS-CoV-2 neutralizing antibodies to downstream manufacturing partners in just 18 days using this workflow. Two of these antibodies form the basis of AstraZeneca’s Evusheld antibody cocktail for the treatment of COVID-19, which was authorized for emergency use from 12/2021 to 1/2023​”.\n\nD. Thaker, M. Fowler, S. Nedungadi, D. V. Banda, B. Bruhn, N. Bozinovic, K. Mobilia, “Systems and methods for optimizing an instrument system workflow”, Patent No. US-11802883-B2 (2023) (submitted and published)."
  },
  {
    "objectID": "media.html#publications",
    "href": "media.html#publications",
    "title": "Media",
    "section": "Publications",
    "text": "Publications\nMy publications span work from bio-microscopy to fiber-optics and optical networking. I designed complex prototypes as well as built robust systems that went to the bottom of the ocean.\nFor citations visit Google Scholar.\nFor paper downloads see below."
  },
  {
    "objectID": "media.html#press",
    "href": "media.html#press",
    "title": "Media",
    "section": "Press",
    "text": "Press\nTogether with Prof. Siddharth Ramachandran at Boston University I invented fiber-optic system based on orbital-angular-momentum multiplexing, following are few of the media releases:"
  },
  {
    "objectID": "media.html#publication-list",
    "href": "media.html#publication-list",
    "title": "Media",
    "section": "Publication list",
    "text": "Publication list\n\nN. Bozinovic, Y. Yue, Y. Ren, M. Tur, P. Kristensen, H. Huang, A. E. Willner, S. Ramachandran, “Terabit-Scale Orbital Angular Momentum Mode Division Multiplexing in Fibers”, Science, 28 June 2013: 340 (6140), 1545-1548. (link / download)\nN. Bozinovic, “Orbital Angular Momentum in Fibers”, PhD thesis, 2013. (download)\nJ. Wang, M.J. Padgett, S. Ramachandran, N. Bozinovic, S. Golowich, M.P.J. Lavery, H. Huang, Y. Yue, A.E. Willner “Multimode communications using OAM,” in Optical Fiber Telecommunications VI-B, I. Kaminow, T. Li, A.E. Willner, Ed., Academic Press, 2013. (link)\nY. Yue, N. Bozinovic, Y. Ren, H. Huang, M. Tur, P. Kristensen, S. Ramachandran, and A. E. Willner, “1.6-Tbit/s Muxing, Transmission and Demuxing through 1.1-km of Vortex Fiber Carrying 2 OAM Beams Each with 10 Wavelength Channels,” in Optical Fiber Communication Conference/National Fiber Optic Engineers Conference 2013, OSA Technical Digest, paper OTh4G.2. (link)\nN. Bozinovic, Y. Yue, Y. Ren, M. Tur, P. Kristensen, A. Willner, and S. Ramachandran, “Orbital Angular Momentum (OAM) Based Mode Division Multiplexing (MDM) over a Km-length Fiber,” in European Conference on Optical Communication, OSA Technical Digest, 2012, post-deadline paper Th.3.C.6. (link / download)\nN. Bozinovic, S. Golowich, P. Kristensen, and S. Ramachandran, “Control of orbital angular momentum of light with optical fibers,” Optics Letters 37, 2451-2453 (2012). (link / download)\nS. Ramachandran, N. Bozinovic, P. Gregg, S. Golowich, and P. Kristensen, “Optical vortices in fibres: A new degree of freedom for mode multiplexing,” in European Conference on Optical Communication, OSA Technical Digest, 2012, invited paper Tu.3.F.3. (link)\nS. Golowich, P. Kristensen, N. Bozinovic, P. Gregg, and S. Ramachandran, “Fibers Supporting Orbital Angular Momentum States for Information Capacity Scaling,” in Frontiers in Optics Conference, OSA Technical Digest, 2012, invited paper FW2D.2. (link)\nS. Golowich, N. Bozinovic, P. Kristensen, and S. Ramachandran, “Vortex Fiber Mode Amplitude Estimation,” in CLEO: Applications and Technology, OSA Technical Digest, 2012, paper JTu2K.2. (link)\nN. Bozinovic, S. Ramachandran, M. Brodsky, and P. Kristensen, “Record-length transmission of entangled photons with orbital angular momentum (vortices),” in Frontiers in Optics, OSA Technical Digest, 2011, post-deadline paper PDPB1. (link) (download)\nN. Bozinovic, P. Kristensen, and S. Ramachandran, “Long-range fiber-transmission of photons with orbital angular momentum,” in CLEO:2011 - Laser Applications to Photonic Applications, OSA Technical Digest , 2011, paper CTuB1. (link / download)\nN. Bozinovic, P. Kristensen, and S. Ramachandran, “Are Orbital Angular Momentum (OAM/Vortex) States of Light Long-Lived in Fibers?,” in Frontiers in Optics/Laser Science, OSA Technical Digest, 2011, paper LWL3. (link)\nS. Santos, K. Chu, D. Lim, N. Bozinovic, T. Ford, C. Hourtoule, A. C. Bartoo, S. K. Singh, J. Mertz, Optically sectioned fluorescence endomicroscopy with hybrid-illumination imaging through a flexible fiber bundle”, J. Biomed. Opt. 14, 030502, (2009) (link)\nS. Santos, K. Chu, D. Lim, N. Bozinovic, T. Ford, C. Hourtoule, A. Bartoo, S. Singh, and J. Mertz, “Optically Sectioned Fluorescence Endomicroscopy with Hybrid-Illumination Imaging through a Flexible Fiber Bundle,” in Novel Techniques in Microscopy, OSA Technical Digest, 2009, paper NWC3. (link)\nN. Bozinovic, C. Ventalon, T. Ford, and J. Mertz, “Fluorescence endomicroscopy with structured illumination,” Opt. Express 16, 8016-8025 (2008). (link)\nN. Bozinovic, C. Ventalon, T. Ford, and J. Mertz, “Fluorescence Endomicroscopy with Out-of-Focus Background Rejection,” in Biomedical Optics, OSA Technical Digest, 2008, paper BTuF57. (link / download)"
  },
  {
    "objectID": "tips/DesignPatterns/creational.html",
    "href": "tips/DesignPatterns/creational.html",
    "title": "Creational",
    "section": "",
    "text": "Creational patterns provide object creation mechanisms that increase flexibility and reuse of existing code.\nTaken fully from refactoring.guru, head there for a more detailed explanation. No code was written by me, I am just able to execute it here.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/creational.html#factory-method",
    "href": "tips/DesignPatterns/creational.html#factory-method",
    "title": "Creational",
    "section": "Factory Method",
    "text": "Factory Method\nThe factory method provides interface in a superclass, but allows subclasses to deal with flavors. It is usually responsible for creating a single product. \n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\n\nclass Creator(ABC):\n    \"\"\"\n    The Creator class declares the factory method that is supposed to return an\n    object of a Product class. The Creator's subclasses usually provide the\n    implementation of this method.\n    \"\"\"\n\n    @abstractmethod\n    def factory_method(self):\n        \"\"\"\n        Note that the Creator may also provide some default implementation of\n        the factory method.\n        \"\"\"\n        pass\n\n    def some_operation(self) -&gt; str:\n        \"\"\"\n        Also note that, despite its name, the Creator's primary responsibility\n        is not creating products. Usually, it contains some core business logic\n        that relies on Product objects, returned by the factory method.\n        Subclasses can indirectly change that business logic by overriding the\n        factory method and returning a different type of product from it.\n        \"\"\"\n\n        # Call the factory method to create a Product object.\n        product = self.factory_method()    # KEY POINT: product is created dynamically\n\n        # Now, use the product.\n        result = f\"Creator: The same creator's code has just worked with {product.operation()}\"\n\n        return result\n\n\n\"\"\"\nConcrete Creators override the factory method in order to change the resulting\nproduct's type.\n\"\"\"\n\n\nclass ConcreteCreator1(Creator):\n    \"\"\"\n    Note that the signature of the method still uses the abstract product type,\n    even though the concrete product is actually returned from the method. This\n    way the Creator can stay independent of concrete product classes.\n    \"\"\"\n\n    def factory_method(self) -&gt; Product:\n        return ConcreteProduct1()   \n\n\nclass ConcreteCreator2(Creator):\n    def factory_method(self) -&gt; Product:\n        return ConcreteProduct2()\n\n\nclass Product(ABC):\n    \"\"\"\n    The Product interface declares the operations that all concrete products\n    must implement.\n    \"\"\"\n\n    @abstractmethod\n    def operation(self) -&gt; str:\n        pass\n\n\n\"\"\"\nConcrete Products provide various implementations of the Product interface.\n\"\"\"\n\n\nclass ConcreteProduct1(Product):\n    def operation(self) -&gt; str:\n        return \"{Result of the ConcreteProduct1}\"\n\n\nclass ConcreteProduct2(Product):\n    def operation(self) -&gt; str:\n        return \"{Result of the ConcreteProduct2}\"\n\n\ndef client_code(creator: Creator) -&gt; None:\n    \"\"\"\n    The client code works with an instance of a concrete creator, albeit through\n    its base interface. As long as the client keeps working with the creator via\n    the base interface, you can pass it any creator's subclass.\n    \"\"\"\n\n    print(f\"Client: I'm not aware of the creator's class, but it still works.\\n\"\n          f\"{creator.some_operation()}\", end=\"\")\n\n\nif __name__ == \"__main__\":\n    print(\"App: Launched with the ConcreteCreator1.\")\n    client_code(ConcreteCreator1())\n    print(\"\\n\")\n\n    print(\"App: Launched with the ConcreteCreator2.\")\n    client_code(ConcreteCreator2())\n\nApp: Launched with the ConcreteCreator1.\nClient: I'm not aware of the creator's class, but it still works.\nCreator: The same creator's code has just worked with {Result of the ConcreteProduct1}\n\nApp: Launched with the ConcreteCreator2.\nClient: I'm not aware of the creator's class, but it still works.\nCreator: The same creator's code has just worked with {Result of the ConcreteProduct2}",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/creational.html#abstract-factory",
    "href": "tips/DesignPatterns/creational.html#abstract-factory",
    "title": "Creational",
    "section": "Abstract Factory",
    "text": "Abstract Factory\nAbstract factory pattern is expansion of Factory method, but it creates entire families of related products.\nFor example there are products (A, B, …) and flavor factories (1, 2, …). Client code only deals with abstract factory.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\n\nclass AbstractFactory(ABC):\n    \"\"\"\n    The Abstract Factory interface declares a set of methods that return\n    different abstract products. These products are called a family and are\n    related by a high-level theme or concept. Products of one family are usually\n    able to collaborate among themselves. A family of products may have several\n    variants, but the products of one variant are incompatible with products of\n    another.\n    \"\"\"\n    @abstractmethod\n    def create_product_a(self) -&gt; AbstractProductA:\n        pass\n\n    @abstractmethod\n    def create_product_b(self) -&gt; AbstractProductB:\n        pass\n\n\nclass ConcreteFactory1(AbstractFactory):\n    \"\"\"\n    Concrete Factories produce a family of products that belong to a single\n    variant. The factory guarantees that resulting products are compatible. Note\n    that signatures of the Concrete Factory's methods return an abstract\n    product, while inside the method a concrete product is instantiated.\n    \"\"\"\n\n    def create_product_a(self) -&gt; AbstractProductA:\n        return ConcreteProductA1()\n\n    def create_product_b(self) -&gt; AbstractProductB:\n        return ConcreteProductB1()\n\n\nclass ConcreteFactory2(AbstractFactory):\n    \"\"\"\n    Each Concrete Factory has a corresponding product variant.\n    \"\"\"\n\n    def create_product_a(self) -&gt; AbstractProductA:\n        return ConcreteProductA2()\n\n    def create_product_b(self) -&gt; AbstractProductB:\n        return ConcreteProductB2()\n\n\nclass AbstractProductA(ABC):\n    \"\"\"\n    Each distinct product of a product family should have a base interface. All\n    variants of the product must implement this interface.\n    \"\"\"\n\n    @abstractmethod\n    def useful_function_a(self) -&gt; str:\n        pass\n\n\n\"\"\"\nConcrete Products are created by corresponding Concrete Factories.\n\"\"\"\n\n\nclass ConcreteProductA1(AbstractProductA):\n    def useful_function_a(self) -&gt; str:\n        return \"The result of the product A1.\"\n\n\nclass ConcreteProductA2(AbstractProductA):\n    def useful_function_a(self) -&gt; str:\n        return \"The result of the product A2.\"\n\n\nclass AbstractProductB(ABC):\n    \"\"\"\n    Here's the the base interface of another product. All products can interact\n    with each other, but proper interaction is possible only between products of\n    the same concrete variant.\n    \"\"\"\n    @abstractmethod\n    def useful_function_b(self) -&gt; None:\n        \"\"\"\n        Product B is able to do its own thing...\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def another_useful_function_b(self, collaborator: AbstractProductA) -&gt; None:\n        \"\"\"\n        ...but it also can collaborate with the ProductA.\n\n        The Abstract Factory makes sure that all products it creates are of the\n        same variant and thus, compatible.\n        \"\"\"\n        pass\n\n\n\"\"\"\nConcrete Products are created by corresponding Concrete Factories.\n\"\"\"\n\n\nclass ConcreteProductB1(AbstractProductB):\n    def useful_function_b(self) -&gt; str:\n        return \"The result of the product B1.\"\n\n    \"\"\"\n    The variant, Product B1, is only able to work correctly with the variant,\n    Product A1. Nevertheless, it accepts any instance of AbstractProductA as an\n    argument.\n    \"\"\"\n\n    def another_useful_function_b(self, collaborator: AbstractProductA) -&gt; str:\n        result = collaborator.useful_function_a()\n        return f\"The result of the B1 collaborating with the ({result})\"\n\n\nclass ConcreteProductB2(AbstractProductB):\n    def useful_function_b(self) -&gt; str:\n        return \"The result of the product B2.\"\n\n    def another_useful_function_b(self, collaborator: AbstractProductA):\n        \"\"\"\n        The variant, Product B2, is only able to work correctly with the\n        variant, Product A2. Nevertheless, it accepts any instance of\n        AbstractProductA as an argument.\n        \"\"\"\n        result = collaborator.useful_function_a()\n        return f\"The result of the B2 collaborating with the ({result})\"\n\n\ndef client_code(factory: AbstractFactory) -&gt; None:\n    \"\"\"\n    The client code works with factories and products only through abstract\n    types: AbstractFactory and AbstractProduct. This lets you pass any factory\n    or product subclass to the client code without breaking it.\n    \"\"\"\n    product_a = factory.create_product_a()\n    product_b = factory.create_product_b()\n\n    print(f\"{product_b.useful_function_b()}\")\n    print(f\"{product_b.another_useful_function_b(product_a)}\", end=\"\")\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    The client code can work with any concrete factory class.\n    \"\"\"\n    print(\"Client: Testing client code with the first factory type:\")\n    client_code(ConcreteFactory1())   # KEY POINT\n\n    print(\"\\n\")\n\n    print(\"Client: Testing the same client code with the second factory type:\")\n    client_code(ConcreteFactory2())\n\nClient: Testing client code with the first factory type:\nThe result of the product B1.\nThe result of the B1 collaborating with the (The result of the product A1.)\n\nClient: Testing the same client code with the second factory type:\nThe result of the product B2.\nThe result of the B2 collaborating with the (The result of the product A2.)",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/creational.html#builder",
    "href": "tips/DesignPatterns/creational.html#builder",
    "title": "Creational",
    "section": "Builder",
    "text": "Builder\nThe builder pattern might seem similar to the abstract factory pattern but it creates an object step by step whereas the abstract factory pattern returns the object in one go.\n\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\n\nclass Builder(ABC):\n    \"\"\"\n    The Builder interface specifies methods for creating the different parts of\n    the Product objects.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def product(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def produce_part_a(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def produce_part_b(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def produce_part_c(self) -&gt; None:\n        pass\n\n\nclass ConcreteBuilder1(Builder):\n    \"\"\"\n    The Concrete Builder classes follow the Builder interface and provide\n    specific implementations of the building steps. Your program may have\n    several variations of Builders, implemented differently.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        A fresh builder instance should contain a blank product object, which is\n        used in further assembly.\n        \"\"\"\n        self.reset()\n\n    def reset(self) -&gt; None:\n        self._product = Product1()\n\n    @property\n    def product(self) -&gt; Product1:\n        \"\"\"\n        Concrete Builders are supposed to provide their own methods for\n        retrieving results. That's because various types of builders may create\n        entirely different products that don't follow the same interface.\n        Therefore, such methods cannot be declared in the base Builder interface\n        (at least in a statically typed programming language).\n\n        Usually, after returning the end result to the client, a builder\n        instance is expected to be ready to start producing another product.\n        That's why it's a usual practice to call the reset method at the end of\n        the `getProduct` method body. However, this behavior is not mandatory,\n        and you can make your builders wait for an explicit reset call from the\n        client code before disposing of the previous result.\n        \"\"\"\n        product = self._product\n        self.reset()\n        return product\n\n    def produce_part_a(self) -&gt; None:\n        self._product.add(\"PartA1\")\n\n    def produce_part_b(self) -&gt; None:\n        self._product.add(\"PartB1\")\n\n    def produce_part_c(self) -&gt; None:\n        self._product.add(\"PartC1\")\n\n\nclass Product1():\n    \"\"\"\n    It makes sense to use the Builder pattern only when your products are quite\n    complex and require extensive configuration.\n\n    Unlike in other creational patterns, different concrete builders can produce\n    unrelated products. In other words, results of various builders may not\n    always follow the same interface.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.parts = []\n\n    def add(self, part: Any) -&gt; None:\n        self.parts.append(part)\n\n    def list_parts(self) -&gt; None:\n        print(f\"Product parts: {', '.join(self.parts)}\", end=\"\")\n\n\nclass Director:\n    \"\"\"\n    The Director is only responsible for executing the building steps in a\n    particular sequence. It is helpful when producing products according to a\n    specific order or configuration. Strictly speaking, the Director class is\n    optional, since the client can control builders directly.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._builder = None\n\n    @property\n    def builder(self) -&gt; Builder:\n        return self._builder\n\n    @builder.setter\n    def builder(self, builder: Builder) -&gt; None:\n        \"\"\"\n        The Director works with any builder instance that the client code passes\n        to it. This way, the client code may alter the final type of the newly\n        assembled product.\n        \"\"\"\n        self._builder = builder\n\n    \"\"\"\n    The Director can construct several product variations using the same\n    building steps.\n    \"\"\"\n\n    def build_minimal_viable_product(self) -&gt; None:\n        self.builder.produce_part_a()\n\n    def build_full_featured_product(self) -&gt; None:   # KEY POINT\n        self.builder.produce_part_a()\n        self.builder.produce_part_b()\n        self.builder.produce_part_c()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    The client code creates a builder object, passes it to the director and then\n    initiates the construction process. The end result is retrieved from the\n    builder object.\n    \"\"\"\n\n    director = Director()\n    builder = ConcreteBuilder1()\n    director.builder = builder\n\n    print(\"Standard basic product: \")\n    director.build_minimal_viable_product()\n    builder.product.list_parts()\n\n    print(\"\\n\")\n\n    print(\"Standard full featured product: \")\n    director.build_full_featured_product()\n    builder.product.list_parts()\n\n    print(\"\\n\")\n\n    # Remember, the Builder pattern can be used without a Director class.\n    print(\"Custom product: \")\n    builder.produce_part_a()\n    builder.produce_part_b()\n    builder.product.list_parts()\n\nStandard basic product: \nProduct parts: PartA1\n\nStandard full featured product: \nProduct parts: PartA1, PartB1, PartC1\n\nCustom product: \nProduct parts: PartA1, PartB1",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/creational.html#prototype",
    "href": "tips/DesignPatterns/creational.html#prototype",
    "title": "Creational",
    "section": "Prototype",
    "text": "Prototype\nThe prototype is used when we want to copy/clone the object. It python this is done using copy.copy (for shallow copy, i.e. objects are passed by reference) and copy.deepcopy (new object are created).\n\n\nimport copy\n\n\nclass SelfReferencingEntity:\n    def __init__(self):\n        self.parent = None\n\n    def set_parent(self, parent):\n        self.parent = parent\n\n\nclass SomeComponent:\n    \"\"\"\n    Python provides its own interface of Prototype via `copy.copy` and\n    `copy.deepcopy` functions. And any class that wants to implement custom\n    implementations have to override `__copy__` and `__deepcopy__` member   # KEY POINT\n    functions.\n    \"\"\"\n\n    def __init__(self, some_int, some_list_of_objects, some_circular_ref):\n        self.some_int = some_int\n        self.some_list_of_objects = some_list_of_objects\n        self.some_circular_ref = some_circular_ref\n\n    def __copy__(self):\n        \"\"\"\n        Create a shallow copy. This method will be called whenever someone calls\n        `copy.copy` with this object and the returned value is returned as the\n        new shallow copy.\n        \"\"\"\n\n        # First, let's create copies of the nested objects.\n        some_list_of_objects = copy.copy(self.some_list_of_objects)\n        some_circular_ref = copy.copy(self.some_circular_ref)\n\n        # Then, let's clone the object itself, using the prepared clones of the\n        # nested objects.\n        new = self.__class__(\n            self.some_int, some_list_of_objects, some_circular_ref\n        )\n        new.__dict__.update(self.__dict__)\n\n        return new\n\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Create a deep copy. This method will be called whenever someone calls\n        `copy.deepcopy` with this object and the returned value is returned as\n        the new deep copy.\n\n        What is the use of the argument `memo`? Memo is the dictionary that is\n        used by the `deepcopy` library to prevent infinite recursive copies in\n        instances of circular references. Pass it to all the `deepcopy` calls\n        you make in the `__deepcopy__` implementation to prevent infinite\n        recursions.\n        \"\"\"\n        if memo is None:\n            memo = {}\n\n        # First, let's create copies of the nested objects.\n        some_list_of_objects = copy.deepcopy(self.some_list_of_objects, memo)\n        some_circular_ref = copy.deepcopy(self.some_circular_ref, memo)\n\n        # Then, let's clone the object itself, using the prepared clones of the\n        # nested objects.\n        new = self.__class__(\n            self.some_int, some_list_of_objects, some_circular_ref\n        )\n        new.__dict__ = copy.deepcopy(self.__dict__, memo)\n\n        return new\n\n\nif __name__ == \"__main__\":\n\n    list_of_objects = [1, {1, 2, 3}, [1, 2, 3]]\n    circular_ref = SelfReferencingEntity()\n    component = SomeComponent(23, list_of_objects, circular_ref)\n    circular_ref.set_parent(component)\n\n    shallow_copied_component = copy.copy(component)\n\n    # Let's change the list in shallow_copied_component and see if it changes in\n    # component.\n    shallow_copied_component.some_list_of_objects.append(\"another object\")\n    if component.some_list_of_objects[-1] == \"another object\":\n        print(\n            \"Adding elements to `shallow_copied_component`'s \"\n            \"some_list_of_objects adds it to `component`'s \"\n            \"some_list_of_objects.\"\n        )\n    else:\n        print(\n            \"Adding elements to `shallow_copied_component`'s \"\n            \"some_list_of_objects doesn't add it to `component`'s \"\n            \"some_list_of_objects.\"\n        )\n\n    # Let's change the set in the list of objects.\n    component.some_list_of_objects[1].add(4)\n    if 4 in shallow_copied_component.some_list_of_objects[1]:\n        print(\n            \"Changing objects in the `component`'s some_list_of_objects \"\n            \"changes that object in `shallow_copied_component`'s \"\n            \"some_list_of_objects.\"\n        )\n    else:\n        print(\n            \"Changing objects in the `component`'s some_list_of_objects \"\n            \"doesn't change that object in `shallow_copied_component`'s \"\n            \"some_list_of_objects.\"\n        )\n\n    deep_copied_component = copy.deepcopy(component)\n\n    # Let's change the list in deep_copied_component and see if it changes in\n    # component.\n    deep_copied_component.some_list_of_objects.append(\"one more object\")\n    if component.some_list_of_objects[-1] == \"one more object\":\n        print(\n            \"Adding elements to `deep_copied_component`'s \"\n            \"some_list_of_objects adds it to `component`'s \"\n            \"some_list_of_objects.\"\n        )\n    else:\n        print(\n            \"Adding elements to `deep_copied_component`'s \"\n            \"some_list_of_objects doesn't add it to `component`'s \"\n            \"some_list_of_objects.\"\n        )\n\n    # Let's change the set in the list of objects.\n    component.some_list_of_objects[1].add(10)\n    if 10 in deep_copied_component.some_list_of_objects[1]:\n        print(\n            \"Changing objects in the `component`'s some_list_of_objects \"\n            \"changes that object in `deep_copied_component`'s \"\n            \"some_list_of_objects.\"\n        )\n    else:\n        print(\n            \"Changing objects in the `component`'s some_list_of_objects \"\n            \"doesn't change that object in `deep_copied_component`'s \"\n            \"some_list_of_objects.\"\n        )\n\n    print(\n        f\"id(deep_copied_component.some_circular_ref.parent): \"\n        f\"{id(deep_copied_component.some_circular_ref.parent)}\"\n    )\n    print(\n        f\"id(deep_copied_component.some_circular_ref.parent.some_circular_ref.parent): \"\n        f\"{id(deep_copied_component.some_circular_ref.parent.some_circular_ref.parent)}\"\n    )\n    print(\n        \"^^ This shows that deepcopied objects contain same reference, they \"\n        \"are not cloned repeatedly.\"\n    )\n\nAdding elements to `shallow_copied_component`'s some_list_of_objects adds it to `component`'s some_list_of_objects.\nChanging objects in the `component`'s some_list_of_objects changes that object in `shallow_copied_component`'s some_list_of_objects.\nAdding elements to `deep_copied_component`'s some_list_of_objects doesn't add it to `component`'s some_list_of_objects.\nChanging objects in the `component`'s some_list_of_objects doesn't change that object in `deep_copied_component`'s some_list_of_objects.\nid(deep_copied_component.some_circular_ref.parent): 4532238608\nid(deep_copied_component.some_circular_ref.parent.some_circular_ref.parent): 4532238608\n^^ This shows that deepcopied objects contain same reference, they are not cloned repeatedly.",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/DesignPatterns/creational.html#singleton-thread-safe",
    "href": "tips/DesignPatterns/creational.html#singleton-thread-safe",
    "title": "Creational",
    "section": "Singleton (thread-safe)",
    "text": "Singleton (thread-safe)\nWhen we want to have only one instance of the object.\n\n\nfrom threading import Lock, Thread\n\n\nclass SingletonMeta(type):\n    \"\"\"\n    This is a thread-safe implementation of Singleton.\n    \"\"\"\n\n    _instances = {}\n\n    _lock: Lock = Lock()\n    \"\"\"\n    We now have a lock object that will be used to synchronize threads during\n    first access to the Singleton.\n    \"\"\"\n\n    def __call__(cls, *args, **kwargs):\n        \"\"\"\n        Possible changes to the value of the `__init__` argument do not affect\n        the returned instance.\n        \"\"\"\n        # Now, imagine that the program has just been launched. Since there's no\n        # Singleton instance yet, multiple threads can simultaneously pass the\n        # previous conditional and reach this point almost at the same time. The\n        # first of them will acquire lock and will proceed further, while the\n        # rest will wait here.\n        with cls._lock:\n            # The first thread to acquire the lock, reaches this conditional,\n            # goes inside and creates the Singleton instance. Once it leaves the\n            # lock block, a thread that might have been waiting for the lock\n            # release may then enter this section. But since the Singleton field\n            # is already initialized, the thread won't create a new object.\n            if cls not in cls._instances:   # KEY POINT\n                instance = super().__call__(*args, **kwargs)\n                cls._instances[cls] = instance\n        return cls._instances[cls]\n\n\nclass Singleton(metaclass=SingletonMeta):\n    value: str = None\n    \"\"\"\n    We'll use this property to prove that our Singleton really works.\n    \"\"\"\n\n    def __init__(self, value: str) -&gt; None:\n        self.value = value\n\n    def some_business_logic(self):\n        \"\"\"\n        Finally, any singleton should define some business logic, which can be\n        executed on its instance.\n        \"\"\"\n\n\ndef test_singleton(value: str) -&gt; None:\n    singleton = Singleton(value)\n    print(singleton.value)\n\n\nif __name__ == \"__main__\":\n    # The client code.\n\n    print(\"If you see the same value, then singleton was reused (yay!)\\n\"\n          \"If you see different values, \"\n          \"then 2 singletons were created (booo!!)\\n\\n\"\n          \"RESULT:\\n\")\n\n    process1 = Thread(target=test_singleton, args=(\"FOO\",))\n    process2 = Thread(target=test_singleton, args=(\"BAR\",))\n    process1.start()\n    process2.start()\n\nIf you see the same value, then singleton was reused (yay!)\nIf you see different values, then 2 singletons were created (booo!!)\n\nRESULT:\n\nFOO\nFOO",
    "crumbs": [
      "Projects",
      "TILs",
      "Design Patterns",
      "Creational"
    ]
  },
  {
    "objectID": "tips/MLOps/made_with_ml.html",
    "href": "tips/MLOps/made_with_ml.html",
    "title": "Made with ML",
    "section": "",
    "text": "For our multi-class task (where each project has exactly one tag), we want to ensure that the data splits have similar class distributions. We can achieve this by specifying how to stratify the split by using the stratify keyword argument with sklearn’s train_test_split() function.\ntrain_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=1234)"
  },
  {
    "objectID": "tips/MLOps/made_with_ml.html#stratify",
    "href": "tips/MLOps/made_with_ml.html#stratify",
    "title": "Made with ML",
    "section": "",
    "text": "For our multi-class task (where each project has exactly one tag), we want to ensure that the data splits have similar class distributions. We can achieve this by specifying how to stratify the split by using the stratify keyword argument with sklearn’s train_test_split() function.\ntrain_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=1234)"
  },
  {
    "objectID": "tips/MLOps/made_with_ml.html#iterate-on-data",
    "href": "tips/MLOps/made_with_ml.html#iterate-on-data",
    "title": "Made with ML",
    "section": "Iterate on data",
    "text": "Iterate on data\nInstead of using a fixed dataset and iterating on the models, we could keep the model constant and iterate on the dataset. This is useful to improve the quality of our datasets.\n\nremove or fix data samples (false positives & negatives)\nprepare and transform features\nexpand or consolidate classes\nincorporate auxiliary datasets\nidentify unique slices to boost"
  },
  {
    "objectID": "tips/MLOps/made_with_ml.html#embeddings",
    "href": "tips/MLOps/made_with_ml.html#embeddings",
    "title": "Made with ML",
    "section": "Embeddings",
    "text": "Embeddings\nThe main idea of embeddings is to have fixed length representations for the tokens in a text regardless of the number of tokens in the vocabulary. With one-hot encoding, each token is represented by an array of size vocab_size, but with embeddings, each token now has the shape embed_dim. The values in the representation will are not fixed binary values but rather, changing floating points allowing for fine-grained learned representations."
  },
  {
    "objectID": "tips/060_Misc/latex.html",
    "href": "tips/060_Misc/latex.html",
    "title": "Latex",
    "section": "",
    "text": "New line \\\\\nURL link \\href{link}{label}\nTo have hyperlinks available one must have proper library installed (use external TypeSetter like MacTeX). Also have this line in a template file \\usepackage[hidelinks, linkcolor=blue]{hyperref} to hide the default boxes around links in PDFs.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Latex"
    ]
  },
  {
    "objectID": "tips/060_Misc/vscode.html",
    "href": "tips/060_Misc/vscode.html",
    "title": "VS Code",
    "section": "",
    "text": "add breakpoint: F9\n\nrerun last test: Command + ; + L\nstep next: F10\n\ninto: F11\n\nout: Shift + F11\n\ncontinue: F5\n\nstop: Shift F5\n\nevaluate in debug console: Command + E + C # my shortcut",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "VS Code"
    ]
  },
  {
    "objectID": "tips/060_Misc/vscode.html#debugging-shortcuts",
    "href": "tips/060_Misc/vscode.html#debugging-shortcuts",
    "title": "VS Code",
    "section": "",
    "text": "add breakpoint: F9\n\nrerun last test: Command + ; + L\nstep next: F10\n\ninto: F11\n\nout: Shift + F11\n\ncontinue: F5\n\nstop: Shift F5\n\nevaluate in debug console: Command + E + C # my shortcut",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "VS Code"
    ]
  },
  {
    "objectID": "tips/060_Misc/sql/setup_sql.html",
    "href": "tips/060_Misc/sql/setup_sql.html",
    "title": "Create table and copy data from csv file",
    "section": "",
    "text": "brew install mysql\nbrew services start mysql\nbrew services stop mysql\nbrew services list\nmysql_secure_installation\nmysql --version\nmysql -u root -p\nPut this into a file sudo nano /etc/my.cnf:\n[client]\nlocal_infile=1\n\n[mysqld]\nlocal_infile=1\nbrew services restart mysql\nCREATE DATABASE database_name;"
  },
  {
    "objectID": "tips/060_Misc/sql/setup_sql.html#setup-mysql-database",
    "href": "tips/060_Misc/sql/setup_sql.html#setup-mysql-database",
    "title": "Create table and copy data from csv file",
    "section": "",
    "text": "brew install mysql\nbrew services start mysql\nbrew services stop mysql\nbrew services list\nmysql_secure_installation\nmysql --version\nmysql -u root -p\nPut this into a file sudo nano /etc/my.cnf:\n[client]\nlocal_infile=1\n\n[mysqld]\nlocal_infile=1\nbrew services restart mysql\nCREATE DATABASE database_name;"
  },
  {
    "objectID": "tips/060_Misc/sql/setup_sql.html#setup-postgres-sql-database",
    "href": "tips/060_Misc/sql/setup_sql.html#setup-postgres-sql-database",
    "title": "Create table and copy data from csv file",
    "section": "Setup postgres SQL database",
    "text": "Setup postgres SQL database\nInstall Postgres via website or via brew:\nbrew install postgresql\nbrew services start postgresql\npsql --version\nAdd psql to path to ~/.zshrc:\nnano ~/.zshrc\nexport PATH=\"/Library/PostgreSQL/16/bin:$PATH\"\nNow login as postgres user:\npsql -U postgres\nCrate a new database:\nCREATE DATABASE hello_world_db;\nList all databases:\n\\l\nNow one can use SQLTools extension in VSCode, and connect to database."
  },
  {
    "objectID": "tips/060_Misc/sql/setup_sql.html#constraint-in-sql",
    "href": "tips/060_Misc/sql/setup_sql.html#constraint-in-sql",
    "title": "Create table and copy data from csv file",
    "section": "Constraint in SQL",
    "text": "Constraint in SQL\nForeign key (FK) constraint can be added to a table, for example below, Orders table must have a CustomerID that matches Customer(CustomerID). This ensures data integrity, so we don’t have nonsense Orders that have no matching Customer.\nCREATE TABLE Customers (\n    CustomerID INT PRIMARY KEY,\n    CustomerName VARCHAR(100)\n);\n\nCREATE TABLE Orders (\n    OrderID INT PRIMARY KEY,\n    OrderDate DATE,\n    CustomerID INT,\n    CONSTRAINT FK_CustomerOrder FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\n);"
  },
  {
    "objectID": "tips/060_Misc/sql/setup_sql.html#exercises",
    "href": "tips/060_Misc/sql/setup_sql.html#exercises",
    "title": "Create table and copy data from csv file",
    "section": "Exercises",
    "text": "Exercises\nhint: avg(Survived) is the same as (COUNT(CASE WHEN Survived = 1 THEN 1 END) * 1.0 / count(*)) since Survived is 0 or 1.\nSELECT * FROM titanic LIMIT 10;\nselect (COUNT(CASE WHEN Survived = 1 THEN 1 END) * 1.0 / count(*)) as overall_rate FROM titanic;\nselect avg(Survived) as women_children_rate FROM titanic WHERE (Sex=\"female\" OR Age&lt;=12);\nselect avg(Survived) as others_rate FROM titanic WHERE NOT (Sex=\"female\" OR Age&lt;=12);\nSELECT \n    Pclass, \n    AVG(Survived) AS survival_rate\nFROM \n    titanic\nGROUP BY \n    Pclass\nORDER BY \n    Pclass;\nselect\n    Products.ProductName,\n    Suppliers.CompanyName\nfrom\n    Products\nleft join\n    Suppliers\non\n    Products.SupplierID = Suppliers.SupplierID\norder by\n    Products.ProductName;"
  },
  {
    "objectID": "tips/060_Misc/video.html",
    "href": "tips/060_Misc/video.html",
    "title": "Video capture and processing",
    "section": "",
    "text": "To grab a video on MacOS use Command + Shift + 5.\nTo refer to the video in markdown like README.md use this (btw GitHub automatically embeds video):",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Video capture and processing"
    ]
  },
  {
    "objectID": "tips/060_Misc/video.html#speed-up-video",
    "href": "tips/060_Misc/video.html#speed-up-video",
    "title": "Video capture and processing",
    "section": "Speed up video",
    "text": "Speed up video\n\nfrom moviepy.editor import VideoFileClip\n\ndef speed_up_video(input_video_path, output_video_path, speed_factor=2):\n    # Load the video file\n    video_clip = VideoFileClip(input_video_path)\n    \n    # Speed up the video\n    new_clip = video_clip.speedx(factor=speed_factor)\n    \n    # Write the resulting video to the output file\n    new_clip.write_videofile(output_video_path, codec='libx264', audio_codec='aac')\n\ninput_video_path = 'demo.mov'\noutput_video_path = 'demo_2x.mov'\nspeed_up_video(input_video_path, output_video_path, 2)",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Video capture and processing"
    ]
  },
  {
    "objectID": "tips/060_Misc/video.html#convert-to-gif",
    "href": "tips/060_Misc/video.html#convert-to-gif",
    "title": "Video capture and processing",
    "section": "Convert to gif",
    "text": "Convert to gif\n\nfrom moviepy.editor import VideoFileClip\n\ndef convert_mov_to_gif(source_file, target_file, frame_subsampling=1, start_time=None, end_time=None, resize=None):\n    # Load the source video file\n    clip = VideoFileClip(source_file)  # .subclip(start_time, end_time)\n    \n    # Optionally resize the clip if resize dimensions are provided\n    if resize:\n        clip = clip.resize(resize)\n    \n    if frame_subsampling &gt; 1:\n        clip = clip.set_fps(clip.fps / frame_subsampling)\n        \n    # Write the result to a gif file\n    clip.write_gif(target_file)\n\n# Usage\nconvert_mov_to_gif('/Users/nenadbozinovic/Desktop/demo_extracted_data.mov', '/Users/nenadbozinovic/Desktop/demo_extracted_data_2.gif', frame_subsampling=20)  # , resize=(320, 240))\n\nMoviePy - Building file /Users/nenadbozinovic/Desktop/demo_extracted_data_2.gif with imageio.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Video capture and processing"
    ]
  },
  {
    "objectID": "tips/060_Misc/wsl.html",
    "href": "tips/060_Misc/wsl.html",
    "title": "WSL",
    "section": "",
    "text": "WSL can be installed easily: wsl --install.\nNote that drives are mounted, but access to files outside WSL can be slow, always copy files to WSL directly.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "WSL"
    ]
  },
  {
    "objectID": "tips/060_Misc/wsl.html#reclaim-disk-space",
    "href": "tips/060_Misc/wsl.html#reclaim-disk-space",
    "title": "WSL",
    "section": "Reclaim disk space",
    "text": "Reclaim disk space\nWSL also doesn’t automatically release space back to Local Hard drive. To release the disk space occupied by WSL follow this:\n### Optimize (shrink) WSL 2 .vhdx\n## Must be run in PowerShell as Administrator user\n# DistroFolder found at: $env:LOCALAPPDATA\\Packages\\\n# Examples:\n#   CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\n#   CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\n\ncd $env:LOCALAPPDATA\\Packages\\REPLACE_ME_WITH_TARGET_DISTRO_FOLDERNAME\\LocalState\\\nwsl --shutdown\noptimize-vhd -Path .\\ext4.vhdx -Mode full\n#Run `wsl` or your favorite terminal to resume use\noptimize-vhd requires Hyperviser:\nEnable-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V-All -Online",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "WSL"
    ]
  },
  {
    "objectID": "tips/060_Misc/wsl.html#limit-cpu-memory-usage",
    "href": "tips/060_Misc/wsl.html#limit-cpu-memory-usage",
    "title": "WSL",
    "section": "Limit CPU memory usage",
    "text": "Limit CPU memory usage\nWSL can take a lot of CPU memory, to prevent this follow this, create /user/.wslconfig\n[wsl2]\nmemory=6GB   # set this line what you want to limit memory\nswapFile=E:\\\\swap.vhdx   # in case you need more memory specify swapping location with the hard drive, use double backslash \nvmmem is the wsl2 process for CPU memory that you can observe in Task Manager.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "WSL"
    ]
  },
  {
    "objectID": "tips/060_Misc/kaggle.html",
    "href": "tips/060_Misc/kaggle.html",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle -h # to see the help\nList the currently active competitions:\nkaggle competitions list\ndownload files associated with a competition:\nkaggle competitions download -c [COMPETITION]\nmake a competition submission:\nkaggle competitions submit -c [COMPETITION] -f [FILE] -m [MESSAGE]",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Kaggle"
    ]
  },
  {
    "objectID": "tips/060_Misc/git.html",
    "href": "tips/060_Misc/git.html",
    "title": "Git",
    "section": "",
    "text": ".gitignore file (no extension) example (finish folder with / to add all files)",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Git"
    ]
  },
  {
    "objectID": "tips/060_Misc/git.html#git-clone-via-ssh",
    "href": "tips/060_Misc/git.html#git-clone-via-ssh",
    "title": "Git",
    "section": "git clone via SSH",
    "text": "git clone via SSH\nThere are several ways to clone a repo but SSH is the easiest and most secure so use it always instead of HTTPS.\nSee this to set up SSH access.\nIf git connection is HTTPS then user is asked for username/password. Instead of a password, provide a token, that can be generated like this. It’s better however to simply use SSH.",
    "crumbs": [
      "Projects",
      "TILs",
      "Misc",
      "Git"
    ]
  },
  {
    "objectID": "tips/010_Python/copy_and_deepcopy.html",
    "href": "tips/010_Python/copy_and_deepcopy.html",
    "title": "copy module",
    "section": "",
    "text": "One has to be careful when wanting to copy a variable/objects in Python as simple assigment doesn’t copy an object but just a reference. This problem is solver by the copy module. For simple lists and dictionaries use copy.copy, for class instances one must use copy.deepcopy to actually copy an object. Following is example that illustrates this:\n\nimport copy\n\n\nclass Something():\n    a = 1\n    def __init__(self):        \n        self.b = 1\n        self._value = 1\n        self.list = [1,1,1]\n\n    @property\n    def value(self):\n        return self._value\n    \n    @value.setter\n    def value(self, value):\n        self._value = value\n\nLet’s just assign a new variable p=s, this will not be a copy but just a refernece to the same object:\n\ns = Something()\np = s\nid(p) == id(s)\n\nTrue\n\n\nIf we now change p, s will change too:\n\ndef change_p_and_print(p, s):\n    print('s is initially:', s.a, s.b, s.value, s.list)\n    p.a = 10\n    p.b = 10\n    p.value = 10\n    p.list.append(10)\n    print('p is:          ', p.a, p.b, p.value, p.list)\n    print('s is now:      ', s.a, s.b, s.value, s.list)\n    print('s id:', id(s))\n    print('p id:', id(p))\n\n\nchange_p_and_print(p, s)\n\ns is initially: 1 1 1 [1, 1, 1]\np is:           10 10 10 [1, 1, 1, 10]\ns is now:       10 10 10 [1, 1, 1, 10]\ns id: 4352964496\np id: 4352964496\n\n\nLet’s now make a copy (aka shallow copy):\n\ns = Something()\np = copy.copy(s)\nchange_p_and_print(p, s)\n\ns is initially: 1 1 1 [1, 1, 1]\np is:           10 10 10 [1, 1, 1, 10]\ns is now:       1 1 1 [1, 1, 1, 10]\ns id: 4352951920\np id: 4352964352\n\n\nSo we see that non-list variables are now copied, but list variable is still just referenced, and can be changed. As this website puts it: “a shallow copy doesn’t create a copy of nested objects, instead it just copies the reference of nested objects. This means, a copy process does not recurse or create copies of nested objects itself.”. id-s are now different.\nTo make a proper (i.e. deep copy) we use deep copy.\n\ns = Something()\nprint(s.a, s.b, s.value, s.list)\np = copy.deepcopy(s)\nchange_p_and_print(p, s)\n\n1 1 1 [1, 1, 1]\ns is initially: 1 1 1 [1, 1, 1]\np is:           10 10 10 [1, 1, 1, 10]\ns is now:       1 1 1 [1, 1, 1]\ns id: 4352956480\np id: 4421994192",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "`copy` module"
    ]
  },
  {
    "objectID": "tips/010_Python/functools.html",
    "href": "tips/010_Python/functools.html",
    "title": "Functools",
    "section": "",
    "text": "from functools import partial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef general_quadratic_function(x: np.array, params: tuple):\n    a,b,c = params\n    return a*(x**2) + (b*x) + c\n\n\nour_quadratic_function = partial(general_quadratic_function, params=(1,2,3))\n\n\nour_quadratic_function(np.arange(10))\n\narray([  3,   6,  11,  18,  27,  38,  51,  66,  83, 102])",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Functools"
    ]
  },
  {
    "objectID": "tips/010_Python/functools.html#partial-functions",
    "href": "tips/010_Python/functools.html#partial-functions",
    "title": "Functools",
    "section": "",
    "text": "from functools import partial\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef general_quadratic_function(x: np.array, params: tuple):\n    a,b,c = params\n    return a*(x**2) + (b*x) + c\n\n\nour_quadratic_function = partial(general_quadratic_function, params=(1,2,3))\n\n\nour_quadratic_function(np.arange(10))\n\narray([  3,   6,  11,  18,  27,  38,  51,  66,  83, 102])",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Functools"
    ]
  },
  {
    "objectID": "tips/010_Python/functools.html#plot-functions",
    "href": "tips/010_Python/functools.html#plot-functions",
    "title": "Functools",
    "section": "Plot functions",
    "text": "Plot functions\n\nnp.arange(0,10,0.1)\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n       1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5,\n       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n       5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4,\n       6.5, 6.6, 6.7, 6.8, 6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7,\n       7.8, 7.9, 8. , 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9. ,\n       9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9])\n\n\n\ndef plot_function(func, x=np.arange(0,10,0.1), ax=None, **kwargs):\n    if 'figsize' not in kwargs:\n        kwargs['figsize'] = (3,3)\n    if ax is None:\n        ax = plt.subplots(**kwargs)[1]\n\n    y = func(x)\n    ax.plot(x, y)\n\n\nplot_function(our_quadratic_function)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Functools"
    ]
  },
  {
    "objectID": "tips/010_Python/functools.html#typing",
    "href": "tips/010_Python/functools.html#typing",
    "title": "Functools",
    "section": "Typing",
    "text": "Typing\n\nfrom typing import List, TypeVar\nMyType = TypeVar('MyType', str, int )\n\ndef foo(a: List[int]):\n    print(a)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Functools"
    ]
  },
  {
    "objectID": "tips/010_Python/pil.html",
    "href": "tips/010_Python/pil.html",
    "title": "PIL",
    "section": "",
    "text": "from PIL import Image\nfrom PIL import ImageDraw, ImageStat\nfrom pathlib import Path\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\nim = Image.open(\"assets/example.png\")\nim\n\n\n\n\n\n\n\n\nThis will show image in a separate window, title doesn’t work, see here.\n\nim.show('pera')\n\nThis can show image in Jupyter with title but not true colors:\n\nplt.imshow(im)\nplt.title('pera')\nplt.show()\n\n\n\n\n\n\n\n\nIn jupyter one can use only:\n\nim  # display(im)\n\n\n\n\n\n\n\n\nTo create new image:\n\nwidth, height = 256, 256\nmask = Image.new('L', (width, height), 0)\n\nExample to draw a polygon:\n\npolygon = [(4, 1), (1, 54), (1, 222), (13, 1)]\nImageDraw.Draw(mask).polygon(polygon, outline=255, fill=255)\n\nTo load image from png, show bands (i.e. channels):\n\nim = Image.open('assets/example.png')\nprint(im.getbands())\n\n('R', 'G', 'B')\n\n\n\nim.getchannel('R')\n\n\n\n\n\n\n\n\nTo convert to RGB (from RGBA for example):\n\nim.convert('RGB')\n\n\n\n\n\n\n\n\n\nim.convert(\"RGBA\")\n\n\n\n\n\n\n\n\n\nim.convert('L')\n\n\n\n\n\n\n\n\nTo convert to tensor:\n\ntensorizer = ToTensor()\nim_tensor = tensorizer(im)\nprint(im_tensor.shape)\nim_tensor[:, :3, :3]\n\ntorch.Size([3, 256, 256])\n\n\ntensor([[[0.3020, 0.2902, 0.2706],\n         [0.2941, 0.2824, 0.2667],\n         [0.2824, 0.2706, 0.2627]],\n\n        [[0.3333, 0.3216, 0.3059],\n         [0.3294, 0.3176, 0.3059],\n         [0.3137, 0.3059, 0.3020]],\n\n        [[0.3059, 0.2980, 0.2863],\n         [0.2941, 0.2902, 0.2863],\n         [0.2745, 0.2745, 0.2784]]])\n\n\n\ndef save_image(im: Image, filepath: Path, overwrite: bool = False):\n    if filepath.exists() and not overwrite:\n        return\n    im.save(filepath, \"PNG\")\n\n\nStat\n\nstat = ImageStat.Stat(im)\nprint(\"\"\"\n* Min/max values for each band in the image:\n    {.extrema}\n\n* Total number of pixels for each band in the image:\n    {.count}\n\n* Sum of all pixels for each band in the image:\n    {.sum}\n\n* Squared sum of all pixels for each band in the image:\n    {.sum2}\n\n* Average (arithmetic mean) pixel level for each band in the image:\n    {.mean}\n\n* Median pixel level for each band in the image:\n    {.median}\n\n* RMS (root-mean-square) for each band in the image:\n    {.rms}\n\n* Variance for each band in the image:\n    {.var}\n\n* Standard deviation for each band in the image:\n    {.stddev}\n\"\"\".format(*((stat, ) * 9)))\n\n\n* Min/max values for each band in the image:\n    [(1, 142), (1, 137), (1, 128)]\n\n* Total number of pixels for each band in the image:\n    [65536, 65536, 65536]\n\n* Sum of all pixels for each band in the image:\n    [3093106.0, 3904015.0, 2879192.0]\n\n* Squared sum of all pixels for each band in the image:\n    [225487516.0, 325785395.0, 221232102.0]\n\n* Average (arithmetic mean) pixel level for each band in the image:\n    [47.197052001953125, 59.57054138183594, 43.9329833984375]\n\n* Median pixel level for each band in the image:\n    [53, 72, 54]\n\n* RMS (root-mean-square) for each band in the image:\n    [58.657194297640025, 70.50596160572695, 58.10106692885669]\n\n* Variance for each band in the image:\n    [1213.1047251960263, 1422.4412214232143, 1445.6269479840994]\n\n* Standard deviation for each band in the image:\n    [34.82965295830589, 37.71526509814314, 38.0214011838609]\n\n\n\n\n\nMerge two images using Image.paste\n\nim1 = im.crop((left, top, right, bottom))\nim1 = im1.resize( (300, 300))\nfrontImage = frontImage.convert(\"RGBA\")\nbackground.paste(frontImage, (width, height), frontImage)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PIL"
    ]
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html",
    "href": "tips/010_Python/pyspark_apriori.html",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "",
    "text": "import itertools\nimport findspark\nfindspark.init()\nimport pyspark\n\n\nimport os\nimport findspark\n\n# Set environment variables within the notebook\nos.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home'  # Verify this path\nos.environ['SPARK_HOME'] = '/opt/homebrew/opt/apache-spark'  # Verify this path\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[5], line 12\n      9 from pyspark.sql import SparkSession\n     11 # Initialize Spark Session\n---&gt; 12 spark = SparkSession.builder.appName(\"example\").getOrCreate()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/sql/session.py:497, in SparkSession.Builder.getOrCreate(self)\n    495     sparkConf.set(key, value)\n    496 # This SparkContext may be an existing one.\n--&gt; 497 sc = SparkContext.getOrCreate(sparkConf)\n    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    499 # by all sessions.\n    500 session = SparkSession(sc, options=self._options)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nspark.getOrCreate()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 spark.getOrCreate()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/sql/session.py:497, in SparkSession.Builder.getOrCreate(self)\n    495     sparkConf.set(key, value)\n    496 # This SparkContext may be an existing one.\n--&gt; 497 sc = SparkContext.getOrCreate(sparkConf)\n    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    499 # by all sessions.\n    500 session = SparkSession(sc, options=self._options)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\ndata = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\ndf = spark.createDataFrame(data, [\"Name\", \"Value\"])\n\ndf.show()\n\n\nconf = pyspark.SparkConf()\nconf.setAppName('apriori')\nconf.setMaster('local')\ncontext = pyspark.SparkContext(conf=conf)\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[10], line 4\n      2 conf.setAppName('apriori')\n      3 conf.setMaster('local')\n----&gt; 4 context = pyspark.SparkContext(conf=conf)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nfrom pyspark import SparkContext\nSparkContext.getOrCreate().stop()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[11], line 2\n      1 from pyspark import SparkContext\n----&gt; 2 SparkContext.getOrCreate().stop()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nsession = pyspark.sql.SparkSession(context)\n\n\nconf.getAll()"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-1-import-the-libraries-and-set-up-the-environment",
    "href": "tips/010_Python/pyspark_apriori.html#task-1-import-the-libraries-and-set-up-the-environment",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "",
    "text": "import itertools\nimport findspark\nfindspark.init()\nimport pyspark\n\n\nimport os\nimport findspark\n\n# Set environment variables within the notebook\nos.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home'  # Verify this path\nos.environ['SPARK_HOME'] = '/opt/homebrew/opt/apache-spark'  # Verify this path\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[5], line 12\n      9 from pyspark.sql import SparkSession\n     11 # Initialize Spark Session\n---&gt; 12 spark = SparkSession.builder.appName(\"example\").getOrCreate()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/sql/session.py:497, in SparkSession.Builder.getOrCreate(self)\n    495     sparkConf.set(key, value)\n    496 # This SparkContext may be an existing one.\n--&gt; 497 sc = SparkContext.getOrCreate(sparkConf)\n    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    499 # by all sessions.\n    500 session = SparkSession(sc, options=self._options)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nspark.getOrCreate()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 spark.getOrCreate()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/sql/session.py:497, in SparkSession.Builder.getOrCreate(self)\n    495     sparkConf.set(key, value)\n    496 # This SparkContext may be an existing one.\n--&gt; 497 sc = SparkContext.getOrCreate(sparkConf)\n    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    499 # by all sessions.\n    500 session = SparkSession(sc, options=self._options)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\ndata = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\ndf = spark.createDataFrame(data, [\"Name\", \"Value\"])\n\ndf.show()\n\n\nconf = pyspark.SparkConf()\nconf.setAppName('apriori')\nconf.setMaster('local')\ncontext = pyspark.SparkContext(conf=conf)\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[10], line 4\n      2 conf.setAppName('apriori')\n      3 conf.setMaster('local')\n----&gt; 4 context = pyspark.SparkContext(conf=conf)\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nfrom pyspark import SparkContext\nSparkContext.getOrCreate().stop()\n\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: Permission denied\n/opt/homebrew/opt/apache-spark/bin/load-spark-env.sh: line 2: exec: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec/bin/load-spark-env.sh: cannot execute: Undefined error: 0\n\n\n\n---------------------------------------------------------------------------\nPySparkRuntimeError                       Traceback (most recent call last)\nCell In[11], line 2\n      1 from pyspark import SparkContext\n----&gt; 2 SparkContext.getOrCreate().stop()\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n    513 with SparkContext._lock:\n    514     if SparkContext._active_spark_context is None:\n--&gt; 515         SparkContext(conf=conf or SparkConf())\n    516     assert SparkContext._active_spark_context is not None\n    517     return SparkContext._active_spark_context\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n    196     raise ValueError(\n    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n    198         \" is not allowed as it is a security risk.\"\n    199     )\n--&gt; 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    202 try:\n    203     self._do_init(\n    204         master,\n    205         appName,\n   (...)\n    215         memory_profiler_cls,\n    216     )\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n    434 with SparkContext._lock:\n    435     if not SparkContext._gateway:\n--&gt; 436         SparkContext._gateway = gateway or launch_gateway(conf)\n    437         SparkContext._jvm = SparkContext._gateway.jvm\n    439     if instance:\n\nFile ~/Documents/blog/venv_blog/lib/python3.12/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n    104     time.sleep(0.1)\n    106 if not os.path.isfile(conn_info_file):\n--&gt; 107     raise PySparkRuntimeError(\n    108         error_class=\"JAVA_GATEWAY_EXITED\",\n    109         message_parameters={},\n    110     )\n    112 with open(conn_info_file, \"rb\") as info:\n    113     gateway_port = read_int(info)\n\nPySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.\n\n\n\n\nsession = pyspark.sql.SparkSession(context)\n\n\nconf.getAll()"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-3-generate-combinationssubset-frequency-property",
    "href": "tips/010_Python/pyspark_apriori.html#task-3-generate-combinationssubset-frequency-property",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 3: Generate Combinations—Subset Frequency Property",
    "text": "Task 3: Generate Combinations—Subset Frequency Property\n\ndef post_check(k_size_comb, freq_k_1, k):\n    filtered = []\n    for  comb in  k_size_comb:\n        flag = False\n        for sub_comb in itertools.combinations(comb, k-1):\n            if sub_comb not in freq_k_1:\n                flag = True\n        if flag == False:\n            filtered.append(tuple(comb))\n    return filtered"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-4-count-check",
    "href": "tips/010_Python/pyspark_apriori.html#task-4-count-check",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 4: Count Check",
    "text": "Task 4: Count Check\n\ndef count_check(filtered, lines, supCount):\n    results = []\n    counts = dict(zip(filtered, [0]*len(filtered)))\n    for combination in filtered:\n        present = [False]*len(combination)\n        for i in range(len(combination)):\n            for line in lines: \n                if combination[i] in line:\n                    present[i] = True\n                if all(present):\n                    counts[combination] +=1\n\n    for word, count in counts.items():\n        if (count&gt;=supCount):\n            results.append(word)\n    return results"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-5-generate-k-size-combinations",
    "href": "tips/010_Python/pyspark_apriori.html#task-5-generate-k-size-combinations",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 5: Generate k-Size Combinations",
    "text": "Task 5: Generate k-Size Combinations\n\ndef generator(freq_k_1, k, partition, support):\n    \n    lines = list(partition)\n    supCount = len(lines)*support\n\n    k_size_comb = pre_check(freq_k_1, k)\n    \n    filtered = post_check(k_size_comb, freq_k_1, k)\n    \n    return count_check(filtered, lines, supCount)"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-6-generate-singles",
    "href": "tips/010_Python/pyspark_apriori.html#task-6-generate-singles",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 6: Generate Singles",
    "text": "Task 6: Generate Singles\n\ndef get_singles(lines, support):\n    supCount = len(list(lines))*support\n    vocab = set([])\n    for line in lines:\n        for word in line:\n            vocab.add(word)\n    counts = dict(zip(vocab, [0]*len(list(vocab))))\n    combinations = []\n    for line in lines:\n        for word in line:\n            counts[word] +=1\n    for word, count in counts.items():\n        if (count&gt;=supCount):\n            combinations.append(tuple((word,))) \n    return sorted(combinations)"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-7-the-worker-partition-mapper",
    "href": "tips/010_Python/pyspark_apriori.html#task-7-the-worker-partition-mapper",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 7: The Worker Partition Mapper",
    "text": "Task 7: The Worker Partition Mapper\n\nseq_len = context.broadcast(2)\n\n\ndef apriori(iterator):\n    partition = []\n    for v in iterator:\n        partition.append(v)\n    support = sup.value\n    results= get_singles(partition, support)\n    print('starting with', results)\n\n    for k in range(2, seq_len.value+1):\n        print('sequence length', k)\n     \n        combos = generator(results, k, partition, support)\n\n        if len(combos) == 0:\n            print('ending at sequence length' ,k-1)\n            return results\n\n        results = combos\n    return results"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-8-load-data-and-preprocess",
    "href": "tips/010_Python/pyspark_apriori.html#task-8-load-data-and-preprocess",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 8: Load Data and Preprocess",
    "text": "Task 8: Load Data and Preprocess\n\nrdd = context.textFile(\"usercode/Dataset.csv\")\ntagsheader = rdd.first() \ntags = context.parallelize(tagsheader)\nseq_len = context.broadcast(3)\ndata = rdd.subtract(tags)\nlength = context.broadcast(data.count())\nsup = context.broadcast(0.03)\nlines = data.map(lambda x: x.lstrip('\"').rstrip('\"').split(','))"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-9-the-distributed-transform",
    "href": "tips/010_Python/pyspark_apriori.html#task-9-the-distributed-transform",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 9: The Distributed Transform",
    "text": "Task 9: The Distributed Transform\n\nfreq = lines.mapPartitions(apriori)\nfreq = freq.distinct()\ncomb = freq.collect()\nprint(\"Possible frequent itemset(s):\\n\", comb)"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-10-auxiliary-function-to-check-presence",
    "href": "tips/010_Python/pyspark_apriori.html#task-10-auxiliary-function-to-check-presence",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 10: Auxiliary Function to Check Presence",
    "text": "Task 10: Auxiliary Function to Check Presence\n\ndef auxiliary(row, combinations):\n    present= []\n    for combination in combinations:\n        presence = [False]*len(combination)\n        for i in range(len(combination)):\n            presence[i] = combination[i] in row\n        if all(presence):\n            present+=[combination]\n    return present"
  },
  {
    "objectID": "tips/010_Python/pyspark_apriori.html#task-11-count-check-at-master",
    "href": "tips/010_Python/pyspark_apriori.html#task-11-count-check-at-master",
    "title": "Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark",
    "section": "Task 11: Count Check at Master",
    "text": "Task 11: Count Check at Master\n\ncomb = context.broadcast(comb)\nfreq1 = lines.map(lambda x: [(key, 1) for key in auxiliary(x, comb.value)]).filter(lambda x: len(x)&gt;0)\n\nfreq2 = freq1.flatMap(lambda x: x)\nfreq3 = freq2.reduceByKey(lambda x, y: x+y)\nfreq4 = freq3.filter(lambda x: x[1]&gt;sup.value*length.value).map(lambda x: x[0])\nfreq4.collect()"
  },
  {
    "objectID": "tips/010_Python/visualizations.html",
    "href": "tips/010_Python/visualizations.html",
    "title": "Visualizations",
    "section": "",
    "text": "from pathlib import PosixPath\nimport ipyplot\nimport cv2\nfrom PIL import Image\nis_jupyter = get_ipython().__class__.__name__ == 'ZMQInteractiveShell'",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Visualizations"
    ]
  },
  {
    "objectID": "tips/010_Python/visualizations.html#matplotlib.pyplot.axis.plot",
    "href": "tips/010_Python/visualizations.html#matplotlib.pyplot.axis.plot",
    "title": "Visualizations",
    "section": "Matplotlib.pyplot.axis.plot",
    "text": "Matplotlib.pyplot.axis.plot\nSee here.",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Visualizations"
    ]
  },
  {
    "objectID": "tips/010_Python/visualizations.html#plot-multiple-images",
    "href": "tips/010_Python/visualizations.html#plot-multiple-images",
    "title": "Visualizations",
    "section": "Plot multiple images",
    "text": "Plot multiple images\nWe can use ipyplot (pip install ipyplot), but it has annoying issue. Instead I made a custom one:\n\nfrom utils.plot import plot_pil_images\n\n\nplot_pil_images([im_pil, im_pil, im_pil.transpose(Image.ROTATE_90)], \n                ['orig', 'no change', 'transpose 90'])",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Visualizations"
    ]
  },
  {
    "objectID": "tips/010_Python/logging.html",
    "href": "tips/010_Python/logging.html",
    "title": "Logging",
    "section": "",
    "text": "import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nlogging.debug(\"This is a DEBUG message\")\nlogging.info(\"This is an INFO message\")\nlogging.warning(\"This is a WARNING message\")\nlogging.error(\"This is an ERROR message\")\nlogging.critical(\"This is a CRITICAL message\")\nOne can add handlers so one log even can be handled in multiple ways, for example FileHandler stores to SDD:\nhandler = logging.FileHandler('myclass1.log')\n            handler.setLevel(logging.INFO)\n            self.logger.addHandler(handler)\n            self.logger.setLevel(logging.INFO)\nFinally one can also parse log using grep:\ngrep \"ERROR\" your_log_file.log &gt; errors_only.log\n\ngrep \"^W\" your_log_file.log &gt; warnings.log\n\n# to get every 100th line of file x.txt\nawk 'NR % 100 == 0' x.txt &gt; y.txt",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "Logging"
    ]
  },
  {
    "objectID": "tips/010_Python/pydata.html",
    "href": "tips/010_Python/pydata.html",
    "title": "PyData",
    "section": "",
    "text": "import numpy as np\nnp.random.seed(42)\na = np.random.randn(1,10)\nprint(a)\nprint(a.max())\nprint(a.dtype)\na = a.astype(np.uint8)\nprint(a)\nprint(a.max())\nprint(a.dtype)\nprint(a.shape)\na = a.squeeze()\nprint(a.shape)\nTo scale tensor from 0, 255, convert to uint8\nb = (np.array(mask_tensor) / np.array(mask_tensor).max() * 255).astype(np.uint8).squeeze()\nb.max()\nb = (np.array(mask_tensor) / np.array(mask_tensor).max()).astype(np.float32).squeeze()\nb.max()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial import polynomial as P\n\nM = 100;\nN = 4;\nx = np.linspace(-20, 20, M)\nX = np.fliplr(np.vander(x , N + 1))\n# for i in range(N+1):\n#     X[:,i] = x**i\nnoise = 20 * np.random.randn(M)\nbeta = [18, -12, 2, 0.1, 0.1];\ny = np.dot(X, beta) + noise\n\nbeta_r = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\ndef plot_something(p):\n    fig, ax = plt.subplots()\n    ax.plot(p, 'o')\n    ax.set_title('Random')\n    plt.show()\n\n\nbeta_r, stats = P.polyfit(x, y, 4, full=True)\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\nhelp(P.polyfit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelp on function polyfit in module numpy.polynomial.polynomial:\n\npolyfit(x, y, deg, rcond=None, full=False, w=None)\n    Least-squares fit of a polynomial to data.\n    \n    Return the coefficients of a polynomial of degree `deg` that is the\n    least squares fit to the data values `y` given at points `x`. If `y` is\n    1-D the returned coefficients will also be 1-D. If `y` is 2-D multiple\n    fits are done, one for each column of `y`, and the resulting\n    coefficients are stored in the corresponding columns of a 2-D return.\n    The fitted polynomial(s) are in the form\n    \n    .. math::  p(x) = c_0 + c_1 * x + ... + c_n * x^n,\n    \n    where `n` is `deg`.\n    \n    Parameters\n    ----------\n    x : array_like, shape (`M`,)\n        x-coordinates of the `M` sample (data) points ``(x[i], y[i])``.\n    y : array_like, shape (`M`,) or (`M`, `K`)\n        y-coordinates of the sample points.  Several sets of sample points\n        sharing the same x-coordinates can be (independently) fit with one\n        call to `polyfit` by passing in for `y` a 2-D array that contains\n        one data set per column.\n    deg : int or 1-D array_like\n        Degree(s) of the fitting polynomials. If `deg` is a single integer\n        all terms up to and including the `deg`'th term are included in the\n        fit. For NumPy versions &gt;= 1.11.0 a list of integers specifying the\n        degrees of the terms to include may be used instead.\n    rcond : float, optional\n        Relative condition number of the fit.  Singular values smaller\n        than `rcond`, relative to the largest singular value, will be\n        ignored.  The default value is ``len(x)*eps``, where `eps` is the\n        relative precision of the platform's float type, about 2e-16 in\n        most cases.\n    full : bool, optional\n        Switch determining the nature of the return value.  When ``False``\n        (the default) just the coefficients are returned; when ``True``,\n        diagnostic information from the singular value decomposition (used\n        to solve the fit's matrix equation) is also returned.\n    w : array_like, shape (`M`,), optional\n        Weights. If not None, the weight ``w[i]`` applies to the unsquared\n        residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n        chosen so that the errors of the products ``w[i]*y[i]`` all have the\n        same variance.  When using inverse-variance weighting, use\n        ``w[i] = 1/sigma(y[i])``.  The default value is None.\n    \n        .. versionadded:: 1.5.0\n    \n    Returns\n    -------\n    coef : ndarray, shape (`deg` + 1,) or (`deg` + 1, `K`)\n        Polynomial coefficients ordered from low to high.  If `y` was 2-D,\n        the coefficients in column `k` of `coef` represent the polynomial\n        fit to the data in `y`'s `k`-th column.\n    \n    [residuals, rank, singular_values, rcond] : list\n        These values are only returned if ``full == True``\n    \n        - residuals -- sum of squared residuals of the least squares fit\n        - rank -- the numerical rank of the scaled Vandermonde matrix\n        - singular_values -- singular values of the scaled Vandermonde matrix\n        - rcond -- value of `rcond`.\n    \n        For more details, see `numpy.linalg.lstsq`.\n    \n    Raises\n    ------\n    RankWarning\n        Raised if the matrix in the least-squares fit is rank deficient.\n        The warning is only raised if ``full == False``.  The warnings can\n        be turned off by:\n    \n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.simplefilter('ignore', np.RankWarning)\n    \n    See Also\n    --------\n    numpy.polynomial.chebyshev.chebfit\n    numpy.polynomial.legendre.legfit\n    numpy.polynomial.laguerre.lagfit\n    numpy.polynomial.hermite.hermfit\n    numpy.polynomial.hermite_e.hermefit\n    polyval : Evaluates a polynomial.\n    polyvander : Vandermonde matrix for powers.\n    numpy.linalg.lstsq : Computes a least-squares fit from the matrix.\n    scipy.interpolate.UnivariateSpline : Computes spline fits.\n    \n    Notes\n    -----\n    The solution is the coefficients of the polynomial `p` that minimizes\n    the sum of the weighted squared errors\n    \n    .. math:: E = \\sum_j w_j^2 * |y_j - p(x_j)|^2,\n    \n    where the :math:`w_j` are the weights. This problem is solved by\n    setting up the (typically) over-determined matrix equation:\n    \n    .. math:: V(x) * c = w * y,\n    \n    where `V` is the weighted pseudo Vandermonde matrix of `x`, `c` are the\n    coefficients to be solved for, `w` are the weights, and `y` are the\n    observed values.  This equation is then solved using the singular value\n    decomposition of `V`.\n    \n    If some of the singular values of `V` are so small that they are\n    neglected (and `full` == ``False``), a `RankWarning` will be raised.\n    This means that the coefficient values may be poorly determined.\n    Fitting to a lower order polynomial will usually get rid of the warning\n    (but may not be what you want, of course; if you have independent\n    reason(s) for choosing the degree which isn't working, you may have to:\n    a) reconsider those reasons, and/or b) reconsider the quality of your\n    data).  The `rcond` parameter can also be set to a value smaller than\n    its default, but the resulting fit may be spurious and have large\n    contributions from roundoff error.\n    \n    Polynomial fits using double precision tend to \"fail\" at about\n    (polynomial) degree 20. Fits using Chebyshev or Legendre series are\n    generally better conditioned, but much can still depend on the\n    distribution of the sample points and the smoothness of the data.  If\n    the quality of the fit is inadequate, splines may be a good\n    alternative.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; np.random.seed(123)\n    &gt;&gt;&gt; from numpy.polynomial import polynomial as P\n    &gt;&gt;&gt; x = np.linspace(-1,1,51) # x \"data\": [-1, -0.96, ..., 0.96, 1]\n    &gt;&gt;&gt; y = x**3 - x + np.random.randn(len(x))  # x^3 - x + Gaussian noise\n    &gt;&gt;&gt; c, stats = P.polyfit(x,y,3,full=True)\n    &gt;&gt;&gt; np.random.seed(123)\n    &gt;&gt;&gt; c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1\n    array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286]) # may vary\n    &gt;&gt;&gt; stats # note the large SSR, explaining the rather poor results\n     [array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316, # may vary\n              0.28853036]), 1.1324274851176597e-014]\n    \n    Same thing without the added noise\n    \n    &gt;&gt;&gt; y = x**3 - x\n    &gt;&gt;&gt; c, stats = P.polyfit(x,y,3,full=True)\n    &gt;&gt;&gt; c # c[0], c[2] should be \"very close to 0\", c[1] ~= -1, c[3] ~= 1\n    array([-6.36925336e-18, -1.00000000e+00, -4.08053781e-16,  1.00000000e+00])\n    &gt;&gt;&gt; stats # note the minuscule SSR\n    [array([  7.46346754e-31]), 4, array([ 1.38446749,  1.32119158, # may vary\n               0.50443316,  0.28853036]), 1.1324274851176597e-014]\n\n\n\n\n\n\nSome random image manipulations:\n\nimport cv2\n\nvideo = cv2.VideoCapture(0)\n\nfirst_frame = None\n\nwhile True:\n\n    check, frame = video.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    if first_frame is None:\n        first_frame = gray\n        continue\n\n\n    delta_frame = cv2.absdiff(first_frame, gray)\n    thresh_frame = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n    thresh_frame = cv2.dilate(thresh_frame, None, iterations=2)\n\n    ctns, _ = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n\n    for contour in ctns:\n        if cv2.contourArea(contour) &lt; 1000:\n            continue\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n\n    cv2.imshow(\"Gray frame\", gray)\n    cv2.imshow(\"Delta frame\", delta_frame)\n    cv2.imshow(\"Threshold frame\", thresh_frame)\n    cv2.imshow(\"Color frame\", frame)\n\n    key = cv2.waitKey(10)\n\n    if key == ord('q'):\n        break\n\nvideo.release()\n\n\n\n\nTo read a csv file with columns [‘time’, ‘a’, ‘b’, ‘c’, ‘d’, ‘e’]:\ndf = pd.read_csv(file_str)   # header by default is infer\ndf = pd.read_csv(file_str, header=1, names=['time', 'a', 'b', 'c', 'd', 'e'])\nTo log data to a csv file with columns var1, var2, var3:\nimport pandas as pd\n\n# Define the variables for the new row\nvar1 = 'New Value 1'\nvar2 = 99\nvar3 = 'World'\n\n# Create a DataFrame with the new row\nnew_row = pd.DataFrame([{'var1': var1, \n                         'var2': var2, \n                         'var3': var3}])\n\n# Define the existing CSV file path\ncsv_file = 'test_data.csv'\n\n# Read the existing CSV file (if it exists)\ntry:\n    existing_data = pd.read_csv(csv_file)\nexcept FileNotFoundError:\n    existing_data = pd.DataFrame()\n\n# Append the new row to the existing data\nupdated_data = existing_data.append(new_row, ignore_index=True)\n\n# Write the updated data to the CSV file\nupdated_data.to_csv(csv_file, index=False)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PyData"
    ]
  },
  {
    "objectID": "tips/010_Python/pydata.html#polynomial-fitting",
    "href": "tips/010_Python/pydata.html#polynomial-fitting",
    "title": "PyData",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial import polynomial as P\n\nM = 100;\nN = 4;\nx = np.linspace(-20, 20, M)\nX = np.fliplr(np.vander(x , N + 1))\n# for i in range(N+1):\n#     X[:,i] = x**i\nnoise = 20 * np.random.randn(M)\nbeta = [18, -12, 2, 0.1, 0.1];\ny = np.dot(X, beta) + noise\n\nbeta_r = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\ndef plot_something(p):\n    fig, ax = plt.subplots()\n    ax.plot(p, 'o')\n    ax.set_title('Random')\n    plt.show()\n\n\nbeta_r, stats = P.polyfit(x, y, 4, full=True)\n\ny_r = np.dot(X, beta_r)\nplt.plot(x, y,'.')\nplt.plot(x, y_r, 'r')\nplt.title(str(beta_r))\nplt.show()\n\nhelp(P.polyfit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHelp on function polyfit in module numpy.polynomial.polynomial:\n\npolyfit(x, y, deg, rcond=None, full=False, w=None)\n    Least-squares fit of a polynomial to data.\n    \n    Return the coefficients of a polynomial of degree `deg` that is the\n    least squares fit to the data values `y` given at points `x`. If `y` is\n    1-D the returned coefficients will also be 1-D. If `y` is 2-D multiple\n    fits are done, one for each column of `y`, and the resulting\n    coefficients are stored in the corresponding columns of a 2-D return.\n    The fitted polynomial(s) are in the form\n    \n    .. math::  p(x) = c_0 + c_1 * x + ... + c_n * x^n,\n    \n    where `n` is `deg`.\n    \n    Parameters\n    ----------\n    x : array_like, shape (`M`,)\n        x-coordinates of the `M` sample (data) points ``(x[i], y[i])``.\n    y : array_like, shape (`M`,) or (`M`, `K`)\n        y-coordinates of the sample points.  Several sets of sample points\n        sharing the same x-coordinates can be (independently) fit with one\n        call to `polyfit` by passing in for `y` a 2-D array that contains\n        one data set per column.\n    deg : int or 1-D array_like\n        Degree(s) of the fitting polynomials. If `deg` is a single integer\n        all terms up to and including the `deg`'th term are included in the\n        fit. For NumPy versions &gt;= 1.11.0 a list of integers specifying the\n        degrees of the terms to include may be used instead.\n    rcond : float, optional\n        Relative condition number of the fit.  Singular values smaller\n        than `rcond`, relative to the largest singular value, will be\n        ignored.  The default value is ``len(x)*eps``, where `eps` is the\n        relative precision of the platform's float type, about 2e-16 in\n        most cases.\n    full : bool, optional\n        Switch determining the nature of the return value.  When ``False``\n        (the default) just the coefficients are returned; when ``True``,\n        diagnostic information from the singular value decomposition (used\n        to solve the fit's matrix equation) is also returned.\n    w : array_like, shape (`M`,), optional\n        Weights. If not None, the weight ``w[i]`` applies to the unsquared\n        residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n        chosen so that the errors of the products ``w[i]*y[i]`` all have the\n        same variance.  When using inverse-variance weighting, use\n        ``w[i] = 1/sigma(y[i])``.  The default value is None.\n    \n        .. versionadded:: 1.5.0\n    \n    Returns\n    -------\n    coef : ndarray, shape (`deg` + 1,) or (`deg` + 1, `K`)\n        Polynomial coefficients ordered from low to high.  If `y` was 2-D,\n        the coefficients in column `k` of `coef` represent the polynomial\n        fit to the data in `y`'s `k`-th column.\n    \n    [residuals, rank, singular_values, rcond] : list\n        These values are only returned if ``full == True``\n    \n        - residuals -- sum of squared residuals of the least squares fit\n        - rank -- the numerical rank of the scaled Vandermonde matrix\n        - singular_values -- singular values of the scaled Vandermonde matrix\n        - rcond -- value of `rcond`.\n    \n        For more details, see `numpy.linalg.lstsq`.\n    \n    Raises\n    ------\n    RankWarning\n        Raised if the matrix in the least-squares fit is rank deficient.\n        The warning is only raised if ``full == False``.  The warnings can\n        be turned off by:\n    \n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; warnings.simplefilter('ignore', np.RankWarning)\n    \n    See Also\n    --------\n    numpy.polynomial.chebyshev.chebfit\n    numpy.polynomial.legendre.legfit\n    numpy.polynomial.laguerre.lagfit\n    numpy.polynomial.hermite.hermfit\n    numpy.polynomial.hermite_e.hermefit\n    polyval : Evaluates a polynomial.\n    polyvander : Vandermonde matrix for powers.\n    numpy.linalg.lstsq : Computes a least-squares fit from the matrix.\n    scipy.interpolate.UnivariateSpline : Computes spline fits.\n    \n    Notes\n    -----\n    The solution is the coefficients of the polynomial `p` that minimizes\n    the sum of the weighted squared errors\n    \n    .. math:: E = \\sum_j w_j^2 * |y_j - p(x_j)|^2,\n    \n    where the :math:`w_j` are the weights. This problem is solved by\n    setting up the (typically) over-determined matrix equation:\n    \n    .. math:: V(x) * c = w * y,\n    \n    where `V` is the weighted pseudo Vandermonde matrix of `x`, `c` are the\n    coefficients to be solved for, `w` are the weights, and `y` are the\n    observed values.  This equation is then solved using the singular value\n    decomposition of `V`.\n    \n    If some of the singular values of `V` are so small that they are\n    neglected (and `full` == ``False``), a `RankWarning` will be raised.\n    This means that the coefficient values may be poorly determined.\n    Fitting to a lower order polynomial will usually get rid of the warning\n    (but may not be what you want, of course; if you have independent\n    reason(s) for choosing the degree which isn't working, you may have to:\n    a) reconsider those reasons, and/or b) reconsider the quality of your\n    data).  The `rcond` parameter can also be set to a value smaller than\n    its default, but the resulting fit may be spurious and have large\n    contributions from roundoff error.\n    \n    Polynomial fits using double precision tend to \"fail\" at about\n    (polynomial) degree 20. Fits using Chebyshev or Legendre series are\n    generally better conditioned, but much can still depend on the\n    distribution of the sample points and the smoothness of the data.  If\n    the quality of the fit is inadequate, splines may be a good\n    alternative.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; np.random.seed(123)\n    &gt;&gt;&gt; from numpy.polynomial import polynomial as P\n    &gt;&gt;&gt; x = np.linspace(-1,1,51) # x \"data\": [-1, -0.96, ..., 0.96, 1]\n    &gt;&gt;&gt; y = x**3 - x + np.random.randn(len(x))  # x^3 - x + Gaussian noise\n    &gt;&gt;&gt; c, stats = P.polyfit(x,y,3,full=True)\n    &gt;&gt;&gt; np.random.seed(123)\n    &gt;&gt;&gt; c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1\n    array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286]) # may vary\n    &gt;&gt;&gt; stats # note the large SSR, explaining the rather poor results\n     [array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316, # may vary\n              0.28853036]), 1.1324274851176597e-014]\n    \n    Same thing without the added noise\n    \n    &gt;&gt;&gt; y = x**3 - x\n    &gt;&gt;&gt; c, stats = P.polyfit(x,y,3,full=True)\n    &gt;&gt;&gt; c # c[0], c[2] should be \"very close to 0\", c[1] ~= -1, c[3] ~= 1\n    array([-6.36925336e-18, -1.00000000e+00, -4.08053781e-16,  1.00000000e+00])\n    &gt;&gt;&gt; stats # note the minuscule SSR\n    [array([  7.46346754e-31]), 4, array([ 1.38446749,  1.32119158, # may vary\n               0.50443316,  0.28853036]), 1.1324274851176597e-014]",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PyData"
    ]
  },
  {
    "objectID": "tips/010_Python/pydata.html#opencv",
    "href": "tips/010_Python/pydata.html#opencv",
    "title": "PyData",
    "section": "",
    "text": "Some random image manipulations:\n\nimport cv2\n\nvideo = cv2.VideoCapture(0)\n\nfirst_frame = None\n\nwhile True:\n\n    check, frame = video.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    if first_frame is None:\n        first_frame = gray\n        continue\n\n\n    delta_frame = cv2.absdiff(first_frame, gray)\n    thresh_frame = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n    thresh_frame = cv2.dilate(thresh_frame, None, iterations=2)\n\n    ctns, _ = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n\n    for contour in ctns:\n        if cv2.contourArea(contour) &lt; 1000:\n            continue\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n\n    cv2.imshow(\"Gray frame\", gray)\n    cv2.imshow(\"Delta frame\", delta_frame)\n    cv2.imshow(\"Threshold frame\", thresh_frame)\n    cv2.imshow(\"Color frame\", frame)\n\n    key = cv2.waitKey(10)\n\n    if key == ord('q'):\n        break\n\nvideo.release()",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PyData"
    ]
  },
  {
    "objectID": "tips/010_Python/pydata.html#pandas",
    "href": "tips/010_Python/pydata.html#pandas",
    "title": "PyData",
    "section": "",
    "text": "To read a csv file with columns [‘time’, ‘a’, ‘b’, ‘c’, ‘d’, ‘e’]:\ndf = pd.read_csv(file_str)   # header by default is infer\ndf = pd.read_csv(file_str, header=1, names=['time', 'a', 'b', 'c', 'd', 'e'])\nTo log data to a csv file with columns var1, var2, var3:\nimport pandas as pd\n\n# Define the variables for the new row\nvar1 = 'New Value 1'\nvar2 = 99\nvar3 = 'World'\n\n# Create a DataFrame with the new row\nnew_row = pd.DataFrame([{'var1': var1, \n                         'var2': var2, \n                         'var3': var3}])\n\n# Define the existing CSV file path\ncsv_file = 'test_data.csv'\n\n# Read the existing CSV file (if it exists)\ntry:\n    existing_data = pd.read_csv(csv_file)\nexcept FileNotFoundError:\n    existing_data = pd.DataFrame()\n\n# Append the new row to the existing data\nupdated_data = existing_data.append(new_row, ignore_index=True)\n\n# Write the updated data to the CSV file\nupdated_data.to_csv(csv_file, index=False)",
    "crumbs": [
      "Projects",
      "TILs",
      "Python",
      "PyData"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html",
    "href": "tips/040_PyTorch/augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "Augmentations improve generalization of the model by using specified transformations during training. They do not increase the number of samples in the dataset, instead, they transform the samples during training, so with each epoch training sees augmented image. The rate of augmentation is controled by torchvision.transforms.RandomApply.\nAugmentations can be found in torchvision.transforms module, or in albumentations which claims to be fast.\nimport torch\nfrom torchvision.transforms import ColorJitter, RandomApply, ToTensor\nfrom PIL import Image\nfrom PIL.ImageStat import Stat\nimport matplotlib.pyplot as plt\nfrom utils.plot import plot_pil_images\nimport numpy as np",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Augmentations"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#brightness",
    "href": "tips/040_PyTorch/augmentations.html#brightness",
    "title": "Augmentations",
    "section": "Brightness",
    "text": "Brightness\n\ndef print_stats(im, aug_imgs):\n    \"\"\"\n    Print \n    \"\"\"\n    mean_orig, stdev_orig = Stat(im).mean, Stat(im).stddev\n    stats_mean = torch.zeros(len(aug_imgs), 3)\n    stats_stdev = torch.zeros(len(aug_imgs), 3)\n    for i, img in enumerate(aug_imgs):\n        stats_mean[i,:] = torch.tensor(Stat(img).mean) / torch.tensor(mean_orig)\n        stats_stdev[i,:] = torch.tensor(Stat(img).stddev) / torch.tensor(stdev_orig)\n    print(f'Brightness min/max: {stats_mean.min():.02f} / {stats_mean.max():.02f}')\n    print(f'Contrast min/max: {stats_stdev.min():.02f} / {stats_stdev.max():.02f}')\n\n\naug_imgs = [ColorJitter(brightness=(0.5, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.58 / 0.96\nContrast min/max: 0.58 / 0.97",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Augmentations"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#contrast",
    "href": "tips/040_PyTorch/augmentations.html#contrast",
    "title": "Augmentations",
    "section": "Contrast",
    "text": "Contrast\n\naug_imgs = [ColorJitter(contrast=(0.25, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.96 / 1.16\nContrast min/max: 0.30 / 0.99",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Augmentations"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/augmentations.html#brightness-contrast",
    "href": "tips/040_PyTorch/augmentations.html#brightness-contrast",
    "title": "Augmentations",
    "section": "Brightness + contrast",
    "text": "Brightness + contrast\nTogether the have cummuliteve effect:\n\naug_imgs = [ColorJitter(brightness=(0.5, 1), contrast=(0.25, 1))(im) for _ in range(10)]\n_ = plot_pil_images(aug_imgs)\nprint_stats(im, aug_imgs)\n\nBrightness min/max: 0.53 / 1.01\nContrast min/max: 0.18 / 0.86",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Augmentations"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/feature_engineering.html",
    "href": "tips/040_PyTorch/feature_engineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Feature engineering is a critcal stage in any machine learning project. It’s main purpose is to extract features from the raw data and generate numerical values that can be used to train a machine learning model. In this notebook, we will explore the different types of features and how to generate them.",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/feature_engineering.html#label-encoding",
    "href": "tips/040_PyTorch/feature_engineering.html#label-encoding",
    "title": "Feature Engineering",
    "section": "Label encoding",
    "text": "Label encoding\nLabel encoding generates number for category:\n\nimport sklearn.preprocessing as preprocessing\ntargets = np.array([\"Sun\", \"Sun\", \"Moon\", \"Earth\", \"Monn\", \"Venus\"])\nlabelenc = preprocessing.LabelEncoder()\nlabelenc.fit(targets)\ntargets_trans = labelenc.transform(targets)\nprint(\"The original data\")\nprint(targets)\nprint(\"The transform data using LabelEncoder\")\nprint(targets_trans)\n\nThe original data\n['Sun' 'Sun' 'Moon' 'Earth' 'Monn' 'Venus']\nThe transform data using LabelEncoder\n[3 3 2 0 1 4]\n\n\nThe label encoding operation must be performed on both the train and test dataset at the same time. We can use .astype(\"category\") and pandas.Series.cat.coded to do the same:\n\ndf = pd.DataFrame({\"col1\": [\"Sun\", \"Sun\", \"Moon\", \"Earth\", \"Monn\", \"Venus\"]})\nprint(\"The original types of DataFrame\")\nprint(df.dtypes)\nprint(\"*\"*30)\ndf[\"col1\"] = df[\"col1\"].astype(\"category\")\nprint(\"The new types of DataFrame\")\nprint(df.dtypes)\nprint(\"*\"*30)\ndf[\"col1_label_encoding\"] = df[\"col1\"].cat.codes\nprint(\"The new column.\")\ndf\n\nThe original types of DataFrame\ncol1    object\ndtype: object\n******************************\nThe new types of DataFrame\ncol1    category\ndtype: object\n******************************\nThe new column.\n\n\n\n\n\n\n\n\n\ncol1\ncol1_label_encoding\n\n\n\n\n0\nSun\n3\n\n\n1\nSun\n3\n\n\n2\nMoon\n2\n\n\n3\nEarth\n0\n\n\n4\nMonn\n1\n\n\n5\nVenus\n4",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/feature_engineering.html#one-hot-encoding",
    "href": "tips/040_PyTorch/feature_engineering.html#one-hot-encoding",
    "title": "Feature Engineering",
    "section": "One-hot encoding",
    "text": "One-hot encoding\nEven though we generated numbers from catogory, most times these numbers have no order (i.e. they are nominal, not ordinal). In that case, proper way of encoding is to use one-hot encoding.\n\nimport sklearn.preprocessing as preprocessing\n\ntargets = np.array([\"Sun\", \"Sun\", \"Moon\", \"Earth\", \"Moon\",\n                    \"Venus\"])\nlabelEnc = preprocessing.LabelEncoder()\nnew_target = labelEnc.fit_transform(targets)\nonehotEnc = preprocessing.OneHotEncoder()\nonehotEnc.fit(new_target.reshape(-1, 1))\ntargets_trans = onehotEnc.transform(new_target.reshape(-1, 1))\nprint(\"The original data\")\nprint(targets)\nprint(\"The transform data using OneHotEncoder\")\nprint(targets_trans.toarray())\n\nThe original data\n['Sun' 'Sun' 'Moon' 'Earth' 'Moon' 'Venus']\nThe transform data using OneHotEncoder\n[[0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\n\n\n\ndf_new = pd.get_dummies(df, columns=[\"col1\"], prefix=\"Planet\")\ndf_new\n\n\n\n\n\n\n\n\ncol1_label_encoding\nPlanet_Earth\nPlanet_Monn\nPlanet_Moon\nPlanet_Sun\nPlanet_Venus\n\n\n\n\n0\n3\n0\n0\n0\n1\n0\n\n\n1\n3\n0\n0\n0\n1\n0\n\n\n2\n2\n0\n0\n1\n0\n0\n\n\n3\n0\n1\n0\n0\n0\n0\n\n\n4\n1\n0\n1\n0\n0\n0\n\n\n5\n4\n0\n0\n0\n0\n1",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/feature_engineering.html#weight-of-evidence-woe-encoding",
    "href": "tips/040_PyTorch/feature_engineering.html#weight-of-evidence-woe-encoding",
    "title": "Feature Engineering",
    "section": "Weight of Evidence (WOE) Encoding",
    "text": "Weight of Evidence (WOE) Encoding\nWOE is a technique used to encode categorical features for classification tasks. We assign several probabilites to the categories. For example for binary classification problems, WOE is defined as: \\[ WOE = \\log \\frac{p(1)}{p(0)} \\]\n\ndf = pd.DataFrame({\n    \"col1\": [\"Moon\", \"Sun\", \"Moon\", \"Sun\", \"Sun\"],\n    \"Target\": [1, 1, 0, 1, 0]\n})\ndf[\"Target\"] = df[\"Target\"].astype(\"float64\")\nprint(\"The original dataset\")\nprint(df)\nprint(\"*\" * 30)\nd = df.groupby([\"col1\"])[\"Target\"].mean().to_dict()\ndf[\"p1\"] = df[\"col1\"].map(d)\ndf[\"p0\"] = 1 - df[\"p1\"]\ndf[\"woe\"] = np.log(df[\"p1\"] / df[\"p0\"])\nprint(\"The new transform dataset\")\nprint(df)\n\nThe original dataset\n   col1  Target\n0  Moon     1.0\n1   Sun     1.0\n2  Moon     0.0\n3   Sun     1.0\n4   Sun     0.0\n******************************\nThe new transform dataset\n   col1  Target        p1        p0       woe\n0  Moon     1.0  0.500000  0.500000  0.000000\n1   Sun     1.0  0.666667  0.333333  0.693147\n2  Moon     0.0  0.500000  0.500000  0.000000\n3   Sun     1.0  0.666667  0.333333  0.693147\n4   Sun     0.0  0.666667  0.333333  0.693147",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/models.html",
    "href": "tips/040_PyTorch/models.html",
    "title": "Models",
    "section": "",
    "text": "torch.nn.BatchNorm2d(\n_num_features_,\n_eps=1e-05_,\n_momentum=0.1_,\n_affine=True_,\n_track_running_stats=True_,\n_device=None_,\n_dtype=None_\n)\nOne must define all layers in the __init__ in order to initialize them. Remember that forward method will be called during training so no layers with parameters should be defined there.\n\nSoftmax\nFor multi-classifications, Softmax converts logits to predictions:\n\npredictions = nn.Softmax(dim=1)(logits)",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Models"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/quadratic_function_fit.html",
    "href": "tips/040_PyTorch/quadratic_function_fit.html",
    "title": "Quadratic function fitting",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\nfrom shared.step_by_step import StepByStep\n\nfrom torchviz import make_dot\nplt.style.use('fivethirtyeight')\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nLet’s make up some data:\nnp.random.seed(43)\na0, a1, a2 = 5., -1.5, +4.\nN = 100\nx = -2.5 + 5*np.random.rand(N,1)\nepsilon = np.random.randn(N,1)\ny = a0 + a1*x + a2*x**2 + epsilon\nplt.plot(x,y,'.')\nplt.show()",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Quadratic function fitting"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "href": "tips/040_PyTorch/quadratic_function_fit.html#same-but-using-sklearn.pipeline",
    "title": "Quadratic function fitting",
    "section": "Same but using sklearn.pipeline",
    "text": "Same but using sklearn.pipeline\nOne can make the process more streamlined using Pipeline:\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                  ('linear', LinearRegression(fit_intercept=False))])\n# fit to an order-2 polynomial data\nmodel = model.fit(x, y)\nprint(model.named_steps['linear'].coef_)\nprint(f'Real values {a0}, {a1}, {a2}')\n\n[[ 4.98812164 -1.61954639  4.02342307]]\nReal values 5.0, -1.5, 4.0",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Quadratic function fitting"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/quadratic_function_fit.html#data-preparation",
    "href": "tips/040_PyTorch/quadratic_function_fit.html#data-preparation",
    "title": "Quadratic function fitting",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nnp.random.seed(42)\nN = len(x)\nidx = list(range(N))\nnp.random.shuffle(idx)\n\n\nsplit_idx = int(.8*N)\ntrain_idx = idx[:split_idx]\nval_idx = idx[split_idx:]\ntrain_x = torch.as_tensor(x[train_idx], device=device).float()\ntrain_y = torch.as_tensor(y[train_idx], device=device).float()\nval_x = torch.as_tensor(x[val_idx], device=device).float()\nval_y = torch.as_tensor(y[val_idx], device=device).float()\n\n\ntrain_dataset = TensorDataset(train_x, train_y)\nval_dataset = TensorDataset(val_x, val_y)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Quadratic function fitting"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/quadratic_function_fit.html#training",
    "href": "tips/040_PyTorch/quadratic_function_fit.html#training",
    "title": "Quadratic function fitting",
    "section": "Training",
    "text": "Training\n\nmodel=nn.Sequential(\n        nn.Linear(1,10),\n        nn.ReLU(),\n        nn.Linear(10,1)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\nsbs = StepByStep(model, optimizer, loss_fn)\n\nLet’s train for 200 epoch and plot losses:\n\nsbs.set_loaders(train_loader, val_loader)\nsbs.train(200)\n\n\nsbs.plot_losses()\n\n\n\n\n\n\n\n\nLet’s make predictions:\n\ntest =np.linspace(-4.,4.,num=N).reshape(-1,1)\ntest_predictions = sbs.predict(test)\nplt.plot(x,y,'.')\nplt.plot(test,test_predictions,'.')\nplt.show()\n\n\n\n\n\n\n\n\nThis is good though, unfortunatelly, the true values of quadratic function are now lost in the sea of weights of the the two linear layers:\n\nsbs.model.state_dict()\n\nOrderedDict([('0.weight',\n              tensor([[ 1.3475],\n                      [-2.2383],\n                      [-2.1243],\n                      [ 2.0004],\n                      [-1.9875],\n                      [-2.2052],\n                      [ 0.1436],\n                      [-1.8479],\n                      [ 2.6974],\n                      [ 2.1781]])),\n             ('0.bias',\n              tensor([-1.2300, -3.2117,  0.8249, -1.5303, -0.2013, -2.3025,  1.3949, -0.0182,\n                       0.2817, -3.1922])),\n             ('2.weight',\n              tensor([[0.7446, 2.5052, 1.1556, 1.2103, 1.3438, 1.6768, 0.8039, 1.2448, 1.4132,\n                       2.6946]])),\n             ('2.bias', tensor([1.5188]))])",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Quadratic function fitting"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html",
    "href": "tips/040_PyTorch/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "Jaccard Index (aka Intersection over Union, aka IoU)\nimport torch\nfrom torchmetrics.classification import BinaryJaccardIndex, JaccardIndex",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Metrics"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html#binary",
    "href": "tips/040_PyTorch/metrics.html#binary",
    "title": "Metrics",
    "section": "Binary",
    "text": "Binary\n\ntarget = torch.tensor([[1, 1], [1, 0]])\npreds = torch.tensor([[1, 1], [0, 0]])\nmetric = BinaryJaccardIndex()\nmetric(preds, target)\n\ntensor(0.6667)",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Metrics"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/metrics.html#multiclass",
    "href": "tips/040_PyTorch/metrics.html#multiclass",
    "title": "Metrics",
    "section": "Multiclass",
    "text": "Multiclass\n\ntarget = torch.randint(0, 2, (10, 25, 25))\npred = target.clone()\npred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]\njaccard = JaccardIndex(task=\"multiclass\", num_classes=2)\njaccard(pred, target)\n\ntensor(0.9660)",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Metrics"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/linear_regression.html",
    "href": "tips/040_PyTorch/linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Inspiration by Daniel Voigt Godoy’s books",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Linear Regression"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/linear_regression.html#sklearn.linearregression",
    "href": "tips/040_PyTorch/linear_regression.html#sklearn.linearregression",
    "title": "Linear Regression",
    "section": "sklearn.LinearRegression",
    "text": "sklearn.LinearRegression\nLet’s first use very simple linear regression model from sklearn, and only 2 columns:\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nr2 = lr.score(X_valid, y_valid)\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\nplt.title(f'$w_0$ = {float(lr.intercept_):0.3f}, $w_1$ = {float(lr.coef_):0.3f}, $r^2$ = {r2:0.3f}')\nplt.show()\n\n\n\n\n\n\n\n\nSo we see that on average people left 10.5% tip. The r2 score is 0.597.",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Linear Regression"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/linear_regression.html#manual",
    "href": "tips/040_PyTorch/linear_regression.html#manual",
    "title": "Linear Regression",
    "section": "Manual",
    "text": "Manual\n\nw0 = 0.5\nw1 = 0.5\nN = len(X_train)\nalpha = 2e-4\n\nw1_list = [w0]\nw0_list = [w1]\nfor i in range(50):\n    y_new = w0 + X_train*w1\n    cost_function = 1/N *np.sum((y_new - y_train)**2)\n\n    w0 -= alpha*(1/N * 2 * np.sum(y_new - y_train))\n    w1 -= alpha*(1/N * 2 * np.sum((y_new - y_train)*X_train))\n\n    w0_list.append(w0)\n    w1_list.append(w1)\n\nsns.lineplot(x=range(len(w0_list)), y=w0_list, color='red', label='w0')\nsns.lineplot(x=range(len(w1_list)), y=w1_list, color='blue', label='w1')\n\nplt.title(f'$w_0$ = {w0:0.3f}, $w_1$ = {w1:0.3f}')\nplt.show()\n\n\n\n\n\n\n\n\nInteresting that w1 is slightly off 12.1% vs 10% (regularization is not a culprit). Let’s plot together with the scatter plot:\n\n# Create scatterplot\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips)\n\n# Add title and axis labels\nplt.title(\"Tips vs Total Bill\")\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n# seaborn plot a line\nsns.lineplot(x=X_train.flatten(), y=w0 + w1*X_train.flatten(), color='red', label='Linear Regression')\n# Show the plot\nplt.show()",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Linear Regression"
    ]
  },
  {
    "objectID": "tips/040_PyTorch/linear_regression.html#multivariate-linear-regression",
    "href": "tips/040_PyTorch/linear_regression.html#multivariate-linear-regression",
    "title": "Linear Regression",
    "section": "Multivariate linear regression",
    "text": "Multivariate linear regression\nLet’s now include all variables:\n\n_ = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n\n\n\n\n\n\n\n\n\n_ = sns.boxplot(x=\"sex\", y=\"tip\", data=tips)\n\n\n\n\n\n\n\n\nThis graph might not mean males are bigger tipers, since it might have been that more males ate in bigger groups as well. Plotting relative tip (i.e. tip/total_bill) might be more informative:\n\ntips['relative_tip'] = tips['tip'] / tips['total_bill']\n_ = sns.boxplot(x=\"sex\", y=\"relative_tip\", data=tips)\n\n\n\n\n\n\n\n\nIndeed, women left larger percentage of tip (then again, they might have had smaller portionsl there are many angles one can look at this data). How about compare group sizes:\n\n_ = sns.violinplot(x=\"sex\", y=\"size\", data=tips)\n\n\n\n\n\n\n\n\nThat seems very similar distribution.\n\n_ = sns.boxplot(x=\"size\", y=\"tip\", data=tips)",
    "crumbs": [
      "Projects",
      "TILs",
      "PyTorch",
      "Linear Regression"
    ]
  },
  {
    "objectID": "tips/015_Cloud/docker.html",
    "href": "tips/015_Cloud/docker.html",
    "title": "Docker",
    "section": "",
    "text": "A docker contain is a great way to set up an environment. Once the docker container is built and running, one can attach to it from VSCode and use it as a remote machine. To do this, install “Remote Development” extension pack, then one can attach the container in VSCode by going to a command pallette (Cmd+Shift+P) and typing “Attach to running container”. If typing sudo all the time is annoying add user to the docker group:\nand (maybe, not always) restart the machine.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Docker"
    ]
  },
  {
    "objectID": "tips/015_Cloud/docker.html#build-a-docker-image",
    "href": "tips/015_Cloud/docker.html#build-a-docker-image",
    "title": "Docker",
    "section": "Build a Docker image",
    "text": "Build a Docker image\nOne needs Dockerfile and requirements.txt. See Dockerfile for details on steps.\nWe’ll use docker build -t &lt;some_image_name&gt; . to build docker image from files in the current folder (indicated by the . at the end)\nsudo docker build -t my_project:1.0 .  \nVery common command:\nsudo docker images  # to see existing images",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Docker"
    ]
  },
  {
    "objectID": "tips/015_Cloud/docker.html#running-a-container",
    "href": "tips/015_Cloud/docker.html#running-a-container",
    "title": "Docker",
    "section": "Running a container",
    "text": "Running a container\nAfter building an Image we now run a container (one can run many containers from the same image).\nImportant flags: | flag | action | |———-|———-| | -it/-d | runs container in interactivemode |\n| -rm | removes docker after it’s done the task |\n| -p X:Y | maps port X on docker container to port Y on local machine |\nVery common commands:\nsudo docker run &lt;image name&gt;  # to run a container \nsudo docker ps      # to see running containers\nsudo docker container stop &lt;container name&gt;  # to stop specific container\nsudo docker container stop $(sudo docker container ls -aq)  # to stop all containers\n\nDetached mode\nTo run container in detached mode requires defining entry point in the Dockerfile (typically last command there see CMD or ENTRYPOINT):\nsudo docker run -d -p 8501:8501 my_project:1.0\nNow simply go to localhost:8501 in your browser and you should see the app.\nTo connect to the terminal of a running container (find container name using sudo docker ps):\nsudo docker exec -it &lt;container_name&gt; bash\n\n\nInteractive mode\n\nRun container and then run the some commnad (here for example Streamlit app)\n\nsudo docker run -p 8501:8501 -it my_project:1.0 bash    # bash here is what will run \nstreamlit run entry_file.py --server.port 8501\n\n\nTo exit from a container’s bash use:\nexit\nIf you had flag -rm this exit would kill the container. Without the flag the container would still be running (see with sudo docker ps).",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Docker"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/glue.html",
    "href": "tips/015_Cloud/aws/glue.html",
    "title": "AWS Glue",
    "section": "",
    "text": "Credits: Educative.io\nLet’s create: - DynamoDB table, and insert some data into it (see script below) - an IAM role (and give it AWSGlueServiceRole and AWSGluePolicy) - a Glue crawler and point it to the DynamoDB table as source to crawl - a new table that will store extracted metadata Now we can run the crawler that will create a table in the Glue Data Catalog.\nNotice that we didn’t have to specify the schema of the table, Glue automatically inferred it from the data.\nNext, we’ll set up a Glue ETL job that will extract data from the DynamoDB table, transform it, and load it into an S3 bucket. We create: - a S3 bucket - set up “Visual ETL” from Glue and add 3 nodes: - DynamoDB “movies” table as “Source” - SQL query as “Transform” - S3 bucket as “Target” - configure the job: - Set up an example SQL transform script (that will remove “s” from the index, and select only PG-13 movies (see below))\nFinally, run the Visual ETL job, after its completion, check the S3 bucket for the results. You will see bunch of files starting with “run…” that contain the results of the ETL job (we can query them with SQL query). These files can then be used for any subsequent processing, such as ML training.\nWe have successfully extracted data from DynamoDB, transformed it, and loaded it into S3.",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Glue"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/glue.html#appendix",
    "href": "tips/015_Cloud/aws/glue.html#appendix",
    "title": "AWS Glue",
    "section": "Appendix",
    "text": "Appendix\nInsert data into DynamoDB table:\nimport csv\nimport boto3\n\n# Initialize the DynamoDB client\ndynamodb = boto3.client(\n  'dynamodb',\n  aws_access_key_id='ZYX',\n  aws_secret_access_key='XYZ',\n  region_name='us-east-1'\n)\n\n# Define a function to insert a whole CSV into the table using the insert_csv method\ndef insert_csv():\n    csv_file = \"Movies.csv\"\n\n    with open(csv_file, 'r') as csvfile:\n        csv_reader = csv.reader(csvfile)\n        header = next(csv_reader)  # First row is the header\n        for row in csv_reader:\n            item = {}\n            for i in range(len(header)):\n                item[header[i]] = {'S': row[i]}\n            \n            response = dynamodb.put_item(\n                TableName='Movies',\n                Item=item\n            )\n\n# # Insert four items into the table\ninsert_csv()\nprint('Table populated successfully.')\nSQL query:\nSELECT \n    -- Extract only numbers from the string type index column and convert it to an integer and rename it as cleaned_index\n    CAST(REPLACE(index, 's', '') AS INT) AS cleaned_index,\n    -- Include other columns you want to select\n    listed_in,\n    duration,\n    date_added,\n    country,\n    director,\n    rating,\n    release_year,\n    title\nFROM \n    myDataSource\nWHERE \n    -- Choose only the movies that have PG-13 rating\n    rating = 'PG-13'\nORDER BY \n    -- Sort the result set based on the values in the cleaned_index column in ascending order\n    cleaned_index;",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "AWS Glue"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/ec2_cli.html",
    "href": "tips/015_Cloud/aws/ec2_cli.html",
    "title": "Manage EC2 via CLI",
    "section": "",
    "text": "Credits Educative.io\nInstall AWS CLI:\ncurl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\nwhich aws\naws --version\nCheck out help:\naws ec2 help  | grep &lt;some word&gt;\nTo create a user (and also make sure to set up the AmazonEC2FullAccess user policy for the IAM user):\naws iam create-user --user-name testuser\naws iam create-group --group-name testgroup\naws iam attach-group-policy --group-name testgroup --policy-arn arn:aws:iam::aws:policy/AdministratorAccess\naws iam attach-group-policy --group-name testgroup --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\naws iam add-user-to-group --user-name testuser --group-name testgroup\nTo see key value pairs:\naws ec2 describe-key-pairs\naws ec2 describe-key-pairs --output table\n\naws ec2 create-key-pair --key-name ec2project --query 'KeyMaterial' --output text &gt; ec2project.pem\nA VPC (virtual private cloud) is a virtual private network in Amazon’s data centers that has restrictions on its access. Within this VPC, all your instances and services can communicate, but other AWS customers can’t see them.\naws ec2 describe-vpcs\nIn Amazon EC2, security groups are the virtual firewalls that control the inbound and outbound traffic for EC2 instances.\naws ec2 describe-security-groups --filters Name=vpc-id,Values=$VPC_ID --query \"SecurityGroups[*].GroupId\"\naws ec2 describe-security-groups --filters Name=vpc-id,Values=$VPC_ID --query \"SecurityGroups[*].GroupId\" --output table\nAmazon Subnets are smaller ranges of IP addresses within a VPC:\naws ec2 describe-availability-zones --query \"AvailabilityZones[*].{\"RegionName\":RegionName,\"ZoneName\":ZoneName}\" --output table\naws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 172.31.0.0/16 --availability-zone us-west-1c --tag-specifications 'ResourceType=subnet, Tags=[{Key=name,Value=subnet-public-a},{Key=learning, Value=educative}]'\naws ec2 describe-subnets\naws ec2 modify-subnet-attribute --subnet-id $SUBNET_ID --map-public-ip-on-launch\nThe default security group has some default inbound and outbound rules. Let’s add port 5000 utilized by Flask\naws ec2 authorize-security-group-ingress --group-id $SG_ID --protocol tcp --port 5000 --cidr 0.0.0.0/0\nand port 22 used by ssh:\naws ec2 authorize-security-group-ingress --group-id $SG_ID --protocol tcp --port 22 --cidr 0.0.0.0/0\nTo create an instance first find the AMI ID of Linux you want to use (for Ubuntu: ami-0ce2cb35386fc22e9 for Amazon Linux: Amazon Linux 2 AMI: ami-0082110c417e4726e)\naws ec2 run-instances --image-id $AMI_ID --count 1 --instance-type t2.micro --key-name ec2project --security-group-id $SG_ID --subnet-id $SUBNET_ID\nTo check the status of instances:\naws ec2 describe-instances\naws ec2 describe-instances --instance-id i-04918bcdbf7cffd64 --output table\naws ec2 describe-instances --instance-ids i-04918bcdbf7cffd64 --output table --query 'Reservations[*].Instances[*].[InstanceId,State.Name]'\naws ec2 describe-instances --output table --query 'Reservations[*].Instances[*].[InstanceId,State.Name]'\nTo ssh into a EC2 instance:\nssh -i ec2project.pem ec2-user@13.57.29.210\nTo deploy anything to EC2 instance first change mode: chmod 400 ec2project.pem\nscp -i ec2project.pem flask.tar.gz ec2-user@13.57.29.210:flask.tar.gz\nFor Amazon Linux AMI username is ec2-user.\nInstall pip and packages:\ncurl -O https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\npython3 -m pip install flask flask_sqlalchemy\ntar -xvf flask.tar.gz\nexport FLASK_APP=application.py \nflask run --host=0.0.0.0\nTo terminate instance:\naws ec2 terminate-instances --instance-ids i-0ab286b95c7dd99da\nTo delete key-pair:\naws ec2 delete-key-pair --key-name ec2project\nTo delete subnet:\naws ec2 delete-subnet --subnet-id $SUBNET_ID",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Manage EC2 via CLI"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/aws_ecs.html",
    "href": "tips/015_Cloud/aws/aws_ecs.html",
    "title": "1. Create private AWS ECR repository",
    "section": "",
    "text": "Create private AWS ECR repository named “ecs-container-repo”\nPush the locally stored image to the “ecs-container-repo” repository\nCreate an IAM role named “ecs-task-role” that will be “Elastic Container Service Task” based and have two policies attached to it: “AmazonECSTaskExecutionRolePolicy” and “CloudWatchLogsFullAccess”\nCreate an ECS Cluster named “ecs-demo-cluster” with the launch type as “AWS Fargate (serverless)”\nCreate a task definition under “ecs-demo-cluster”, named “ecs-demo-task-definition” with the following configurations:\n\nLaunch type: AWS Fargate\nOperating System/Architecture: Linux/X86_64\nCPU: .5 vCPU\nMemory: 1 GB\nTask Role: None\nTask execution role: ecs-task-role\nContainer name: ecs-demo-app\nImage URI: &lt;ECR repository URI created in step 2&gt;\nContainer port: 80\n\nCreate a new security group “ecs-demo-app-sg” with description “ECS Demo App Security Group” and add a inbound rule to allow HTTP traffic on port 80 from anywhere.\nDeploy task definition\n\nNow run new task “ecs-demo-task-definition” keep “Launch type” as “FARGATE” and the “Platform version” as “LATEST.” Select us-east-1a as “Subnets”"
  },
  {
    "objectID": "tips/015_Cloud/aws/dynamodb.html",
    "href": "tips/015_Cloud/aws/dynamodb.html",
    "title": "AWS DocumentDB",
    "section": "",
    "text": "Credits Educative.io\nAWS DocumentDB creates a MongoDB database cluster, and we’ll store some data in it. We can also create a snapshot and create a new cluster from it.\nSome key points:\n\nwe need to create a DocumentDB cluster. There are two types of clusters using DocumentDB (Instance-based clusters and Elastic clusters). Instance-based clusters are used when we’re working with applications that have predictable and consistent data and number of instances is fixed. Elastic clusters are used when we’re working with applications that have unpredictable data and scaling is managed by AWS.\nwe need to create a security group, and add inbound rule, “Custom TCP”, “Port range” to 27017 (this is the default port through which our DocumentDB cluster communicates), and add security group associated with our Cloud9 environment from the drop-down list available (this security group will look like aws-cloud9-DocDBEnv-… | sgxxxxxxxxx.).\nwe can then install MongoDB shell on our Cloud9 environment, and connect to our DocumentDB cluster using the following command:\n\necho -e “[mongodb-org-4.0] =MongoDB Repository=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/4.0/x86_64/ =https://www.mongodb.org/static/pgp/server-4.0.asc” | sudo tee /etc/yum.repos.d/mongodb-org-4.0.repo &&/ sudo yum install -y mongodb-org-shell\nwget https://truststore.pki.rds.amazonaws.com/global/global-bundle.pem\nmongo –ssl –host docdb-2024-01-13-00-57-19.cluster-cinfonv7x3oh.us-east-1.docdb.amazonaws.com:27017 –sslCAFile global-bundle.pem –username educative –password XYZ\n\n\nMongoDB shell has it’s own language, and we can run commands like show dbs, db.createCollection(“users”), db.users.insert({name: “John”}), db.users.find(), etc. For example, to insert a row into a mp3players collection, we can run the following command:\ndb.mp3playlist.insertMany([\n   {\n      id: 1,\n      title: \"Song 2\",\n      artist: \"Blur\",\n      duration: \"5:47\",\n      album: \"Blur\"\n    }\n])\n\ndb.mp3playlist.find({ \"artist\": \"XYZ\" })"
  },
  {
    "objectID": "tips/015_Cloud/aws/ec2_autoscaling.html",
    "href": "tips/015_Cloud/aws/ec2_autoscaling.html",
    "title": "Auto Scaling Group",
    "section": "",
    "text": "Credits Educative.io\nWe’ll create Auto Scaling Group (ASG), under EC2, and stress machines to see how auto-scaling works in action.\nLet’s create:\nWe then stress the instance to increase the load above 20% for 4 minutes, this triggers creating another instance automatically per auto-scaler:",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Auto Scaling Group"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/ec2_autoscaling.html#to-install-node-and-express-on-a-new-ec2-instance",
    "href": "tips/015_Cloud/aws/ec2_autoscaling.html#to-install-node-and-express-on-a-new-ec2-instance",
    "title": "Auto Scaling Group",
    "section": "To install node and express on a new EC2 instance",
    "text": "To install node and express on a new EC2 instance\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\n. ~/.nvm/nvm.sh\nnvm install 16\nThen install express and ip:\nmkdir eduapp\ncd eduapp/\nnpm install --save express\nnpm install --save ip\ncat &gt; index.js &lt;&lt; EOF\nput this in index.js:\nconst express = require('express');\nvar ip = require(\"ip\");\n\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\napp.get('/',(req, res) =&gt; res.send(ip.address()));\napp.listen(PORT, () =&gt; console.log('Server listening at port 3000'))\nEOF",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Auto Scaling Group"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/host_static_website.html",
    "href": "tips/015_Cloud/aws/host_static_website.html",
    "title": "Hosting a static website via AWS EC2 and Lambda",
    "section": "",
    "text": "Credits Educative.io\n\nVia EC2\nBefore we create EC2 instance we need to create Security group and IAM: - Security groups are rules that manage the inbound and outbound traffic for the instance. In simple terms, they can be considered a firewall for the instance. - AWS Identity Access and Management (IAM) roles are AWS identities with an identity-based policy specifying their access. So far, this is similar to IAM users. However, what makes roles unique is that instead of being directly associated with someone, anyone can assume a role, including IAM users and AWS services. This gives them temporary credentials to access resources and services based on the policy. We can allow our EC2 instances to access other services using IAM roles.\nLet’s create an EC2 instance and host a simple website described by this html:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Sample webpage&lt;/title&gt;\n    &lt;style&gt;\n        .container {\n            display: flex;\n            align-items: center;\n        }\n        .image {\n            max-width: 20%;\n        }\n        .text {\n            max-width: 80%;\n            padding: 20px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;img class=\"image\" src=\"&lt;IMAGE URL&gt;\" alt=\"Sample Image\"&gt;\n        &lt;div class=\"text\"&gt;\n            &lt;h1&gt;Sample Text&lt;/h1&gt;\n            &lt;p&gt;This is some sample text that is arranged side by side with an image.&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNext we install and start nginx that will host our page:\n#!/bin/bash\nsudo apt update\nsudo apt -y install awscli\nsudo apt -y install nginx\nsudo systemctl start nginx.service\nsudo systemctl status nginx.service\n\n\n\nimage.png\n\n\n\n\nVia AWS Lambda\nWe don’t need to use EC2 instance if the page is static, and instead use serverless AWS Lambda function, running following python code as event handler:\nimport boto3\n\ns3 = boto3.client('s3')\ndef lambda_handler(event, context):\n    bucket_name = '&lt;BUCKET-NAME&gt;'\n    index_file = 'index.html'\n    \n    try:\n        response = s3.get_object(Bucket=bucket_name, Key=index_file)\n        content = response['Body'].read().decode('utf-8')\n        \n        return {\n            'statusCode': 200,\n            'headers': {\n                'Content-Type': 'text/html',\n            },\n            'body': content\n        }\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': str(e)\n        }",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Hosting a static website via AWS EC2 and Lambda"
    ]
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html",
    "href": "tips/015_Cloud/aws/sam.html",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "",
    "text": "SAM is all about automation and in theory no need to go to AWS console. SAM uses a CLI and simplified templates which are extension of CloudFormation templates (basically a yaml file). It supports Lambda, API Gateway, DynamoDB, and S3, and event like StepFunctions, SNS, SQS, etc.\nWe’ll create a Hello World example. First install it: pip install aws-sam-cli. We run sam init and chose hello-world options.\nThe template is in template.yaml and the only required fields are transform and resources.\nWe then build and deploy the application.\ncd sam-app && sam build   # default name is sam-app \nsam deploy --stack-name clab-stack --s3-bucket &lt;TEMPLATE-BUCKET-NAME&gt; --capabilities CAPABILITY_IAM   # s3-bucket is optional, capabilities are for CloudFormation to create IAM role\nThis will build CloudFormation stack, API Gateway, Lambda, and IAM role, and create some files in S3 bucket. Lambda is a Node.js function triggered by API Gateway."
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html#basics-of-sam",
    "href": "tips/015_Cloud/aws/sam.html#basics-of-sam",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "",
    "text": "SAM is all about automation and in theory no need to go to AWS console. SAM uses a CLI and simplified templates which are extension of CloudFormation templates (basically a yaml file). It supports Lambda, API Gateway, DynamoDB, and S3, and event like StepFunctions, SNS, SQS, etc.\nWe’ll create a Hello World example. First install it: pip install aws-sam-cli. We run sam init and chose hello-world options.\nThe template is in template.yaml and the only required fields are transform and resources.\nWe then build and deploy the application.\ncd sam-app && sam build   # default name is sam-app \nsam deploy --stack-name clab-stack --s3-bucket &lt;TEMPLATE-BUCKET-NAME&gt; --capabilities CAPABILITY_IAM   # s3-bucket is optional, capabilities are for CloudFormation to create IAM role\nThis will build CloudFormation stack, API Gateway, Lambda, and IAM role, and create some files in S3 bucket. Lambda is a Node.js function triggered by API Gateway."
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html#full-stack-web-application",
    "href": "tips/015_Cloud/aws/sam.html#full-stack-web-application",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "Full stack web application",
    "text": "Full stack web application\nNow we build a full stack web app, we’ll have template that will create a DynamoDB table, then Lambdas which will be APIs called by API Gateway."
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html#dynamodb-table",
    "href": "tips/015_Cloud/aws/sam.html#dynamodb-table",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "DynamoDB table",
    "text": "DynamoDB table\nWe use AWS::Serverless::SimpleTable resource to create DynamoDB table.\ntemplate.yaml file\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n Infrastructure for CLAB \"Building and Deploying Serverless Applications using SAM\"\nResources:\n  CourseTable:\n    Type: AWS::Serverless::SimpleTable\n    Properties:\n      TableName: Courses\n      PrimaryKey:\n        Name: ID\n        Type: Number\nOutputs:\n  DynamoDBTableName:\n    Description: \"DynamoDB Table Name\"\n    Value: !Ref CourseTable\nlet’s add a few items:\naws dynamodb batch-write-item --request-items '{\n  \"Courses\": [\n    {\"PutRequest\": {\"Item\": {\"ID\": {\"N\": \"1\"}, \"CourseName\": {\"S\": \"The Detailed Workings of AWS S3\"}, \"CourseURL\": {\"S\": \"https://www.educative.io/courses/detailed-workings-aws-s3\"}, \"ImageURL\": {\"S\": \"https://www.educative.io/cdn-cgi/image/format=auto,width=950,quality=75/v2api/collection/10370001/6071752037236736/image/6458556865314816\"}}}},\n    {\"PutRequest\": {\"Item\": {\"ID\": {\"N\": \"2\"}, \"CourseName\": {\"S\": \"The Good Parts of AWS: Cutting Through the Clutter\"}, \"CourseURL\": {\"S\": \"https://www.educative.io/courses/good-parts-of-aws\"}, \"ImageURL\": {\"S\": \"https://www.educative.io/cdn-cgi/image/format=auto,width=950,quality=75/v2api/collection/10370001/5943367834796032/image/4534786195456000\"}}}},\n    {\"PutRequest\": {\"Item\": {\"ID\": {\"N\": \"3\"}, \"CourseName\": {\"S\": \"Create an EKS Cluster and Deploy an Application\"}, \"CourseURL\": {\"S\": \"https://www.educative.io/cloudlabs/create-an-eks-cluster-and-deploy-an-application\"}, \"ImageURL\": {\"S\": \"https://www.educative.io/cdn-cgi/image/format=auto,width=750,quality=75/v2api/collection/10370001/5268241073831936/image/6466459398832128\"}}}},\n    {\"PutRequest\": {\"Item\": {\"ID\": {\"N\": \"4\"}, \"CourseName\": {\"S\": \"Educative Bot with Lambda Function Fulfillment using AWS LEX\"}, \"CourseURL\": {\"S\": \"https://www.educative.io/cloudlabs/educative-bot-with-lambda-function-fulfillment-using-aws-lex\"}, \"ImageURL\": {\"S\": \"https://www.educative.io/cdn-cgi/image/format=auto,width=750,quality=75/v2api/collection/10370001/6744845660717056/image/6171933378609152\"}}}}\n  ]\n}'"
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html#lambda-functions",
    "href": "tips/015_Cloud/aws/sam.html#lambda-functions",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "Lambda functions",
    "text": "Lambda functions\nWe use AWS::Serverless::Function resource to create Lambda function.\nTemplate.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: &gt;\n  Lambda functions for CRUD operations\nGlobals:\n  Function:\n    Timeout: 3\n    Tracing: Active\n    Runtime: nodejs18.x\nResources:\n  CourseTable:\n    Type: AWS::Serverless::SimpleTable\n    Properties:\n      TableName: Courses\n      PrimaryKey:\n        Name: ID\n        Type: Number\n  GetCourses:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: readCourses.readCourses\n      Environment:\n        Variables:\n          COURSE_TABLE: !Ref CourseTable\n      Role: \n        !Sub arn:aws:iam::${AWS::AccountId}:role/ClabLambdaRole\n  InsertCourse:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: createCourses.createCourses\n      Environment:\n        Variables:\n          COURSE_TABLE: !Ref CourseTable\n      Role: \n        !Sub arn:aws:iam::${AWS::AccountId}:role/ClabLambdaRole\n  UpdateCourse:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: updateCourses.updateCourses\n      Environment:\n        Variables:\n          COURSE_TABLE: !Ref CourseTable\n      Role: \n        !Sub arn:aws:iam::${AWS::AccountId}:role/ClabLambdaRole\n  DeleteCourse:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: functions/\n      Handler: deleteCourses.deleteCourses\n      Environment:\n        Variables:\n          COURSE_TABLE: !Ref CourseTable\n      Role: \n        !Sub arn:aws:iam::${AWS::AccountId}:role/ClabLambdaRole\nOutputs:\n  DynamoDBTableName:\n    Description: \"DynamoDB Table Name\"\n    Value: !Ref CourseTable\n  GetCourses:\n    Description: \"GetCourses Lambda Function ARN\"\n    Value: !GetAtt GetCourses.Arn\n  InsertCourse:\n    Description: \"InsertCourse Lambda Function ARN\"\n    Value: !GetAtt InsertCourse.Arn\n  UpdateCourse:\n    Description: \"UpdateCourse Lambda Function ARN\"\n    Value: !GetAtt UpdateCourse.Arn\n  DeleteCourse:\n    Description: \"DeleteCourse Lambda Function ARN\"\n    Value: !GetAtt DeleteCourse.Arn\nA ClabLambdaRole role (already created) is used for all Lambda functions. It has the following policy:\n\"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:Query\",\n                \"dynamodb:GetItem\",\n                \"dynamodb:PutItem\",\n                \"dynamodb:UpdateItem\",\n                \"dynamodb:DeleteItem\"\n              ],\n              \"Effect\": \"Allow\",\n              \"Resource\": [\n                \"arn:aws:dynamodb:us-east-1:*:table/Courses\"\n              ]\n            }\n          ]"
  },
  {
    "objectID": "tips/015_Cloud/aws/sam.html#apis-via-openapi",
    "href": "tips/015_Cloud/aws/sam.html#apis-via-openapi",
    "title": "AWS Serverless Application Model (SAM)",
    "section": "APIs (via OpenAPI)",
    "text": "APIs (via OpenAPI)\nWe use AWS::Serverless::Api resource to create REST API. An OpenAPI document defines the configurations of the API.\napi.yaml\nswagger: \"2.0\"\ninfo:\n  version: \"1.0\"\n  title: \"Courses\"\nbasePath: \"Dev/\"\nschemes:\n- \"https\"\npaths:\n  /course:\n    get:\n      responses: {}\n      x-amazon-apigateway-integration:\n        credentials: \n           Fn::Sub: arn:aws:iam::${AWS::AccountId}:role/ClabAPIRole\n        type: \"aws_proxy\"\n        httpMethod: \"POST\"\n        uri:\n          Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${GetCourses.Arn}/invocations\n        passthroughBehavior: \"when_no_match\"\n\n    post:\n      consumes:\n      - \"application/json\"\n      produces:\n      - \"application/json\"\n      responses: {}\n      x-amazon-apigateway-integration:\n        type: \"aws_proxy\"\n        credentials: \n           Fn::Sub: arn:aws:iam::${AWS::AccountId}:role/ClabAPIRole\n        httpMethod: \"POST\"\n        uri:\n          Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${InsertCourse.Arn}/invocations\n        responses:\n          default:\n            statusCode: \"200\"\n        passthroughBehavior: \"when_no_match\"\n\n  /course/{courseId+}:\n    put:\n      produces:\n      - \"application/json\"\n      parameters:\n      - name: \"courseId\"\n        in: \"path\"\n        required: true\n        type: \"string\"\n      responses: {}\n      x-amazon-apigateway-integration:\n        credentials: \n           Fn::Sub: arn:aws:iam::${AWS::AccountId}:role/ClabAPIRole\n        httpMethod: \"POST\"\n        uri:\n          Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${UpdateCourse.Arn}/invocations\n        responses:\n          default:\n            statusCode: \"200\"\n        passthroughBehavior: \"when_no_match\"\n        type: \"aws_proxy\"\n\n    delete:\n      responses: {}\n      x-amazon-apigateway-integration:\n        credentials: \n          Fn::Sub: arn:aws:iam::${AWS::AccountId}:role/ClabAPIRole\n        type: \"aws_proxy\"\n        httpMethod: \"POST\"\n        uri:\n          Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${DeleteCourse.Arn}/invocations\n        passthroughBehavior: \"when_no_match\""
  },
  {
    "objectID": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html",
    "href": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html",
    "title": "Data Engineer certificate",
    "section": "",
    "text": "aws CLI automatically splits large file into multi-upload\n$ aws s3 cp large_test_file s3://DOC-EXAMPLE-BUCKET/\nFor example in a case of multipart upload for a 100 GB file you would have the following API calls for the entire process:\n- A CreateMultipartUpload call to start the process\n- 1000 individual UploadPart calls, each uploading a part of 100 MB, for a total size of 100 GB\n- A CompleteMultipartUpload call to finish the process\nThere would be a total of 1002 API calls\nTo store directly into S3 Glacier:\naws s3 cp your-file.txt s3://your-bucket-name/your-file.txt --storage-class DEEP_ARCHIVE\naws s3api put-bucket-lifecycle-configuration --bucket your-bucket-name --lifecycle-configuration file://lifecycle_policy.json\nTo add a lifecycle policy:\n{\n    \"Rules\": [\n        {\n            \"ID\": \"Move to Glacier after 30 days\",\n            \"Filter\": {\n                \"Tag\": {\n                    \"Key\": \"Lifecycle\",\n                    \"Value\": \"Archive\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Transitions\": [\n                {\n                    \"Days\": 30,\n                    \"StorageClass\": \"GLACIER\"\n                }\n            ]\n        },\n        {\n            \"ID\": \"Delete after 365 days\",\n            \"Filter\": {\n                \"Tag\": {\n                    \"Key\": \"Lifecycle\",\n                    \"Value\": \"Archive\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 365\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html#file-upload-to-s3",
    "href": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html#file-upload-to-s3",
    "title": "Data Engineer certificate",
    "section": "",
    "text": "aws CLI automatically splits large file into multi-upload\n$ aws s3 cp large_test_file s3://DOC-EXAMPLE-BUCKET/\nFor example in a case of multipart upload for a 100 GB file you would have the following API calls for the entire process:\n- A CreateMultipartUpload call to start the process\n- 1000 individual UploadPart calls, each uploading a part of 100 MB, for a total size of 100 GB\n- A CompleteMultipartUpload call to finish the process\nThere would be a total of 1002 API calls\nTo store directly into S3 Glacier:\naws s3 cp your-file.txt s3://your-bucket-name/your-file.txt --storage-class DEEP_ARCHIVE\naws s3api put-bucket-lifecycle-configuration --bucket your-bucket-name --lifecycle-configuration file://lifecycle_policy.json\nTo add a lifecycle policy:\n{\n    \"Rules\": [\n        {\n            \"ID\": \"Move to Glacier after 30 days\",\n            \"Filter\": {\n                \"Tag\": {\n                    \"Key\": \"Lifecycle\",\n                    \"Value\": \"Archive\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Transitions\": [\n                {\n                    \"Days\": 30,\n                    \"StorageClass\": \"GLACIER\"\n                }\n            ]\n        },\n        {\n            \"ID\": \"Delete after 365 days\",\n            \"Filter\": {\n                \"Tag\": {\n                    \"Key\": \"Lifecycle\",\n                    \"Value\": \"Archive\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 365\n            }\n        }\n    ]\n}"
  },
  {
    "objectID": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html#databases",
    "href": "tips/015_Cloud/aws/certificates/data_engineer/data_engineer.html#databases",
    "title": "Data Engineer certificate",
    "section": "Databases",
    "text": "Databases\n\nDynamoDB\nDynamoDB is NoSQL Serverless database, closest to Key-value store (doesn’t not support joins, sum, age, but supports horizontal scaling). MM RPS, 100s TB of storage. Made out of table, partition key (HASH) or partition key and sort key (HASH + RANGE), and attributes that can be any key-values pairs. Max per item is 400 Kb.\nDynamo DB has Provisioned and On-Demand mode (can switch between every 24 hours, On-demand is 2.5X more expensive). Provisioned has to have defined: - Read Capacity Units (RCU) has: - strongly consistent reads (1 RCU = 1 read per second of size 4KB)\n- eventual consistent reads (1 RCU = 2 read per second of size 4KB)\n- Write Capacity Units (WCU) - 1 WCU = 1 write per second of size 1KB,\nThere is temporary BurstCapacity as a buffer but don’t rely too much on it - it uses exponential backoff if surpassed.\nNumber of partitions = ceil(max( RCUs/3000 + WCUs/1000, Total size / 10 Gb))\nWCUs and RCUs are divided evenly across partitions.\n\n\nRDS and Aurora\nFor RDS in CloudWatch monitor ReadIOPS to be small and stable (together with CPU, memory, storage, replica lag)\nMake sure DNS to RDS TTL is not too long, in case of failure you want TTL to be short (like 30 sec.).\nConsider imposing rate limits in the API Gateway.\nUse InnoDB for storage engine for MySQL and MariaDB.\nFor PostgreSQL, when loading data disable DB backups and multi-AZ. Disable synchronous commit and autovacuum (enable all after loading data is done during regular operation).\nSQL server specific: do not enable recovery mode, offline mode, or read-only mode; these break multi-AZ.\nAurora is AWS implementation of MySQL and PostgreSQL.\n\n\nOthers\n\nDocumentDB is AWS implementation of MongoDB (1MM RPS)\nMemoryDB is AWS implementation for Redis (160MM RPS)\nAmazon Keyspaces is AWS implementation for Cassandra (NoSQL) (1K RPS)\nNeptune is AWS implementation for Graph database.\nTimestream is AWS version of Time-series database (like Prometheus). Recent data is in memory, older data in cost-optimized storage.\n\n\n\nRedshift\nRedsihft is data warehouse, designed to store data and online analytic processing (OLAP)."
  },
  {
    "objectID": "tips/015_Cloud/aws/diagrams.html",
    "href": "tips/015_Cloud/aws/diagrams.html",
    "title": "Nenad Bozinovic",
    "section": "",
    "text": "from diagrams import Diagram\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.database import RDS\nfrom diagrams.aws.network import ELB\n\nwith Diagram(\"Grouped Workers\", show=False, direction=\"TB\"):\n    ELB(\"lb\") &gt;&gt; [EC2(\"worker1\"),\n                  EC2(\"worker2\"),\n                  EC2(\"worker3\"),\n                  EC2(\"worker4\"),\n                  EC2(\"worker5\")] &gt;&gt; RDS(\"events\")\n\n\n\nfrom diagrams import Diagram, Cluster\nfrom diagrams.aws.compute import ECS, EC2\nfrom diagrams.aws.network import VPC, PublicSubnet\nfrom diagrams.aws.compute import ECR\nfrom diagrams.aws.security import IAMRole\n\nwith Diagram(\"AWS ECS Deployment\", show=False, filename=\"aws_ecs_deployment_diagram\"):\n    with Cluster(\"AWS Cloud\", ):\n        with Cluster(\"VPC\"):\n            with Cluster(\"Availability Zone\"):\n                with Cluster(\"Public subnet\"):\n                    ecs_instance = EC2(\"ecs-instance\")\n\n        ecs_instance &gt;&gt; ECR(\"Elastic Container Registry\") &gt;&gt; ECS(\"Elastic Container Services Cluster\")\n        ecs_repo = ECR(\"ecs-container-repo\")\n        ecs_role = IAMRole(\"ecs-task-role\")\n\n        ecs_instance - ecs_repo - ecs_role\n\n\nfrom diagrams import Diagram, Cluster\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.storage import S3\nfrom diagrams.onprem.compute import Server\nfrom diagrams.onprem.client import Users\n\n\nwith Diagram(\"MLOps System\", show=False):\n    with Cluster(\"AWS\"):\n        fargate = EC2(\"Fargate Container\")\n        s3 = S3(\"S3 Bucket\")\n        ec2 = EC2(\"EC2 Instance\")\n        with Cluster(\"ML Workflow\"):\n            processing = Server(\"Data Processing\")\n            training = Server(\"Model Training\")\n    \n    users = Users(\"Users1\")\n\n    # Frontend Fargate container triggered by user action\n    users &gt;&gt; fargate\n\n    # Data upload to S3 bucket\n    fargate &gt;&gt; s3\n\n    # Data processing and training\n    s3 &gt;&gt; processing &gt;&gt; training\n    \n    # Report data back to S3 bucket\n    training &gt;&gt; s3"
  },
  {
    "objectID": "tips/015_Cloud/kubernetes/kubernetes.html",
    "href": "tips/015_Cloud/kubernetes/kubernetes.html",
    "title": "Kubernetes",
    "section": "",
    "text": "Credits Educative.io\nThe goal is to host cloud-native application that must:\n\nScale on demand\nSelf-heal\nSupport zero-downtime rolling updates\nRun anywhere (in cloud or non-cloud, like Raspberry PI).\n\nKubernets allows all of this. It is an orchestrator that bring together a set of microservices and organizes them into an application that brings value.\nKubernets emerged in 2014 (from Google) as a way to abstract underlying cloud and server infrastructure. It commoditized infrastructure making and becoming “OS of the cloud”.\n\nA Kubernetes cluster consists of one or more machines that have Kubernetes installed on them.\nMachines in a Kubernetes cluster are referred to as Nodes, but there are Master Nodes (Masters) and Worker Nodes (Nodes).\nWorker nodes run user applications and can either be Linux or Windows Nodes.\nAll Nodes run two main services: kubelet and Container runtime.\nThe kubelet is the main Kubernetes agent. It joins the Node to the cluster and communicates with the control plane, in charge of notifying when tasks are received and reporting on the status of those tasks.\nThe container runtime starts and stops containers.\nHosted Kubernetes is where your cloud provider rents you a Kubernetes cluster. Sometimes, it is called Kubernetes as a Service, where we don’t have to worry about Masters, just Nodes. There are many hosted Kubernets services: AWS: Elastic Kubernetes Service (EKS), Azure: Azure Kubernetes Service (AKS), DO: Digital Ocean Kubernetes Service (DOKS), GCP: Google Kubernetes Engine (GKE), Linode: Linode Kubernetes Engine (LKE). I’ll use LKE here.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nContainerize an App (via Docker)\nFirst we want our App to be containerized. It has a few files:\n\ncd /Users/nenad.bozinovic/Work/blog/nbs/mini-projects/10_kubernetes/usercode\n\n/Users/nenad.bozinovic/Work/blog/nbs/mini-projects/10_kubernets/usercode\n\n\n\n!ls App\n\nDockerfile    app.js        bootstrap.css package.json  views\n\n\n\nDockerfile: This file is not part of the application. It contains a list of instructions that Docker executes to create the container image (i.e. containerize the application).\napp.js: This is the main application file. It is a Node.js application.\nbootstrap.css: This is a stylesheet template, which determines how the application’s web page will look.\npackage.json: This lists the application dependencies.\nviews: This is a folder that contains the HTML used to populate the application’s web page.\n\nDocker file contains following:\nFROM node:current-slim\nCOPY . /src\nRUN cd /src; npm install\nEXPOSE 8080\nCMD cd /src && node ./app.js\n\n!docker --version\n\nDocker version 20.10.23, build 7155243\n\n\nWe’ll now build two docker images called qsk-course version 1.0 and 1.1 under my local account (nesaboz):\n\n#!docker image build -t nesaboz/qsk-course:1.0 App/.\n\nModify the App to create a new version and build version 1.1:\n\n#!docker image build -t nesaboz/qsk-course:1.1 .`\n\nWe can see the nesaboz/qsk-course image now in the list of docker images:\n\n!docker image ls\n\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nnesaboz/qsk-course          1.0       8d9065c8c75f   2 hours ago    268MB\nnesaboz/qsk-course          1.1       60932818191e   5 hours ago    268MB\nnesaboz/docker101tutorial   latest    940714c42b63   2 days ago     47MB\nalpine/git                  latest    22d84a66cda4   3 months ago   43.6MB\n\n\nWe can now actually run the application locally by running:\n\n#!docker run -dp 8080 nesaboz/qsk-course:1.0\n\nDocker will assign some port (can be seen in Docker Desktop), and our app will be running locally in a browser:\n\n\n\nimage.png\n\n\nWe can also run version 1.1:\n\n# !docker run -dp 8080 nesaboz/qsk-course:1.1\n\n\n\n\nimage.png\n\n\n\n\nRegister Images (via DockerHub)\nRunning locally is fine but we want to run in a cloud. There are many hosting services for containers, DockerHub is the easiest to use. We can push container to DockerHub by running the following (in Terminal since it requires password):\ndocker login --username nesaboz\ndocker image push nesaboz/qsk-course:1.0\nAnd we can see now that we have two verions on the DockerHub:\n\n\n\nimage-2.png\n\n\n\n\nSet up Hosted Kubernets (via Linode Kubernets Engine aka LKE)\nLKE (https://cloud.linode.com/) is a paid service to host Kubernets by setting a cluster of machines (we’ll use shared, CPU-only VMs). Once we set up a cluster, we can download a config file (aka kubeconfig file) that looks like this:\napiVersion: v1\nkind: Config\npreferences: {}\n\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CR...\n    server: https://8c292154-27de-4e91-a129-fcb2a1614fee.us-west-1.linodelke.net:443\n  name: lke95373\n\nusers:\n- name: lke95373-admin\n  user:\n    as-user-extra: {}\n    token: eyJhbGc...\n\ncontexts:\n- context:\n    cluster: lke95373\n    namespace: default\n    user: lke95373-admin\n  name: lke95373-ctx\n\ncurrent-context: lke95373-ctx\n\n\nConnect to LKE\nWe can now activate nodes:\nexport KUBECONFIG=/usercode/config    &lt;== path might be different, for example KUBECONFIG=config\nkubectl get nodes\nNAME                           STATUS   ROLES    AGE   VERSION\nlke95373-144188-63fe9e5739a9   Ready    &lt;none&gt;   22h   v1.25.4\nlke95373-144188-63fe9e576608   Ready    &lt;none&gt;   22h   v1.25.4\nThe nodes are now running.\n\n\nkubectl\nMain command that executes everything kubernets is kubectl:\nkubectl version -o yaml\nclientVersion:\n  buildDate: \"2022-11-09T13:36:36Z\"\n  compiler: gc\n  gitCommit: 872a965c6c6526caa949f0c6ac028ef7aff3fb78\n  gitTreeState: clean\n  gitVersion: v1.25.4\n  goVersion: go1.19.3\n  major: \"1\"\n  minor: \"25\"\n  platform: darwin/amd64\nkustomizeVersion: v4.5.7\nserverVersion:\n  buildDate: \"2023-01-18T19:15:26Z\"\n  compiler: gc\n  gitCommit: ff2c119726cc1f8926fb0585c74b25921e866a28\n  gitTreeState: clean\n  gitVersion: v1.25.6\n  goVersion: go1.19.5\n  major: \"1\"\n  minor: \"25\"\n  platform: linux/amd64\n\n\nPod object\nPod is a lightweight wrapper for Docker image we already deployed on DockerHub. The config file for Pod is pod.yml:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: first-pod\n  labels:\n    project: qsk-course\nspec:\n  containers:\n    - name: web-ctr\n      image: nesaboz/qsk-course:1.0\n      ports:\n        - containerPort: 8080\nkubectl apply -f pod.yml\nkubectl get pods\nNAME        READY   STATUS    RESTARTS   AGE\nfirst-pod   1/1     Running   0          3m36s\nWe can run similar though more detailed command (doesn’t run here in notebook):\nkubectl describe pod first-pod\nTo delete pods:\nkubectl delete pod first-pod\nkubectl delete --all pods\n\n\nKubernets Service controller\nService controller provides connectivity to the application running in the Pod i.e. provisions an internet-facing load balancer. To do that we use svc-cloud.yml file that contains following:\napiVersion: v1\nkind: Service\nmetadata:\n  name: cloud-lb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n  selector:\n    project: qsk-course    &lt;- this name must match pod.metadata.labels.project above\nNote that spec.selector.project must match the pod.metadata.labels.project\n(We also have an option of running svc-local.yml skipping that for now)\nkubectl apply -f svc-cloud.yml   #  service/cloud-lb created`\nkubectl get svc\nqsk-cloud    LoadBalancer   10.128.141.64   45.79.230.110   8080:32026/TCP   13s\nor for more details run:\nkubectl describe svc cloud-lb\nnow go to the 45.79.230.110:8080 address in your browser:\n\n\n\nimage.png\n\n\nTo delete svc object:\nkubectl delete svc cloud-lb\n\n\nKubernets Deployment controller\nDeployment service provides self-healing, enable scaling, and rolling updates. The file deploy.yml looks like this:\nkind: Deployment                   &lt;&lt;== Type of object being defined\napiVersion: apps/v1                &lt;&lt;== Version of object specification\nmetadata:\n  name: qsk-deploy\nspec:\n  replicas: 5                      &lt;&lt;== How many Pod replicas\n  selector:\n    matchLabels:                   &lt;&lt;== Tells the Deployment controller\n      project: qsk-course            &lt;&lt;== which Pods to manage, must match svc.spec.selector.project above\n  template:\n    metadata:\n      labels:\n        project: qsk-course          &lt;&lt;== Pod label, must match svc.spec.selector.project above\n    spec:\n      containers:\n      - name: qsk-pod\n        imagePullPolicy: Always           &lt;&lt;== Never use local images\n        ports:\n        - containerPort: 8080             &lt;&lt;== Network port\n        image: nesaboz/qsk-course:1.0     &lt;&lt;== Image containing the app\nNote that spec.template.labels.project must match the svc.spec.selector.project\nkubectl apply -f deploy.yml\nkubectl get deployments\nNAME         READY   UP-TO-DATE   AVAILABLE   AGE\nqsk-deploy   3/5     5            3           3s\nkubectl get pods\nNAME                          READY   STATUS    RESTARTS   AGE\nqsk-deploy-767d99b5c7-5v9vs   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running   0          52s\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          52s\nIf we delete one pod, for example first one qsk-deploy-767d99b5c7-5v9vs:\nkubectl delete pod qsk-deploy-767d99b5c7-5v9vs\nkubectl get pods\nNAME                          READY   STATUS        RESTARTS   AGE\nqsk-deploy-767d99b5c7-4vdjs   1/1     Running       0          13s\nqsk-deploy-767d99b5c7-5v9vs   1/1     Terminating   0          3m19s\nqsk-deploy-767d99b5c7-9h67b   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running       0          3m19s\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running       0          3m19s\nwe see that new one gets running immediately, demonstrating self-healing.\nWe can also see that pods are running on both nodes (lke...a9 and lke...08):\n$kubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE     IP         NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-4vdjs   1/1     Running   0          2m50s   10.2.1.7   lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          5m56s   10.2.0.7   lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-dpsk9   1/1     Running   0          5m56s   10.2.1.6   lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-lwxcf   1/1     Running   0          5m56s   10.2.1.4   lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          5m56s   10.2.0.6   lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\n\nIf we delete a node (in a LKE Cloud Console) then we can see that new pods will be created on a available node:\nkubectl get nodes\nNAME                           STATUS   ROLES    AGE   VERSION\nlke95373-144188-63fe9e576608   Ready    &lt;none&gt;   23h   v1.25.4\nkubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE    IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-2qmgn   1/1     Running   0          105s   10.2.0.8    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          13m    10.2.0.7    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-f6k76   1/1     Running   0          104s   10.2.0.10   lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-vsvgv   1/1     Running   0          104s   10.2.0.9    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          13m    10.2.0.6    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nWe can see that old pods (qsk-deploy...9h67b and qsk-deploy...wh2rh) are still running, the other 3 are new (created 105 seconds ago). If we had set the Autoscale Pool on LKE, then the node will recover eventually:\n\n\n\nimage.png\n\n\nbut the pods will still run only on one (lke...08) node:\nNAME                          READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-767d99b5c7-2qmgn   1/1     Running   0          20m   10.2.0.8    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-9h67b   1/1     Running   0          32m   10.2.0.7    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-f6k76   1/1     Running   0          20m   10.2.0.10   lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-vsvgv   1/1     Running   0          20m   10.2.0.9    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-767d99b5c7-wh2rh   1/1     Running   0          32m   10.2.0.6    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nIf we change a number of replicas in the deployment file, newly pods will intelligently run on a newly recovered node so the distribution is maintained:\nkubectl scale --replicas 10 deployment/qsk-deploy\nkubectl get pods -o wide\nNAME                          READY   STATUS    RESTARTS   AGE    IP          NODE                           NOMINATED NODE   READINESS GATES\nqsk-deploy-679dc78b78-bgmfm   1/1     Running   0          14m    10.2.0.3    lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-ckvqx   1/1     Running   0          14m    10.2.0.5    lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-cvh5t   1/1     Running   0          2m6s   10.2.1.2    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-gll5p   1/1     Running   0          2m6s   10.2.1.6    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-hjsqx   1/1     Running   0          14m    10.2.0.6    lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-qs4gs   1/1     Running   0          14m    10.2.0.10   lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-r4kgl   1/1     Running   0          2m6s   10.2.1.5    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-rqpgh   1/1     Running   0          2m6s   10.2.1.4    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-vblz4   1/1     Running   0          14m    10.2.0.4    lke95373-144188-63fe9e5739a9   &lt;none&gt;           &lt;none&gt;\nqsk-deploy-679dc78b78-x6f2g   1/1     Running   0          2m6s   10.2.1.3    lke95373-144188-63fe9e576608   &lt;none&gt;           &lt;none&gt;\nNote: kubectl scale should not be used since there is a discrpancy between deploy.yml and new nubmer of replicas. The common practice is to always edit deploy.yml file and re-apply.\n\n\nRolling update\nWe can apply rolling update if we add following lines to deploy.yml:\n  minReadySeconds: 20        &lt;== to wait for 20 seconds after updating each replica\n  strategy:                  \n    type: RollingUpdate      \n    rollingUpdate:           \n      maxSurge: 1         &lt;== allows Kubernetes to add one extra Pod during an update operation\n      maxUnavailable: 0   &lt;== prevents Kubernetes from reducing the number of Pods during an update\nas well as update the version of the image:\nimage: nesaboz/qsk-course:1.1      &lt;== this version must exists as an container on DockerHub\nso the deploy.yml looks like this:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: qsk-deploy\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      project: diy\n  minReadySeconds: 20        \n  strategy:                  \n    type: RollingUpdate      \n    rollingUpdate:           \n      maxSurge: 1            \n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        project: diy\n    spec: \n      containers:\n      - name: qsk-pod\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        image: nesaboz/qsk-course:1.1 \nNow if we re-apply deployment:\nkubectl apply -f deploy.yml\ndeployment.apps/qsk-deploy configured\nwe can monitor the rollout status:\nkubectl rollout status deployment qsk-deploy\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 2 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 4 out of 5 new replicas have been updated...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\nWaiting for deployment \"qsk-deploy\" rollout to finish: 1 old replicas are pending termination...\ndeployment \"qsk-deploy\" successfully rolled out\nSoon, the App will be updated:\n\n\n\nimage-2.png\n\n\n\n\nAppendix: Apply service locally\nFor local application let’s use local scv-local.yml file:\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-local\nspec:\n  type: NodePort    &lt;== this was LoadBalancer for cloud servers, now is NodePort\n  ports:\n  - port: 8080\n    protocol: TCP       &lt;== this is a new line\n    targetPort: 8080\n    nodePort: 31111     &lt;== this is a new line\n  selector:\n    project: qsk-course    &lt;== this must match with the pod project",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "Kubernetes"
    ]
  },
  {
    "objectID": "tips/015_Cloud/github_actions.html",
    "href": "tips/015_Cloud/github_actions.html",
    "title": "GitHub Actions with access to AWS",
    "section": "",
    "text": "If you are completely new to GitHub actions watch this ~10 minute video first.\nI wanted for Actions to run command my EC2 instance. To do that, we first need to provide user AWS credentials to GitHub. We can do this by creating a repo settings/secrets and variables/ then create environment I named “AWS” and copy/pasted AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to the secrets. Below you can see reference to these. The needs to have policy AmazonSSMFullAccess attached to it.\nThen I use actions/checkout@v4 and aws-actions/configure-aws-credentials@v4 and aws ssm send-command to run a command on my EC2 instance.\nThe aws ssm does the following: - git pull latest code - kill currently running streamlit app - start a new streamlit app - log everything to a file\nname: Execute SSM SendCommand\n\non: \n  push:\n    branches:\n      - main\n\njobs:\n  ssm-command:\n    runs-on: ubuntu-latest\n    environment: AWS\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-west-1 \n\n    - name: Send SSM Command\n      run: |\n        aws ssm send-command \\\n            --document-name \"AWS-RunShellScript\" \\\n            --targets \"Key=instanceids,Values=i-08b8b6691ed2e1b6d\" \\\n            --parameters commands=\"date &gt;&gt; /home/ubuntu/output.log && sudo -u ubuntu git -C /home/ubuntu/blog pull &gt;&gt; /home/ubuntu/output.log 2&gt;&1 && sudo pkill -f streamlit &gt;&gt; /home/ubuntu/output.log 2&gt;&1 && nohup /home/ubuntu/miniconda/bin/streamlit run /home/ubuntu/blog/nbs/projects/myGPT/myGPT.py --server.enableCORS false --server.enableXsrfProtection false &gt;&gt; /home/ubuntu/output.log 2&gt;&1\" \\\n            --region us-west-1",
    "crumbs": [
      "Projects",
      "TILs",
      "Cloud",
      "GitHub Actions with access to AWS"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html",
    "href": "tips/020_Jupyter/kernels.html",
    "title": "Kernels",
    "section": "",
    "text": "To add existing environment as a jupyter kernel:\nList kernels\nRemove kernel\nInstall new kernel\nis_jupyter = get_ipython().__class__.__name__ == 'ZMQInteractiveShell'\nis_jupyter\n\nTrue",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#custom-path-for-vscode-jupyter-notebook",
    "href": "tips/020_Jupyter/kernels.html#custom-path-for-vscode-jupyter-notebook",
    "title": "Kernels",
    "section": "Custom path for VSCode Jupyter notebook",
    "text": "Custom path for VSCode Jupyter notebook\nGo to global VSCode settings (Cmd + ,) and search for Jupyter Notebook File Root, then set it to any path you like to be imported by default.",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#custom-path-for-jupyter-notebook-in-a-browser",
    "href": "tips/020_Jupyter/kernels.html#custom-path-for-jupyter-notebook-in-a-browser",
    "title": "Kernels",
    "section": "Custom path for Jupyter Notebook in a browser",
    "text": "Custom path for Jupyter Notebook in a browser\nGo to kernel.json file:\njupyter kernelspec list\ncd ~/.local/share/jupyter/kernels/mykernel/\nadd PYTHONPATH environmental variable in the env section:\n{\n\"argv\": [\n    \"/path/to/custom/python\",\n    \"-m\",\n    \"ipykernel_launcher\",\n    \"-f\",\n    \"{connection_file}\"\n],\n\"env\": {\n    \"PYTHONPATH\": \"/path/to/custom/path\"\n},\nNote PYTHONPATH has nothing to do with the path to the python file set in argv.",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#global-path",
    "href": "tips/020_Jupyter/kernels.html#global-path",
    "title": "Kernels",
    "section": "Global path",
    "text": "Global path\nTo add a global path to all kernels we can use ipython_config.py file:\nipython profile create\nipython locate\n/Users/username/.ipython\nvi /Users/username/.ipython/profile_default/ipython_config.py\nadd these lines (use i to enter insert mode and Esc to exit, save and quit as :wq:\nc.InteractiveShellApp.exec_lines = [\n    'import sys; sys.path.append(\"/path/to/your/module\")'\n]",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#manual-path",
    "href": "tips/020_Jupyter/kernels.html#manual-path",
    "title": "Kernels",
    "section": "Manual path",
    "text": "Manual path\nOne can always get the current folder in the notebook a = os.path.abspath('') and then manipulate it. Manually setting up some ENV path variable is also an option. Magic command for environemtal variables:\n%env",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/kernels.html#change-the-default-kernel",
    "href": "tips/020_Jupyter/kernels.html#change-the-default-kernel",
    "title": "Kernels",
    "section": "Change the default kernel",
    "text": "Change the default kernel\njupyter notebook --generate-config\nIn jupyter config file modify and uncomment following line: c.MultiKernelManager.default_kernel_name='newDefault' \\n\"",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Kernels"
    ]
  },
  {
    "objectID": "tips/020_Jupyter/images.html",
    "href": "tips/020_Jupyter/images.html",
    "title": "Images",
    "section": "",
    "text": "To show images in markdown cells:\n![](path to the image)\nfor example:\n![](assets/example.png)\nwill produce:\n\nOne can also use HTML to show images (src) with lot’s of flexibility: - to change the width add width (as string) - for reference add id\n&lt;img width=\"500\" caption=\"Session_rate\" src=\"images/session_rate.jpg\" id=\"figure_id\"/&gt;\nthen refer to the figure as:\n[name to show](#figure_id)\nalso can refer to URL images: (for example in Google Colab we have to replace the reference by it’s github link):\n&lt;center&gt;\n  &lt;figure&gt;\n    &lt;img width=\"500\" src=\"image URL or local filepath\"/&gt;\n    &lt;figcaption&gt;Some caption&lt;a href=\"some link\"&gt;text for link&lt;/a&gt;&lt;/figcaption&gt;\n  &lt;/figure&gt;\n&lt;/center&gt;\n\nOne can also have two images, and offset one:\n&lt;center&gt;\n    &lt;figure&gt;\n      &lt;img width=\"200\" src=\"https://raw.githubusercontent.com/nesaboz/SpaceNet8/main/paper/final-report/figures/peak_training_gpu_memory.png\"/&gt;\n      &lt;img style=\"margin-top: 18px; width: 200px;\" src=\"https://raw.githubusercontent.com/nesaboz/SpaceNet8/main/paper/final-report/figures/epoch_time.png\"/&gt;\n    &lt;/figure&gt;\n&lt;/center&gt;\nSee this great markdown guide for more.",
    "crumbs": [
      "Projects",
      "TILs",
      "Jupyter",
      "Images"
    ]
  },
  {
    "objectID": "tips/050_FastAI/fastai.html",
    "href": "tips/050_FastAI/fastai.html",
    "title": "FastAI",
    "section": "",
    "text": "To install fastai project on any linux machine (more here, also see docs):\nsudo apt update && sudo apt -y install git\ngit clone https://github.com/fastai/fastsetup.git\ncd fastsetup\nsudo ./ubuntu-initial.sh\nwait a couple of minutes for reboot, then ssh back in:\nmamba install -c fastchan fastai\nconda install -c fastchan nbdev\nconda install -c anaconda pillow\nconda install jupyter\npip install gradio",
    "crumbs": [
      "Projects",
      "TILs",
      "FastAI",
      "FastAI"
    ]
  },
  {
    "objectID": "tips/050_FastAI/nbdev.html",
    "href": "tips/050_FastAI/nbdev.html",
    "title": "nbdev",
    "section": "",
    "text": "See main page here\nnbdev_help  # this list all commands \nTutorial says we might need to also run the following after export once: pip install -e . (-e installs a project in editable mode (i.e. setuptools “develop mode”) from a local project path (.))\n\n\n\n\n\n\n\nCommand\nAction\n\n\n\n\nnbdev_prepare\nuse it before commit and push, it will run all the code though\n\n\nnbdev_update\npropagates changes from python to jupyter\n\n\nnbdev_preview\nto see pages, and this one is live\n\n\nnbdev_export\nwill export all cells with #export flag into a filename defined in the top using #| default_exp filename\n\n\n\nOne can also export from jupyter notebook:\nfrom nbdev import nbdev_export\nnbdev_export()\nList of directives",
    "crumbs": [
      "Projects",
      "TILs",
      "FastAI",
      "nbdev"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html",
    "href": "tips/030_Terminal/zsh.html",
    "title": "zsh",
    "section": "",
    "text": "Nice Linux tutorial.\nThere are many shells one can use, Mac default is zsh (located at /bin/zsh). I set up PyCharm to use it as well in the Preferences\\Tools\\Terminal\\Shell Path\nBash/zsh comment is the same as python: #\nTo open terminal from folder I added a shortcut: Control + Alt + Shift + T\nTo see hidden files on Mac in finder: Command + Shift + .",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#command-history",
    "href": "tips/030_Terminal/zsh.html#command-history",
    "title": "zsh",
    "section": "Command History",
    "text": "Command History\nhistory: To see command history\n!10: to call command on line 10\n!! to call last command\n!string to call last command that starts with string\nhistory -c to clear history",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#environment-variables",
    "href": "tips/030_Terminal/zsh.html#environment-variables",
    "title": "zsh",
    "section": "Environment variables",
    "text": "Environment variables\nexport A=\"some value\"   # set env variable A\nenv                     # see all variables\nunset A                 # remove env variable A",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#ls-command",
    "href": "tips/030_Terminal/zsh.html#ls-command",
    "title": "zsh",
    "section": "ls command",
    "text": "ls command\nTo count files in a folder from terminal:\nls /etc | wc -l\nrecursively:\nfind &lt;directory&gt; -type f | wc -l\nfor example:\nls '/Users/nenad.bozinovic/work/Frame/elasticity_logs' | head -4\nto see file sizes:\nls -l\nls -l --block-size=M\nto see size of the whole folder:\ndu -sh share\nto see hidden files:\nls -a\nls -la\nSorts by the last date modified:\nls -lt \ngoes to previous folder:\ncd -",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#tree-command",
    "href": "tips/030_Terminal/zsh.html#tree-command",
    "title": "zsh",
    "section": "tree command",
    "text": "tree command\nTo see tree of directories use tree command:\ntree --help\nTo install on Linux:\n!sudo apt install tree\nto install on Mac:\nbrew install tree",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#gz-files",
    "href": "tips/030_Terminal/zsh.html#gz-files",
    "title": "zsh",
    "section": "gz files",
    "text": "gz files\nTo zip/unzip gz:\nInstall gnu-tar if seeing warning with the tar, then use gtar:\nbrew install gnu-tar\nTo zip:\ngtar -zcvf myfolder.tar.gz myfolder\ngzip filename  # zip it back\ngzip -k filename  # to zip it and keep original\nto see the content of a zipped file:\ngtar -tf myfolder.tar.gz\nto unzip:\ntar -xf labeled_data.tar.gz\ntar -xf labeled_data.tar.gz -C /home/user/destination",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#zip-files",
    "href": "tips/030_Terminal/zsh.html#zip-files",
    "title": "zsh",
    "section": "zip files",
    "text": "zip files\nzip filename.zip file1.txt file2.txt file3.txt\nunzip -vl titanic.zip  # to see content without unzipping\nunzip filename.zip\nunzip filename.zip -d /home/user/destination",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#download",
    "href": "tips/030_Terminal/zsh.html#download",
    "title": "zsh",
    "section": "Download",
    "text": "Download\nTo download a file from URL:\n!curl -OL &lt;URL&gt;\nFrom Google drive (FILE_ID can be found in the sharable link between /d and /view):\nimport gdown\ngdown.download(\"https://drive.google.com/uc?id=FILE_ID\", 'NEW_NAME.jpg')",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#paths",
    "href": "tips/030_Terminal/zsh.html#paths",
    "title": "zsh",
    "section": "Paths",
    "text": "Paths\nPrint path:\necho \"${PATH//:/$'\\n'}\"\nTo add folder to PATH",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#homebrew",
    "href": "tips/030_Terminal/zsh.html#homebrew",
    "title": "zsh",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew is a package manager for macOS, it might have some unique packages that pip doesn’t have, to install wget for example:\nbrew install wget\nwget https://your.link.png",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#pip",
    "href": "tips/030_Terminal/zsh.html#pip",
    "title": "zsh",
    "section": "pip",
    "text": "pip\nTo install package:\npip install torch torchvision tensorboard\nWhen using [] with pip it is important to use \"\" to avoid shell parsing for example:\npip install \"mpl_interactions[jupyter]\"\nTo see version of the package installed:\npip show torch\nTo see all packages installed:\npip list",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#vim",
    "href": "tips/030_Terminal/zsh.html#vim",
    "title": "zsh",
    "section": "vim",
    "text": "vim\n\n\n\n\n\n\n\nCommand\nExplanation\n\n\n\n\ni\ninsert mode\n\n\nEsc\nexit insert mode\n\n\n:w\nSaves the file you are working on\n\n\n:w [filename]\nAllows you to save your file with the name you’ve defined\n\n\n:wq\nSave your file and close Vim\n\n\n:q!\nQuit without first saving the file you were working on\n\n\nv\nvisual mode, this allows you to select the text you want to copy\n\n\ny\nyank (i.e. copy)\n\n\np\npaste",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#if-statement-in-zsh",
    "href": "tips/030_Terminal/zsh.html#if-statement-in-zsh",
    "title": "zsh",
    "section": "if statement in zsh",
    "text": "if statement in zsh\nif ! [[ -n $repo_name ]] || ! [[ -n $python_ver ]]; then  \n  echo \"you didn't enter repo name and/or python version\"  \n  exit 0  \nfi",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#ssh-config",
    "href": "tips/030_Terminal/zsh.html#ssh-config",
    "title": "zsh",
    "section": "ssh config",
    "text": "ssh config\nTo see all remote machines added to local computer:\ncat ~/.ssh/config",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  },
  {
    "objectID": "tips/030_Terminal/zsh.html#rename-filesfolders",
    "href": "tips/030_Terminal/zsh.html#rename-filesfolders",
    "title": "zsh",
    "section": "Rename files/folders",
    "text": "Rename files/folders\nmv source dst # to move and/or rename directory",
    "crumbs": [
      "Projects",
      "TILs",
      "Terminal",
      "zsh"
    ]
  }
]