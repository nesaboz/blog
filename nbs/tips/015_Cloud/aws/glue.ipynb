{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: **Educative.io**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create:\n",
    "- DynamoDB table, and insert some data into it (see script below)\n",
    "- an IAM role (and give it AWSGlueServiceRole and AWSGluePolicy)\n",
    "- a Glue crawler and point it to the DynamoDB table as source to crawl\n",
    "    - a new table that will store extracted metadata\n",
    "Now we can run the crawler that will create a table in the Glue Data Catalog.\n",
    "\n",
    "Notice that we didn't have to specify the schema of the table, Glue automatically inferred it from the data.\n",
    "\n",
    "Next, we'll set up a Glue ETL job that will extract data from the DynamoDB table, transform it, and load it into an S3 bucket. We create:\n",
    "- a S3 bucket\n",
    "- set up \"Visual ETL\" from Glue and add 3 nodes:\n",
    "    - DynamoDB \"movies\" table as \"Source\"\n",
    "    - SQL query as \"Transform\"\n",
    "    - S3 bucket as \"Target\" \n",
    "- configure the job:\n",
    "    - Set up an example SQL transform script (that will remove \"s\" from the index, and select only PG-13 movies (see below))\n",
    "\n",
    "Finally, run the Visual ETL job, after its completion, check the S3 bucket for the results. You will see bunch of files starting with \"run...\" that contain the results of the ETL job (we can query them with SQL query).\n",
    "These files can then be used for any subsequent processing, such as ML training.\n",
    "\n",
    "We have successfully extracted data from DynamoDB, transformed it, and loaded it into S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert data into DynamoDB table:\n",
    "\n",
    "```python\n",
    "import csv\n",
    "import boto3\n",
    "\n",
    "# Initialize the DynamoDB client\n",
    "dynamodb = boto3.client(\n",
    "  'dynamodb',\n",
    "  aws_access_key_id='ZYX',\n",
    "  aws_secret_access_key='XYZ',\n",
    "  region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Define a function to insert a whole CSV into the table using the insert_csv method\n",
    "def insert_csv():\n",
    "    csv_file = \"Movies.csv\"\n",
    "\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        header = next(csv_reader)  # First row is the header\n",
    "        for row in csv_reader:\n",
    "            item = {}\n",
    "            for i in range(len(header)):\n",
    "                item[header[i]] = {'S': row[i]}\n",
    "            \n",
    "            response = dynamodb.put_item(\n",
    "                TableName='Movies',\n",
    "                Item=item\n",
    "            )\n",
    "\n",
    "# # Insert four items into the table\n",
    "insert_csv()\n",
    "print('Table populated successfully.')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    -- Extract only numbers from the string type index column and convert it to an integer and rename it as cleaned_index\n",
    "    CAST(REPLACE(index, 's', '') AS INT) AS cleaned_index,\n",
    "    -- Include other columns you want to select\n",
    "    listed_in,\n",
    "    duration,\n",
    "    date_added,\n",
    "    country,\n",
    "    director,\n",
    "    rating,\n",
    "    release_year,\n",
    "    title\n",
    "FROM \n",
    "    myDataSource\n",
    "WHERE \n",
    "    -- Choose only the movies that have PG-13 rating\n",
    "    rating = 'PG-13'\n",
    "ORDER BY \n",
    "    -- Sort the result set based on the values in the cleaned_index column in ascending order\n",
    "    cleaned_index;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
