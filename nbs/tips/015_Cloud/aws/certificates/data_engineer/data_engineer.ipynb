{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer certificate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File upload to S3\n",
    "\n",
    "aws CLI automatically splits large file into multi-upload \n",
    "```bash\n",
    "$ aws s3 cp large_test_file s3://DOC-EXAMPLE-BUCKET/\n",
    "```\n",
    "For example in a case of multipart upload for a 100 GB file you would have the following API calls for the entire process:  \n",
    "- A CreateMultipartUpload call to start the process   \n",
    "- 1000 individual UploadPart calls, each uploading a part of 100 MB, for a total size of 100 GB   \n",
    "- A CompleteMultipartUpload call to finish the process   \n",
    "There would be a total of 1002 API calls   \n",
    "\n",
    "To store directly into S3 Glacier:\n",
    "```bash\n",
    "aws s3 cp your-file.txt s3://your-bucket-name/your-file.txt --storage-class DEEP_ARCHIVE\n",
    "```\n",
    "\n",
    "```bash\n",
    "aws s3api put-bucket-lifecycle-configuration --bucket your-bucket-name --lifecycle-configuration file://lifecycle_policy.json\n",
    "```\n",
    "\n",
    "To add a lifecycle policy:\n",
    "```json\n",
    "{\n",
    "    \"Rules\": [\n",
    "        {\n",
    "            \"ID\": \"Move to Glacier after 30 days\",\n",
    "            \"Filter\": {\n",
    "                \"Tag\": {\n",
    "                    \"Key\": \"Lifecycle\",\n",
    "                    \"Value\": \"Archive\"\n",
    "                }\n",
    "            },\n",
    "            \"Status\": \"Enabled\",\n",
    "            \"Transitions\": [\n",
    "                {\n",
    "                    \"Days\": 30,\n",
    "                    \"StorageClass\": \"GLACIER\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"ID\": \"Delete after 365 days\",\n",
    "            \"Filter\": {\n",
    "                \"Tag\": {\n",
    "                    \"Key\": \"Lifecycle\",\n",
    "                    \"Value\": \"Archive\"\n",
    "                }\n",
    "            },\n",
    "            \"Status\": \"Enabled\",\n",
    "            \"Expiration\": {\n",
    "                \"Days\": 365\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "### DynamoDB\n",
    "\n",
    "DynamoDB is NoSQL Serverless database, closest to Key-value store (doesn't not support joins, sum, age, but supports horizontal scaling). MM RPS, 100s TB of storage. Made out of table, partition key (HASH) or partition key and sort key (HASH + RANGE), and attributes that can be any key-values pairs. Max per item is 400 Kb.\n",
    "\n",
    "Dynamo DB has Provisioned and On-Demand mode (can switch between every 24 hours, On-demand is 2.5X more expensive). Provisioned has to have defined:\n",
    "- Read Capacity Units (RCU) has:\n",
    "    - strongly consistent reads (1 RCU = 1 read per second of size 4KB)  \n",
    "    - eventual consistent reads (1 RCU = 2 read per second of size 4KB)  \n",
    "- Write Capacity Units (WCU) - 1 WCU = 1 write per second of size 1KB,\n",
    "\n",
    "There is temporary BurstCapacity as a buffer but don't rely too much on it - it uses exponential backoff if surpassed.\n",
    "\n",
    "Number of partitions = ceil(max( RCUs/3000 + WCUs/1000, Total size / 10 Gb))\n",
    "\n",
    "WCUs and RCUs are divided evenly across partitions.\n",
    "\n",
    "### RDS and Aurora\n",
    "\n",
    "For RDS in CloudWatch monitor `ReadIOPS` to be small and stable (together with CPU, memory, storage, replica lag)\n",
    "\n",
    "Make sure DNS to RDS TTL is not too long, in case of failure you want TTL to be short (like 30 sec.).\n",
    "\n",
    "Consider imposing rate limits in the API Gateway.\n",
    "\n",
    "Use InnoDB for storage engine for MySQL and MariaDB.\n",
    "\n",
    "For PostgreSQL, when loading data disable DB backups and multi-AZ. Disable synchronous commit and autovacuum (enable all after loading data is done during regular operation).\n",
    "\n",
    "SQL server specific: do not enable recovery mode, offline mode, or read-only mode; these break multi-AZ.\n",
    "\n",
    "Aurora is AWS implementation of MySQL and PostgreSQL.\n",
    "\n",
    "### Others\n",
    "\n",
    "- DocumentDB is AWS implementation of MongoDB (1MM RPS)\n",
    "- MemoryDB is AWS implementation for Redis (160MM RPS)\n",
    "- Amazon Keyspaces is AWS implementation for Cassandra (NoSQL) (1K RPS)\n",
    "- Neptune is AWS implementation for Graph database.\n",
    "- Timestream is AWS version of Time-series database (like Prometheus). Recent data is in memory, older data in cost-optimized storage.\n",
    "\n",
    "### Redshift\n",
    "Redsihft is data warehouse, designed to store data and online analytic processing (OLAP).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
