{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7e7b02",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fec6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First we install `awscli` which is aws API (follow [Link](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) to install).\n",
    "\n",
    "```zsh\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "unzip awscliv2.zip\n",
    "sudo ./aws/install\n",
    "```\n",
    "\n",
    "```zsh\n",
    "which aws\n",
    "aws --version\n",
    "```\n",
    "\n",
    "Commands that expect aws credential will first look for KEY_ID and SECRET_ACCESS env variables, then local/global `.credentials` set up by  \n",
    "```zsh\n",
    "aws configure  # configures the credentials\n",
    "aws s3 ls      # list all S3 buckets\n",
    "aws sts get-caller-identity  # gets identity of a user\n",
    "```\n",
    "\n",
    "\n",
    "## Set up new conda environment\n",
    "\n",
    "SageMaker comes with many built-in [Images](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html) that have many preloaded packages. Each image can support different Jupyter kernels (or equivalent conda environments).\n",
    "\n",
    "First access Image terminal directly (navigate to the Launcher (click on \"Amazon SageMaker Studio in the top left corner) and \"Open Image terminal\")\n",
    "\n",
    "\n",
    "```zsh\n",
    "conda env list\n",
    "```\n",
    "will print `opt/conda` only.\n",
    "\n",
    "I had to install pip (for some reason it is not preinstalled):\n",
    "```zsh\n",
    "apt-get update\n",
    "apt-get install python-pip\n",
    "```\n",
    "\n",
    "Let's create a new environment:\n",
    "```zsh\n",
    "conda create -n  python=3.9\n",
    "conda activate new_env\n",
    "conda install jupyter\n",
    "```\n",
    "\n",
    "Let's also add this environment as a Jupyter kernel:\n",
    "```\n",
    "jupyter kernelspec list\n",
    "```\n",
    "```\n",
    "ipython kernel install --name new_env --user\n",
    "```\n",
    "Now our custom environment is showing up.\n",
    "\n",
    "Let's install required packages in the `new_env` env:\n",
    "\n",
    "```zsh\n",
    "pip install streamlit pandas polars\n",
    "pip install torch torchvision torchaudio\n",
    "pip install jupyter methodtools pytorch-lightning scikit-learn colorama libtmux onnxruntime openpyxl xlsxwriter matplotlib pulp\n",
    "```\n",
    "most of these are already preinstalled.\n",
    "\n",
    "That's it. Our kernel is now ready to be used.\n",
    "\n",
    "There are [more](https://aws.amazon.com/blogs/machine-learning/four-approaches-to-manage-python-packages-in-amazon-sagemaker-studio-notebooks/) ways this can be done and I dabbled in [Lifecycle Configuration](#optional-set-up-lifecycle-configuration-not-functional-yet) but haven't had success.\n",
    "\n",
    "## Clone GitHub repository\n",
    "\n",
    "Best way to do this is via terminal (SageMaker has GitHub tab option but couldn't make it run). We'll show here one of the easy ways to do this is via Personal Access Token (there is ssh-key option as well). First, In Github, navigate to [Personal Access Tokens](https://github.com/settings/tokens), then in SageMaker Studio terminal clone the repo:\n",
    "```\n",
    "git clone <repo-URL>\n",
    "```\n",
    "**when asked for password enter the token that you just created!**\n",
    "\n",
    "To set up global git credentials, edit and run following lines:\n",
    "```zsh\n",
    "git config --global user.email \"you@example.com\"\n",
    "git config --global user.name \"Your Name\"\n",
    "```\n",
    "that way all the git changes will have your credentials.\n",
    "\n",
    "## To run SageMaker in VS-Code web UI\n",
    "\n",
    "SageMaker Studio is the AWSs' UI. If you prefer the VSCode look then you need to install somewhat awkwardly named \"Code Server\" (see installation [instructions](https://aws.amazon.com/blogs/machine-learning/host-code-server-on-amazon-sagemaker/). You will now be able to run VS Code in a browser.\n",
    "\n",
    "![Alt Text](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/14/Screenshot-2022-09-29-at-11.33.15-2-1024x267.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8c654-21d0-4367-ae4f-c362a971474e",
   "metadata": {},
   "source": [
    "## S3 access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116e6f4-3602-4a0b-919a-bd2a01718f89",
   "metadata": {},
   "source": [
    "For s3 access one must set up credentials.\n",
    "\n",
    "Accessing S3 files is not as straightforward as if having them locally since interaction has to go via AWS APIs. `boto3` and `s3fs` are two libraries that have their own APIs. I find s3fs to be better since \"S3Fs is a Pythonic file interface to S3. It builds on top of botocore\". Pandas is an exception and `pd.read_csv` just works. There is `s3fs-fuse` library (and apparently one with even higher performance called `goofys`, neither in pip) that kind of offer mounting s3 option, however, I ran into issues when trying to install them. Following are some examples:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddd04b-5b30-4469-b803-c4a0ea237b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bucket_name = 'X'\n",
    "file_key = 'Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70589015-fd4e-40bd-9acb-ca5189abb941",
   "metadata": {},
   "source": [
    "## Read into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ee411-0556-49ca-99b4-5ba5722ca8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f's3://{bucket_name}/{file_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ee83a-4b2d-4d5a-ad2d-40f7237dbdb0",
   "metadata": {},
   "source": [
    "## s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0f459-fe3c-4d75-81fc-0af73c9b0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False)  # Set anon to True for public buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b809fb-ed7f-4ffc-8290-e2711c82dc64",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ad924-98f6-4506-aabd-72ef095905e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = s3.ls(bucket_name)\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101a3a1-b207-419f-80cf-72a363c5603d",
   "metadata": {},
   "source": [
    "### glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d282ec1-d499-42f4-be7c-700b57dd7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.glob(f's3://{bucket_name}/**/a*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24134e-2b44-4cdb-af1e-71332243575a",
   "metadata": {},
   "source": [
    "### download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb158ab-76b8-4511-8ff1-9e02376ea024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify local directory for downloaded file\n",
    "local_directory = '/tmp/'\n",
    "local_file_path = os.path.join(local_directory, 'Z')\n",
    "\n",
    "# Ensure the local directory exists\n",
    "os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# Download CSV file\n",
    "s3.download(f's3://{bucket_name}/{file_key}', local_file_path)\n",
    "\n",
    "# Load CSV into Pandas DataFrame\n",
    "df = pd.read_csv(local_file_path)\n",
    "\n",
    "# Clean up downloaded file\n",
    "# os.remove(local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89841a-ced2-426d-bba8-c3a6060350e3",
   "metadata": {},
   "source": [
    "## boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90a4ef-55b5-42fc-a138-d22429ca56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85363e3f-674f-4a87-be24-88be358d5a4c",
   "metadata": {},
   "source": [
    "### download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b529961-9c3b-45fe-b8e1-1a7ed7add8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify local directory for downloaded file\n",
    "local_directory = '/tmp/'\n",
    "local_file_path = os.path.join(local_directory, 'Z')\n",
    "\n",
    "# Ensure the local directory exists\n",
    "os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# Download CSV file\n",
    "s3_client.download_file(bucket_name, file_key, local_file_path)\n",
    "\n",
    "# Load CSV into Pandas DataFrame\n",
    "df = pd.read_csv(local_file_path)\n",
    "\n",
    "# Clean up downloaded file\n",
    "os.remove(local_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484341b4-7094-4b24-84a6-04c5e029d4e6",
   "metadata": {},
   "source": [
    "### ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7ad05-5461-44a5-a839-2ae140aa9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = s3_client.list_objects(Bucket=bucket_name, Prefix='X')['Contents']  \n",
    "for f in contents:  \n",
    "    print(f['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362314cb",
   "metadata": {},
   "source": [
    "## Restore deleted files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82211c34",
   "metadata": {},
   "source": [
    "If Version Control is enabled in S3 then deleted files will just have a tag `Deleted Marker` or similar. The idea is to remove these tags ([link](https://repost.aws/knowledge-center/s3-undelete-configuration)).\n",
    "To do this for many files at once first create a script that will list all the files to be restored:\n",
    "```zsh\n",
    "echo '#!/bin/bash' > undeleteScript.sh\n",
    "\n",
    "aws --output text s3api list-object-versions --bucket <bucket name> --prefix \"Flagging Tool/\" | grep -E \"^DELETEMARKERS\" | awk '{FS = \"[\\t]+\"; print \"aws s3api delete-object --bucket <bucket name>  --key \\42\"$3\"\\42 --version-id \"$5\";\"}' >> undeleteScript.sh\n",
    "```\n",
    "\n",
    "Then just run the\n",
    "```zsh\n",
    "chmod +x undeleteScript.sh\n",
    "./undeleteScript.sh\n",
    "```\n",
    "\n",
    "finally remove it:\n",
    "```zsh\n",
    "rm -f undeleteScript.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-hana-medbravo-py__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1",
   "language": "python",
   "name": "conda-env-hana-medbravo-py__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
