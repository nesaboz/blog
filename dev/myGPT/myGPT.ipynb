{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Layer\n",
    "\n",
    "- `virtualenv env_name_of_choice && source env_name_of_choice/bin/activate`\n",
    "- Lambda has issues with some native libraries per [this](https://docs.aws.amazon.com/lambda/latest/dg/python-package.html#python-package-native-libraries), so make sure to run this when installing packages:\n",
    "```bash\n",
    "pip install --platform manylinux2014_x86_64 --target=package --implementation cp --python-version 3.x --only-binary=:all: --upgrade <package_name> -t ./theEnvFolder/python\n",
    "```\n",
    "or use old version of `pydantic`:\n",
    "```bash\n",
    "pip install openai==1.10.0 pydantic==1.10.12 -t ./theEnvFolder/python   # there is a bug in later versions of pydantic, \n",
    "```\n",
    "\n",
    "now just zip the environment folder and upload it to AWS Lambda:\n",
    "```bash\n",
    "cd theEnvFolder\n",
    "zip -r layer.zip . \n",
    "aws lambda publish-layer-version --layer-name openai_layer --zip-file fileb://layer.zip --compatible-runtimes python3.11 --no-cli-pager\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda\n",
    "\n",
    "Create IAM Role that will allow to execute AWS Lambda.\n",
    "\n",
    "Make sure Lambda Python version matches the version in the Layer above.\n",
    "\n",
    "Let's use this code:\n",
    "\n",
    "```python\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    if 'body' in event:  # this is needed when calling Lambda via URL, since URL call and boto3 have different events\n",
    "        event = json.loads(event.get('body'))\n",
    "        \n",
    "    role = event.get('key1', 'You are software engineer')\n",
    "    question = event.get('key2', 'Tell me a joke')\n",
    "        \n",
    "    chat = [{\"role\": \"system\", \"content\": role}]\n",
    "\n",
    "    chat_history = chat.copy()\n",
    "    chat_history.append({\"role\":\"user\", \"content\": question})\n",
    "\n",
    "    reply = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=chat_history\n",
    "        )\n",
    "    \n",
    "    text = reply.choices[0].message.content\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': text\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we can call Lamda function and pass the payload directly (so no need for AWS API Gateway):\n",
    "- using URL\n",
    "- using boto3 \n",
    "\n",
    "Let's demo both, URL might be better when using HTML.\n",
    "\n",
    "We start with very first instruction for the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [{\"role\": \"system\", \"content\": 'You are software engineer'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can have a first payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.extend([{\"role\": \"user\", \"content\": \"What is the weather like today?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are software engineer'},\n",
       " {'role': 'user', 'content': 'What is the weather like today?'}]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass this onto a chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"Sorry, as an AI, I don't have real-time data or access to external databases such as weather information.\"}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=chat\n",
    "        )\n",
    "reply_message = reply.choices[0].message\n",
    "simple_text = {'role': reply_message.role, 'content':reply_message.content}\n",
    "simple_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow up, with extra question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(chat_history, some_question):\n",
    "        \"\"\"We take a chat_history, append a question as a user, then get a reply from the assistant, and append that too\n",
    "\n",
    "        Args:\n",
    "            chat_history (list): A list of dictionaries, with each dictionary containing a role and content key\n",
    "            some_question (string): \n",
    "        \"\"\"\n",
    "        chat_history.append({\"role\": \"user\", \"content\": some_question})\n",
    "        reply = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=chat\n",
    "                )\n",
    "        reply_message = reply.choices[0].message\n",
    "        chat_history.append({'role': reply_message.role, 'content':reply_message.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### URL call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while GPT4 is done ...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here's a simple implementation of Dijkstra's algorithm in Python.\n",
       "\n",
       "```python\n",
       "import sys\n",
       "import heapq\n",
       "\n",
       "def dijkstra(graph, start_vertex):\n",
       "    D = {v:float('inf') for v in graph}\n",
       "    D[start_vertex] = 0\n",
       "\n",
       "    priority_queue = [(0, start_vertex)]\n",
       "    while priority_queue:\n",
       "        (dist, current_vertex) = heapq.heappop(priority_queue)\n",
       "        for neighbor, neighbor_dist in graph[current_vertex].items():\n",
       "            old_dist = D[neighbor]\n",
       "            new_dist = D[current_vertex] + neighbor_dist\n",
       "            if old_dist < new_dist:\n",
       "                continue\n",
       "            else:\n",
       "                D[neighbor] = new_dist\n",
       "            heapq.heappush(priority_queue, (D[neighbor], neighbor))\n",
       "    return D\n",
       "\n",
       "# Example of a graph\n",
       "graph = {\n",
       "    's': {'a': 2, 'b': 1},\n",
       "    'a': {'s': 3, 'b': 4, 'c':8},\n",
       "    'b': {'s': 4, 'a': 2, 'd':2},\n",
       "    'c': {'a': 2, 'd': 7, 't':4},\n",
       "    'd': {'b': 1, 'c': 11, 't':5},\n",
       "    't': {'c': 3, 'd': 5}\n",
       "}\n",
       "\n",
       "print(dijkstra(graph, 's'))\n",
       "```\n",
       "In this script, `priority_queue` is a min-priority queue implemented with a heap data structure, where vertices are prioritized by their currently known shortest distance from `start_vertex`. `D` is a dictionary mapping vertices to their shortest distances from `start_vertex`.\n",
       "\n",
       "This program prints the shortest path from the 's' vertex to all other vertices in the graph. Note that this is a very simple implementation and might not be the most optimal for larger data sets. You would typically use a proper priority queue or indexed priority queue implementation in real-world cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Your Lambda function's URL\n",
    "lambda_url = \"https://5faukxw75uazcs2zdl4zbvyg2e0lebie.lambda-url.us-west-1.on.aws/\"\n",
    "print(\"Please wait while GPT4 is done ...\")\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response_url = requests.post(headers=headers, url=lambda_url, json=payload)\n",
    "Markdown(response_url.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method is using boto3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here is a Python version of Dijkstra's Algorithm:\n",
       "\n",
       "```python\n",
       "from heapq import heappop, heappush\n",
       "\n",
       "def dijkstra(graph, start):\n",
       "    D = {x:float('inf') for x in graph}\n",
       "    D[start] = 0\n",
       "\n",
       "    queue = [(0, start)]\n",
       "\n",
       "    while queue:\n",
       "        _, current_vertex = heappop(queue)\n",
       "        for neighbor, distance in graph[current_vertex].items():\n",
       "            old_distance = D[neighbor]\n",
       "            new_distance = D[current_vertex] + distance\n",
       "\n",
       "            if new_distance < old_distance:\n",
       "                heappush(queue, (new_distance, neighbor))\n",
       "                D[neighbor] = new_distance\n",
       "\n",
       "    return D\n",
       "\n",
       "# usage\n",
       "# graph = {\n",
       "#     'A': {'B': 1, 'C': 3},\n",
       "#     'B': {'A': 1, 'C': 2},\n",
       "#     'C': {'A': 3, 'B': 2}\n",
       "# }\n",
       "#\n",
       "# distances = dijkstra(graph, 'A')\n",
       "```\n",
       "\n",
       "Now, explanation for each blocks:\n",
       "1. import heapq library: which allows you to convert a regular list to a heap\n",
       "2. Initialise a dictionary `D` that keeps track of the minimum distance from the `start` node to each node in the graph.\n",
       "3. Initialise a priority queue (heap) `queue` with tuple (0, start). Use distance as priority.\n",
       "4. While `queue` is not empty. Remove the vertex from the `queue` that has the smallest distance from start.\n",
       "5. Iterate through the neighbours of current vertex. For each neighbour, check if the shortest distance to start found so far (`old_distance`) is longer than the shortest distance to start through current_vertex (`new_distance`), update the shortest distance to start of neighbour (`D[neighbor]`) and add (new_distance, neighbour) to `queue`.\n",
       "6. Return list `D` that keeps track of the shortest distance to `start` of each node."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "function_name = 'myGPT'\n",
    "\n",
    "# Invoke the Lambda function\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=function_name,\n",
    "    InvocationType='RequestResponse',  # Use 'Event' for asynchronous execution\n",
    "    Payload=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Read the response\n",
    "response_payload = json.loads(response['Payload'].read().decode('utf-8'))\n",
    "Markdown(response_payload['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making this into a WebPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "    <textarea id=\"user_input\" rows=\"2\" cols=\"50\" placeholder=\"Type your message here...\"></textarea>\n",
       "    <button onclick=\"sendMessage()\">Send</button>\n",
       "</div>\n",
       "<div id=\"chat_area\" style=\"height:200px; overflow:auto; border:1px solid #ccc; margin-top:10px; padding:10px;\">\n",
       "    <p><b>ChatBot</b>: Hi, how can I help you today?</p>\n",
       "</div>\n",
       "<script>\n",
       "function sendMessage(){\n",
       "    var userText = document.getElementById('user_input').value;\n",
       "    var chatArea = document.getElementById('chat_area');\n",
       "    \n",
       "    // Append user's message to chat area\n",
       "    chatArea.innerHTML += \"<p><b>You typed</b>: \" + userText + \"</p>\";\n",
       "    \n",
       "    // Clear the user's input field\n",
       "    document.getElementById('user_input').value = \"\";\n",
       "\n",
       "    // Send the message to Python backend\n",
       "    var kernel = Jupyter.notebook.kernel;\n",
       "    var command = \"process_message('\" + userText + \"')\";\n",
       "    kernel.execute(command, {iopub: {output: function(response) {\n",
       "        var res = response.content.text;\n",
       "        // Append bot's response to chat area\n",
       "        chatArea.innerHTML += \"<p><b>ChatBot</b>: \" + res + \"</p>\";\n",
       "        // Scroll to the bottom of chat area\n",
       "        chatArea.scrollTop = chatArea.scrollHeight;\n",
       "    }}});\n",
       "}\n",
       "\n",
       "function process_message(message) {\n",
       "    // Here you would add your logic to process the message and generate a response\n",
       "    // For now, we just echo the message\n",
       "    return \"You said: \" + message;\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Javascript\n",
    "import json\n",
    "\n",
    "html_code = \"\"\"\n",
    "<div>\n",
    "    <textarea id=\"user_input\" rows=\"2\" cols=\"50\" placeholder=\"Type your message here...\"></textarea>\n",
    "    <button onclick=\"sendMessage()\">Send</button>\n",
    "</div>\n",
    "<div id=\"chat_area\" style=\"height:200px; overflow:auto; border:1px solid #ccc; margin-top:10px; padding:10px;\">\n",
    "    <p><b>ChatBot</b>: Hi, how can I help you today?</p>\n",
    "</div>\n",
    "<script>\n",
    "function sendMessage(){\n",
    "    var userText = document.getElementById('user_input').value;\n",
    "    var chatArea = document.getElementById('chat_area');\n",
    "    \n",
    "    // Append user's message to chat area\n",
    "    chatArea.innerHTML += \"<p><b>You typed</b>: \" + userText + \"</p>\";\n",
    "    \n",
    "    // Clear the user's input field\n",
    "    document.getElementById('user_input').value = \"\";\n",
    "\n",
    "    // Send the message to Python backend\n",
    "    var kernel = Jupyter.notebook.kernel;\n",
    "    var command = \"process_message('\" + userText + \"')\";\n",
    "    kernel.execute(command, {iopub: {output: function(response) {\n",
    "        var res = response.content.text;\n",
    "        // Append bot's response to chat area\n",
    "        chatArea.innerHTML += \"<p><b>ChatBot</b>: \" + res + \"</p>\";\n",
    "        // Scroll to the bottom of chat area\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "    }}});\n",
    "}\n",
    "\n",
    "function process_message(message) {\n",
    "    // Here you would add your logic to process the message and generate a response\n",
    "    // For now, we just echo the message\n",
    "    return \"You said: \" + message;\n",
    "}\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
